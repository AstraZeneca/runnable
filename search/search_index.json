{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Runnable","text":"Orchestrate your functions, notebooks, scripts anywhere!! <p> Runner icons created by Leremy - Flaticon </p>"},{"location":"#example","title":"Example","text":"<p>The below data science flavored code is a well-known iris example from scikit-learn.</p> <pre><code>\"\"\"\nYou can execute this example by:\n\n    python examples/iris_demo.py\n\"\"\"\n\n\"\"\"\nExample of Logistic regression using scikit-learn\nhttps://scikit-learn.org/stable/auto_examples/linear_model/plot_iris_logistic.html\n\"\"\"\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import datasets\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom sklearn.linear_model import LogisticRegression\n\n\ndef load_data():\n    # import some data to play with\n    iris = datasets.load_iris()\n    X = iris.data[:, :2]  # we only take the first two features.\n    Y = iris.target\n\n    return X, Y\n\n\ndef model_fit(X: np.ndarray, Y: np.ndarray, C: float = 1e5):\n    logreg = LogisticRegression(C=C)\n    logreg.fit(X, Y)\n\n    return logreg\n\n\ndef generate_plots(X: np.ndarray, Y: np.ndarray, logreg: LogisticRegression):\n    _, ax = plt.subplots(figsize=(4, 3))\n    DecisionBoundaryDisplay.from_estimator(\n        logreg,\n        X,\n        cmap=plt.cm.Paired,\n        ax=ax,\n        response_method=\"predict\",\n        plot_method=\"pcolormesh\",\n        shading=\"auto\",\n        xlabel=\"Sepal length\",\n        ylabel=\"Sepal width\",\n        eps=0.5,\n    )\n\n    # Plot also the training points\n    plt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors=\"k\", cmap=plt.cm.Paired)\n\n    plt.xticks(())\n    plt.yticks(())\n\n    plt.savefig(\"iris_logistic.png\")\n\n    # TODO: What is the right value?\n    return 0.6\n\n\n## Without any orchestration\ndef main():\n    X, Y = load_data()\n    logreg = model_fit(X, Y, C=1.0)\n    generate_plots(X, Y, logreg)\n\n\n## With runnable orchestration\ndef runnable_pipeline():\n    # The below code can be anywhere\n    from runnable import Catalog, Pipeline, PythonTask, metric, pickled\n\n    # X, Y = load_data()\n    load_data_task = PythonTask(\n        function=load_data,\n        name=\"load_data\",\n        returns=[pickled(\"X\"), pickled(\"Y\")],  # (1)\n    )\n\n    # logreg = model_fit(X, Y, C=1.0)\n    model_fit_task = PythonTask(\n        function=model_fit,\n        name=\"model_fit\",\n        returns=[pickled(\"logreg\")],\n    )\n\n    # generate_plots(X, Y, logreg)\n    generate_plots_task = PythonTask(\n        function=generate_plots,\n        name=\"generate_plots\",\n        terminate_with_success=True,\n        catalog=Catalog(put=[\"iris_logistic.png\"]),  # (2)\n        returns=[metric(\"score\")],\n    )\n\n    pipeline = Pipeline(\n        steps=[load_data_task, model_fit_task, generate_plots_task],\n    )  # (4)\n\n    pipeline.execute()\n\n    return pipeline\n\n\nif __name__ == \"__main__\":\n    # main()\n    runnable_pipeline()\n</code></pre> <ol> <li>Return two serialized objects X and Y.</li> <li>Store the file <code>iris_logistic.png</code> for future reference.</li> <li>Define the sequence of tasks.</li> <li>Define a pipeline with the tasks</li> </ol> <p>The difference between native driver and runnable orchestration:</p> <p>Notebooks and Shell scripts</p> <p>You can execute notebooks and shell scripts too!!</p> <p>They can be written just as you would want them, plain old notebooks and scripts.</p> <pre><code>- X, Y = load_data()\n+load_data_task = PythonTask(\n+    function=load_data,\n+     name=\"load_data\",\n+     returns=[pickled(\"X\"), pickled(\"Y\")], (1)\n+    )\n\n-logreg = model_fit(X, Y, C=1.0)\n+model_fit_task = PythonTask(\n+   function=model_fit,\n+   name=\"model_fit\",\n+   returns=[pickled(\"logreg\")],\n+   )\n\n-generate_plots(X, Y, logreg)\n+generate_plots_task = PythonTask(\n+   function=generate_plots,\n+   name=\"generate_plots\",\n+   terminate_with_success=True,\n+   catalog=Catalog(put=[\"iris_logistic.png\"]), (2)\n+   )\n\n\n+pipeline = Pipeline(\n+   steps=[load_data_task, model_fit_task, generate_plots_task], (3)\n</code></pre> <ul> <li> <code>Domain</code> code remains completely independent of <code>driver</code> code.</li> <li> The <code>driver</code> function has an equivalent and intuitive runnable expression</li> <li> Reproducible by default, runnable stores metadata about code/data/config for every execution.</li> <li> The pipeline is <code>runnable</code> in any environment.</li> </ul>"},{"location":"iterative/","title":"Iterative","text":""},{"location":"iterative/#the_problems_we_are_trying_to_solve","title":"The problems we are trying to solve.","text":"<ul> <li> <p>feature:</p> <ul> <li>Users should be able to record custom data to the logs.</li> </ul> </li> <li> <p>There is some data that need not be replicated but should be captured in logs.</p> <ul> <li>DECISION: We would not play a role in sourcing the data and the user is expected to do it.</li> <li>There should be a provision to identify the source data as a soft get.</li> <li>This is true for large datasets. How true is that though?</li> <li>This data could be sourced from a different location than the catalog.<ul> <li>This is a soft requirement and can be justified if it is not satisfied.</li> <li>For simplicity lets assume that this is part of the catalog location.</li> </ul> </li> <li>This could be achieved by using a type of catalog that does not copy but records.</li> </ul> </li> </ul> <p>!!! note:     Can this be simplified by using a \"cached catalog\"?</p> <ul> <li> <p>cached catalog:</p> <ul> <li>the run log will capture the catalog metadata.</li> <li>The catalog will not be run id specific.</li> </ul> </li> <li> <p>Cached behavior: Given a previous run.</p> <ul> <li>Users can refer to data generated from a previous run.</li> <li>If the step by name is part of the pipeline and executed successfully, we want to skip execution of the step.</li> <li>The logs should make it clear that it is a continuation of the previous run.</li> <li>Its OK to assume that the run log is maintained in the same way.</li> <li> <p>Its OK to assume that the catalog is maintained in the same way.</p> </li> <li> <p>Question about the recursive behavior.</p> <ul> <li>What if the referenced run is a continuation of the previous run?</li> <li>The desired behavior should be:<ul> <li>original run_id</li> <li>continuation run_id</li> <li>...</li> </ul> </li> <li>The step will be skipped based on the status of penultimate run only.</li> </ul> </li> <li> <p>What about many runs trying to write the same file all at once?</p> <ul> <li>Do we keep versions of it or error out?</li> <li>The desired behavior could be:<ul> <li>do the processing.</li> <li>While saving, check for the existence of the file,<ul> <li>If it exists, write a versioned file and error out.</li> <li>If it does not exist, continue successfully.</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/","title":"Reference","text":"<p>Please accompany the reference with  <code>examples</code> from the repo.</p>"},{"location":"reference/#pythontask","title":"PythonTask","text":"sdkyaml <pre><code>An execution node of the pipeline of python functions.\nPlease refer to define pipeline/tasks/python for more information.\n\nAs part of the dag definition, a python task is defined as follows:\n\ndag:\n  steps:\n    python_task: # The name of the node\n      type: task\n      command_type: python # this is default\n      command: my_module.my_function # the dotted path to the function. Please refer to the yaml section of\n        define pipeline/tasks/python for concrete details.\n      returns:\n        - name: # The name to assign the return value\n          kind: json # the default value is json,\n            can be object for python objects and metric for metrics\n      secrets:\n        - my_secret_key # A list of secrets to expose by secrets manager\n      catalog:\n        get:\n          - A list of glob patterns to get from the catalog to the local file system\n        put:\n          - A list of glob patterns to put to the catalog from the local file system\n      on_failure: The name of the step to traverse in case of failure\n      overrides:\n        Individual tasks can override the global configuration config by referring to the\n        specific override.\n\n        For example,\n        #Global configuration\n        executor:\n        type: local-container\n        config:\n          docker_image: \"runnable/runnable:latest\"\n          overrides:\n          custom_docker_image:\n            docker_image: \"runnable/runnable:custom\"\n\n        ## In the node definition\n        overrides:\n        local-container:\n          docker_image: \"runnable/runnable:custom\"\n\n        This instruction will override the docker image for the local-container executor.\n      next: The next node to execute after this task, use \"success\" to terminate the pipeline successfully\n        or \"fail\" to terminate the pipeline with an error.\n</code></pre>"},{"location":"reference/#runnable.PythonTask","title":"runnable.PythonTask","text":"<p>An execution node of the pipeline of python functions. Please refer to concepts for more information.</p> <p>Attributes:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the node.</p> </li> <li> <code>function</code>               (<code>callable</code>)           \u2013            <p>The function to execute.</p> </li> <li> <code>terminate_with_success</code>               (<code>bool</code>)           \u2013            <p>Whether to terminate the pipeline with a success after this node.     Defaults to False.</p> </li> <li> <code>terminate_with_failure</code>               (<code>bool</code>)           \u2013            <p>Whether to terminate the pipeline with a failure after this node.     Defaults to False.</p> </li> <li> <code>on_failure</code>               (<code>str</code>)           \u2013            <p>The name of the node to execute if the step fails.</p> </li> <li> <code>returns</code>               (<code>List[Union[str, TaskReturns]]</code>)           \u2013            <p>A list of the names of variables to return from the task. The names should match the order of the variables returned by the function.</p> <p><code>TaskReturns</code>: can be JSON friendly variables, objects or metrics.</p> <p>By default, all variables are assumed to be JSON friendly and will be serialized to JSON. Pydantic models are readily supported and will be serialized to JSON.</p> <p>To return a python object, please use <code>pickled(&lt;name&gt;)</code>. It is advised to use <code>pickled(&lt;name&gt;)</code> for big JSON friendly variables.</p> <p>For example, <pre><code>from runnable import pickled\n\ndef f():\n    ...\n    x = 1\n    return x, df # A simple JSON friendly variable and a python object.\n\ntask = PythonTask(name=\"task\", function=f, returns=[\"x\", pickled(df)]))\n</code></pre></p> <p>To mark any JSON friendly variable as a <code>metric</code>, please use <code>metric(x)</code>. Metric variables should be JSON friendly and can be treated just like any other parameter.</p> </li> <li> <code>catalog</code>               (<code>Optional[Catalog]</code>)           \u2013            <p>The files sync data from/to, refer to Catalog.</p> </li> <li> <code>secrets</code>               (<code>List[str]</code>)           \u2013            <p>List of secrets to pass to the task. They are exposed as environment variables and removed after execution.</p> </li> <li> <code>overrides</code>               (<code>Dict[str, Any]</code>)           \u2013            <p>Any overrides to the command. Individual tasks can override the global configuration config by referring to the specific override.</p> <p>For example,</p> </li> </ul>"},{"location":"reference/#runnable.PythonTask--global_configuration","title":"Global configuration","text":"<pre><code>executor:\n  type: local-container\n  config:\n    docker_image: \"runnable/runnable:latest\"\n    overrides:\n      custom_docker_image:\n        docker_image: \"runnable/runnable:custom\"\n</code></pre>"},{"location":"reference/#runnable.PythonTask--task_specific_configuration","title":"Task specific configuration","text":"<pre><code>task = PythonTask(name=\"task\", function=\"function'\",\n        overrides={'local-container': custom_docker_image})\n</code></pre>"},{"location":"reference/#notebooktask","title":"NotebookTask","text":"sdkyaml <pre><code>An execution node of the pipeline of notebook execution.\nPlease refer to define pipeline/tasks/notebook for more information.\n\nAs part of the dag definition, a notebook task is defined as follows:\n\ndag:\n  steps:\n    notebook_task: # The name of the node\n      type: task\n      command_type: notebook\n      command: the path to the notebook relative to project root.\n      optional_ploomber_args: a dictionary of arguments to be passed to ploomber engine\n      returns:\n        - name: # The name to assign the return value\n          kind: json # the default value is json,\n            can be object for python objects and metric for metrics\n      secrets:\n        - my_secret_key # A list of secrets to expose by secrets manager\n      catalog:\n        get:\n          - A list of glob patterns to get from the catalog to the local file system\n        put:\n          - A list of glob patterns to put to the catalog from the local file system\n      on_failure: The name of the step to traverse in case of failure\n      overrides:\n        Individual tasks can override the global configuration config by referring to the\n        specific override.\n\n        For example,\n        #Global configuration\n        executor:\n        type: local-container\n        config:\n          docker_image: \"runnable/runnable:latest\"\n          overrides:\n            custom_docker_image:\n              docker_image: \"runnable/runnable:custom\"\n\n        ## In the node definition\n        overrides:\n          local-container:\n            docker_image: \"runnable/runnable:custom\"\n\n        This instruction will override the docker image for the local-container executor.\n      next: The next node to execute after this task, use \"success\" to terminate the pipeline successfully\n        or \"fail\" to terminate the pipeline with an error.\n</code></pre>"},{"location":"reference/#runnable.NotebookTask","title":"runnable.NotebookTask","text":"<p>An execution node of the pipeline of notebook. Please refer to concepts for more information.</p> <p>We internally use Ploomber engine to execute the notebook.</p> <p>Attributes:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the node.</p> </li> <li> <code>notebook</code>               (<code>str</code>)           \u2013            <p>The path to the notebook relative the project root.</p> </li> <li> <code>optional_ploomber_args</code>               (<code>Dict[str, Any]</code>)           \u2013            <p>Any optional ploomber args, please refer to Ploomber engine for more information.</p> </li> <li> <code>terminate_with_success</code>               (<code>bool</code>)           \u2013            <p>Whether to terminate the pipeline with a success after this node.     Defaults to False.</p> </li> <li> <code>terminate_with_failure</code>               (<code>bool</code>)           \u2013            <p>Whether to terminate the pipeline with a failure after this node.     Defaults to False.</p> </li> <li> <code>on_failure</code>               (<code>str</code>)           \u2013            <p>The name of the node to execute if the step fails.</p> </li> <li> <code>returns</code>               (<code>List[Union[str, TaskReturns]]</code>)           \u2013            <p>A list of the names of variables to return from the task. The names should match the order of the variables returned by the function.</p> <p><code>TaskReturns</code>: can be JSON friendly variables, objects or metrics.</p> <p>By default, all variables are assumed to be JSON friendly and will be serialized to JSON. Pydantic models are readily supported and will be serialized to JSON.</p> <p>To return a python object, please use <code>pickled(&lt;name&gt;)</code>. It is advised to use <code>pickled(&lt;name&gt;)</code> for big JSON friendly variables.</p> <p>For example, <pre><code>from runnable import pickled\n\n# assume, example.ipynb is the notebook with df and x as variables in some cells.\n\ntask = Notebook(name=\"task\", notebook=\"example.ipynb\", returns=[\"x\", pickled(df)]))\n</code></pre></p> <p>To mark any JSON friendly variable as a <code>metric</code>, please use <code>metric(x)</code>. Metric variables should be JSON friendly and can be treated just like any other parameter.</p> </li> <li> <code>catalog</code>               (<code>Optional[Catalog]</code>)           \u2013            <p>The files sync data from/to, refer to Catalog.</p> </li> <li> <code>secrets</code>               (<code>List[str]</code>)           \u2013            <p>List of secrets to pass to the task. They are exposed as environment variables and removed after execution.</p> </li> <li> <code>overrides</code>               (<code>Dict[str, Any]</code>)           \u2013            <p>Any overrides to the command. Individual tasks can override the global configuration config by referring to the specific override.</p> <p>For example,</p> </li> </ul>"},{"location":"reference/#runnable.NotebookTask--global_configuration","title":"Global configuration","text":"<pre><code>executor:\n  type: local-container\n  config:\n    docker_image: \"runnable/runnable:latest\"\n    overrides:\n      custom_docker_image:\n        docker_image: \"runnable/runnable:custom\"\n</code></pre>"},{"location":"reference/#runnable.NotebookTask--task_specific_configuration","title":"Task specific configuration","text":"<pre><code>task = NotebookTask(name=\"task\", notebook=\"example.ipynb\",\n        overrides={'local-container': custom_docker_image})\n</code></pre>"},{"location":"reference/#shelltask","title":"ShellTask","text":"sdkyaml <pre><code>An execution node of the pipeline of notebook execution.\nPlease refer to define pipeline/tasks/notebook for more information.\n\nAs part of the dag definition, a notebook task is defined as follows:\n\ndag:\n  steps:\n    notebook_task: # The name of the node\n      type: task\n      command_type: notebook\n      command: the path to the notebook relative to project root.\n      optional_ploomber_args: a dictionary of arguments to be passed to ploomber engine\n      returns:\n        - name: # The name to assign the return value\n          kind: json # the default value is json,\n            can be object for python objects and metric for metrics\n      secrets:\n        - my_secret_key # A list of secrets to expose by secrets manager\n      catalog:\n        get:\n          - A list of glob patterns to get from the catalog to the local file system\n        put:\n          - A list of glob patterns to put to the catalog from the local file system\n      on_failure: The name of the step to traverse in case of failure\n      overrides:\n        Individual tasks can override the global configuration config by referring to the\n        specific override.\n\n        For example,\n        #Global configuration\n        executor:\n        type: local-container\n        config:\n          docker_image: \"runnable/runnable:latest\"\n          overrides:\n            custom_docker_image:\n              docker_image: \"runnable/runnable:custom\"\n\n        ## In the node definition\n        overrides:\n          local-container:\n            docker_image: \"runnable/runnable:custom\"\n\n        This instruction will override the docker image for the local-container executor.\n      next: The next node to execute after this task, use \"success\" to terminate the pipeline successfully\n        or \"fail\" to terminate the pipeline with an error.\n</code></pre>"},{"location":"reference/#runnable.ShellTask","title":"runnable.ShellTask","text":"<p>An execution node of the pipeline of shell script. Please refer to concepts for more information.</p> <p>Attributes:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the node.</p> </li> <li> <code>command</code>               (<code>str</code>)           \u2013            <p>The path to the notebook relative the project root.</p> </li> <li> <code>terminate_with_success</code>               (<code>bool</code>)           \u2013            <p>Whether to terminate the pipeline with a success after this node.     Defaults to False.</p> </li> <li> <code>terminate_with_failure</code>               (<code>bool</code>)           \u2013            <p>Whether to terminate the pipeline with a failure after this node.     Defaults to False.</p> </li> <li> <code>on_failure</code>               (<code>str</code>)           \u2013            <p>The name of the node to execute if the step fails.</p> </li> <li> <code>returns</code>               (<code>List[str]</code>)           \u2013            <p>A list of the names of environment variables to collect from the task.</p> <p>The names should match the order of the variables returned by the function. Shell based tasks can only return JSON friendly variables.</p> <p>To mark any JSON friendly variable as a <code>metric</code>, please use <code>metric(x)</code>. Metric variables should be JSON friendly and can be treated just like any other parameter.</p> </li> <li> <code>catalog</code>               (<code>Optional[Catalog]</code>)           \u2013            <p>The files sync data from/to, refer to Catalog.</p> </li> <li> <code>secrets</code>               (<code>List[str]</code>)           \u2013            <p>List of secrets to pass to the task. They are exposed as environment variables and removed after execution.</p> </li> <li> <code>overrides</code>               (<code>Dict[str, Any]</code>)           \u2013            <p>Any overrides to the command. Individual tasks can override the global configuration config by referring to the specific override.</p> <p>For example,</p> </li> </ul>"},{"location":"reference/#runnable.ShellTask--global_configuration","title":"Global configuration","text":"<pre><code>executor:\n  type: local-container\n  config:\n    docker_image: \"runnable/runnable:latest\"\n    overrides:\n      custom_docker_image:\n        docker_image: \"runnable/runnable:custom\"\n</code></pre>"},{"location":"reference/#runnable.ShellTask--task_specific_configuration","title":"Task specific configuration","text":"<pre><code>task = ShellTask(name=\"task\", command=\"export x=1\",\n        overrides={'local-container': custom_docker_image})\n</code></pre>"},{"location":"reference/#stub","title":"Stub","text":"sdkyaml"},{"location":"reference/#runnable.Stub","title":"runnable.Stub","text":"<p>A node that passes through the pipeline with no action. Just like <code>pass</code> in Python. Please refer to concepts for more information.</p> <p>A stub node can tak arbitrary number of arguments.</p> <p>Attributes:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the node.</p> </li> <li> <code>command</code>               (<code>str</code>)           \u2013            <p>The path to the notebook relative the project root.</p> </li> <li> <code>terminate_with_success</code>               (<code>bool</code>)           \u2013            <p>Whether to terminate the pipeline with a success after this node.     Defaults to False.</p> </li> <li> <code>terminate_with_failure</code>               (<code>bool</code>)           \u2013            <p>Whether to terminate the pipeline with a failure after this node.     Defaults to False.</p> </li> <li> <code>on_failure</code>               (<code>str</code>)           \u2013            <p>The name of the node to execute if the step fails.</p> </li> </ul>"},{"location":"reference/#catalog","title":"Catalog","text":"sdkyaml"},{"location":"reference/#runnable.Catalog","title":"runnable.Catalog","text":"<p>Use to instruct a task to sync data from/to the central catalog. Please refer to concepts for more information.</p> <p>Attributes:</p> <ul> <li> <code>get</code>               (<code>List[str]</code>)           \u2013            <p>List of glob patterns to get from central catalog to the compute data folder.</p> </li> <li> <code>put</code>               (<code>List[str]</code>)           \u2013            <p>List of glob patterns to put into central catalog from the compute data folder.</p> </li> <li> <code>store_copy</code>               (<code>bool</code>)           \u2013            <p>Whether to store a copy of the data in the central catalog.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from runnable import Catalog\n&gt;&gt;&gt; catalog = Catalog(compute_data_folder=\"/path/to/data\", get=[\"*.csv\"], put=[\"*.csv\"])\n</code></pre>"},{"location":"reference/#pipeline","title":"Pipeline","text":"sdkyaml"},{"location":"reference/#runnable.Pipeline","title":"runnable.Pipeline","text":"<p>A Pipeline is a sequence of Steps.</p> <p>Attributes:</p> <ul> <li> <code>steps</code>               (<code>List[Stub | PythonTask | NotebookTask | ShellTask | Parallel | Map]]</code>)           \u2013            <p>A list of Steps that make up the Pipeline.</p> <p>The order of steps is important as it determines the order of execution. Any on failure behavior should the first step in <code>on_failure</code> pipelines.</p> </li> <li> <code>on_failure</code>               (<code>List[List[Pipeline]</code>)           \u2013            <p>A list of Pipelines to execute in case of failure.</p> <p>For example, for the below pipeline:     step1 &gt;&gt; step2     and step1 to reach step3 in case of failure.</p> <pre><code>failure_pipeline = Pipeline(steps=[step1, step3])\n\npipeline = Pipeline(steps=[step1, step2, on_failure=[failure_pipeline])\n</code></pre> </li> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the Pipeline. Defaults to \"\".</p> </li> <li> <code>description</code>               (<code>str</code>)           \u2013            <p>A description of the Pipeline. Defaults to \"\".</p> </li> </ul> <p>The pipeline implicitly add success and fail nodes.</p>"},{"location":"reference/#runnable.Pipeline.execute","title":"execute","text":"<pre><code>execute(configuration_file: str = '', run_id: str = '', tag: str = '', parameters_file: str = '', log_level: str = defaults.LOG_LEVEL)\n</code></pre> <p>Overloaded method: - Could be called by the user when executing the pipeline via SDK - Could be called by the system itself when getting the pipeline definition</p>"},{"location":"reference/#parallel","title":"Parallel","text":"sdkyaml"},{"location":"reference/#runnable.Parallel","title":"runnable.Parallel","text":"<p>A node that executes multiple branches in parallel. Please refer to concepts for more information.</p> <p>Attributes:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the node.</p> </li> <li> <code>branches</code>               (<code>Dict[str, Pipeline]</code>)           \u2013            <p>A dictionary of branches to execute in parallel.</p> </li> <li> <code>terminate_with_failure</code>               (<code>bool</code>)           \u2013            <p>Whether to terminate the pipeline with a failure after this node.</p> </li> <li> <code>terminate_with_success</code>               (<code>bool</code>)           \u2013            <p>Whether to terminate the pipeline with a success after this node.</p> </li> <li> <code>on_failure</code>               (<code>str</code>)           \u2013            <p>The name of the node to execute if any of the branches fail.</p> </li> </ul>"},{"location":"reference/#map","title":"Map","text":"sdkyaml"},{"location":"reference/#runnable.Map","title":"runnable.Map","text":"<p>A node that iterates over a list of items and executes a pipeline for each item. Please refer to concepts for more information.</p> <p>Attributes:</p> <ul> <li> <code>branch</code>               (<code>Pipeline</code>)           \u2013            <p>The pipeline to execute for each item.</p> </li> <li> <code>iterate_on</code>               (<code>str</code>)           \u2013            <p>The name of the parameter to iterate over. The parameter should be defined either by previous steps or statically at the start of execution.</p> </li> <li> <code>iterate_as</code>               (<code>str</code>)           \u2013            <p>The name of the iterable to be passed to functions.</p> </li> <li> <code>reducer</code>               (<code>Callable</code>)           \u2013            <p>The function to reduce the results of the branches.</p> </li> <li> <code>overrides</code>               (<code>Dict[str, Any]</code>)           \u2013            <p>Any overrides to the command.</p> </li> </ul>"},{"location":"usage/","title":"Usage","text":""},{"location":"usage/#installation","title":"Installation","text":"<p>runnable is a python package and should be installed like any other python package. The minimum python version is <code>3.8</code></p> <pre><code>pip install runnable\n</code></pre> <p>We recommend the installation in a virtual environment using <code>poetry</code> or any other package manager.</p>"},{"location":"usage/#extras","title":"Extras","text":"<p>The below extras expand the functionality of <code>runnable</code> to different environments.</p> <p>They can be installed by <code>\"pip install runnable[&lt;extra&gt;]\"</code></p> <ul> <li><code>docker</code> : enable pipelines/jobs in a container</li> <li><code>notebook</code> : enables notebooks as tasks/jobs</li> <li><code>k8s</code> : enables running jobs in kubernetes or minikube clusters</li> <li><code>s3</code> : enables using <code>s3</code> buckets for <code>run log store</code> and <code>catalog</code></li> </ul>"},{"location":"usage/#usage","title":"Usage","text":""},{"location":"usage/#execute_a_pipeline","title":"Execute a pipeline","text":"<p>Pipelines defined in runnable can be either via python sdk or <code>yaml</code> based definitions.</p> <p>The options are detailed below:</p> <pre><code>runnable execute\n\n\u256d\u2500 Arguments \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *    yaml_file      TEXT      The pipeline definition file [default: None] [required]                                                              \u2502\n\u2502      run_id         [RUN_ID]  An optional run_id, one would be generated if its not provided [env var: RUNNABLE_RUN_ID]                            \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --config      -c      TEXT                              The configuration file specifying the services                                             \u2502\n\u2502 --parameters  -p      TEXT                              Parameters, in yaml,  accessible by the application                                        \u2502\n\u2502 --log-level           [INFO|DEBUG|WARNING|ERROR|FATAL]  The log level [default: WARNING]                                                           \u2502\n\u2502 --tag                 TEXT                              A tag attached to the run                                                                  \u2502\n\u2502 --help                                                  Show this message and exit.                                                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"usage/#execute_a_job","title":"Execute a job","text":"<p>Jobs defined in runnable can be via python sdk</p>"},{"location":"usage/#examples","title":"Examples","text":"<p>All the examples in the documentation are present in the <code>examples</code> directory of the repo with instructions on how to run them.</p> <p>All the examples are tested, with multiple configurations, as part of our CI test suite.</p>"},{"location":"why-runnable/","title":"Why runnable?","text":"<p>Obviously, there are a lot of orchestration tools. A well maintained and curated list is available here.</p> <p>Broadly, they could be classed into <code>native</code> or <code>meta</code> orchestrators.</p> <p> </p>"},{"location":"why-runnable/#native_orchestrators","title":"native orchestrators","text":"<ul> <li>Focus on resource management, job scheduling, robustness and scalability.</li> <li>Have less features on domain (data engineering, data science) activities.</li> <li>Difficult to run locally.</li> <li>Not ideal for quick experimentation or research activities.</li> </ul>"},{"location":"why-runnable/#meta_orchestrators","title":"meta orchestrators","text":"<ul> <li>An abstraction over native orchestrators.</li> <li>Oriented towards domain (data engineering, data science) features.</li> <li>Easy to get started and run locally.</li> <li>Ideal for quick experimentation or research activities.</li> </ul> <p><code>runnable</code> is a meta orchestrator with simple API, geared towards data engineering, data science projects. It works in conjunction with native orchestrators and an alternative to kedro or metaflow, in the design philosophy.</p> <p><code>runnable</code> could also function as an SDK for native orchestrators as it always compiles pipeline definitions to native orchestrators.</p> <p></p> <ul> <li> <p> Easy to adopt, its mostly your code</p> <p>Your application code remains as it is. Runnable exists outside of it.</p> <ul> <li>No API's or decorators or any imposed structure.</li> </ul> <p> Getting started</p> </li> <li> <p> Bring your infrastructure</p> <p><code>runnable</code> is not a platform. It works with your platforms.</p> <ul> <li><code>runnable</code> composes pipeline definitions suited to your infrastructure.</li> </ul> <p> Infrastructure</p> </li> <li> <p> Reproducibility</p> <p>Runnable tracks key information to reproduce the execution. All this happens without any additional code.</p> <p> Run Log</p> </li> <li> <p> Retry failues</p> <p>Debug any failure in your local development environment.</p> <p> Retry</p> </li> <li> <p> Testing</p> <p>Unit test your code and pipelines.</p> <ul> <li>mock/patch the steps of the pipeline</li> <li>test your functions as you normally do.</li> </ul> <p> Test</p> </li> <li> <p> Move on</p> <p>Moving away from runnable is as simple as deleting relevant files.</p> <ul> <li>Your application code remains as it is.</li> </ul> </li> </ul>"},{"location":"why-runnable/#comparisons","title":"Comparisons","text":"<p>To simplify the core of <code>runnable</code>,  consider the following function:</p> <pre><code>def func(x: int, y: pd.DataFrame):\n    # Access some data, input.csv\n    # do something with the inputs.\n    # Write a file called output.csv for downstream steps.\n    # return an output.\n    return z\n</code></pre> <p>It takes</p> <ul> <li>inputs x (integer) and y (a pandas dataframe or any other object),</li> <li>processes input data, input.csv expected on local file system</li> <li>writes a file, output.csv to local filesystem</li> <li>returns z (a simple datatype or object)</li> </ul> <p>The function in wrapped in runnable as:</p> <pre><code>from somewhere import func\nfrom runnable import PythonTask, pickled, Catalog\n\n# instruction to get input.csv from catalog at the start of the step.\n# and move output.csv to the catalog at the end of the step\ncatalog = Catalog(get=[\"input.csv\"], put=[\"output.csv\"])\n\n# Call the function, func and expect it to return \"z\" while moving the files\n# It is expected that \"x\" and \"y\" are parameters set by some upstream step.\n# If the return parameter is an object, use pickled(\"z\")\nfunc_task = PythonTask(name=\"function\", function=func, returns=[\"z\"], catalog=catalog)\n</code></pre> <p>Briefly:</p> <ol> <li>The function remains the same as written</li> <li>The required data sets are put in place for the function execution</li> <li>The required input parameters are inspected and passed in from the available parameters</li> <li>After the function call, the return parameters are added to the parameter space</li> <li>The processed data is stored for future use.</li> </ol> <p>Below are the implementations in alternative frameworks. Note that the below are the best of our understanding of the frameworks, please let us know if there are better implementations.</p> <p>Along with the observations,</p> <ul> <li>We have implemented MNIST example in pytorch in multiple frameworks for easier practical comparison.</li> <li>The tutorials are inspired from tutorials of popular frameworks to give a flavor of <code>runnable</code>.</li> </ul>"},{"location":"why-runnable/#metaflow","title":"metaflow","text":"<p>The function in metaflow's step would roughly be:</p> <pre><code>from metaflow import step, conda, FlowSpec\n\nclass Flow(FlowSpec)\n\n    @conda(libraries={...})\n    @step\n    def func_step(self):\n        from somewhere import func\n        self.z = func(self.x, self.y)\n\n        # Use metaflow.S3 to move files\n        # Move to next step.\n        ...\n</code></pre> <p>Though the philosophy is similar, there are some implementation differences in:</p> <ol> <li>Dependency management - metaflow requiring decorators while runnable works in the project environment</li> <li>Dataflow - runnable moves data in and out via the configuration while in metaflow the user is expected to write code.</li> <li>Support for notebooks - runnable allows notebooks to be steps.</li> <li>Platform vs package - runnable is a package while metaflow takes a platform perspective</li> </ol>"},{"location":"why-runnable/#kedro","title":"kedro","text":"<p>The function in <code>kedro</code> implementation would roughly be:</p> <p>Note that any movement of files should happen via data catalog.</p> <pre><code>from kedro.pipeline import Pipeline, node, pipeline\nfrom somewhere import func\n\ndef create_pipeline(**kwargs) -&gt; Pipeline:\n    return pipeline(\n        [\n            node(\n                func=func,\n                inputs=[\"params:x\", \"y\"],\n                outputs=[\"z\"],\n                name=\"my_function\",\n            ),\n            ...\n        ]\n    )\n</code></pre> <p><code>kedro</code> has a larger footprint in the domain code by the configuration files. It imposes a structure and code organization while runnable does not have an opinion on the code structure.</p> <p><code>runnable</code> supports notebooks, dynamic pipelines while kedro lacks support for these.</p>"},{"location":"concepts/catalog/","title":"Catalog","text":"<p>tasks might also need to pass <code>files</code> between them.</p>"},{"location":"concepts/catalog/#concept","title":"Concept","text":"<p>For example:</p> <pre><code>def generate():\n    with open(\"data.csv\", \"w\"):\n        # write content\n        ...\n\ndef consume():\n    with open(\"data.csv\", \"r\"):\n        # read content\n        ...\n\ngenerate()\nconsume()\n</code></pre>"},{"location":"concepts/catalog/#syntax","title":"Syntax","text":"<p>The same can be represented in <code>runnable</code> as catalog.</p> <p>For example, the above snippet would be:</p> sdkyaml <pre><code>from runnable import PythonTask, Pipeline, Catalog\n\nwrite_catalog = Catalog(put=[\"data.csv\"])\nread_catalog = Catalog(get=[\"read.csv\"])\n\ngenerate_task = PythonTask(name=\"generate\", function=generate, catalog=write_catalog)\nconsume_task = PythonTask(name=\"consume\", function=consume, catalog=read_catalog)\n\npipeline = Pipeline(steps=[generate_task, consume_task])\npipeline.execute()\n</code></pre> <pre><code>dag:\n  start_at: generate_data\n  steps:\n    generate:\n      type: task\n      command: examples.common.functions.write_files\n      catalog:\n        put:\n          - data.csv\n      next: consume\n    consume:\n      type: task\n      command_type: python\n      command: examples.common.functions.read_files\n      catalog:\n        get:\n          - df.csv\n          - data_folder/data.txt\n      next: success\n    success:\n      type: success\n    fail:\n        type: fail\n</code></pre>"},{"location":"concepts/catalog/#example","title":"Example","text":"sdkyaml <pre><code>\"\"\"\nDemonstrates moving files within tasks.\n\n- generate_data: creates df.csv and data_folder/data.txt\n\n- delete_local_after_generate: deletes df.csv and data_folder/data.txt\n    This step ensures that the local files are deleted after the step\n\n- read_data_py: reads df.csv and data_folder/data.txt\n\n- delete_local_after_python_get: deletes df.csv and data_folder/data.txt\n    This step ensures that the local files are deleted after the step\n\n- read_data_shell: reads df.csv and data_folder/data.txt\n\n- delete_local_after_shell_get: deletes df.csv and data_folder/data.txt\n    This step ensures that the local files are deleted after the step\n\n- read_data_notebook: reads df.csv and data_folder/data.txt\n\n- delete_local_after_notebook_get: deletes df.csv and data_folder/data.txt\n\nUse this pattern to move files that are not dill friendly.\n\nAll the files are stored in catalog.\n\nRun this pipeline as:\n    python examples/04-catalog/catalog.py\n\nYou can execute this pipeline by:\n\n    python examples/04-catalog/catalog.py\n\"\"\"\n\nfrom examples.common.functions import read_files, write_files\nfrom runnable import Catalog, NotebookTask, Pipeline, PythonTask, ShellTask\n\n\ndef main():\n    write_catalog = Catalog(put=[\"df.csv\", \"data_folder/data.txt\"])\n    generate_data = PythonTask(\n        name=\"generate_data\",\n        function=write_files,\n        catalog=write_catalog,\n    )\n\n    delete_files_command = \"\"\"\n        rm df.csv || true &amp;&amp; \\\n        rm data_folder/data.txt || true\n    \"\"\"\n    # delete from local files after generate\n    # since its local catalog, we delete to show \"get from catalog\"\n    delete_local_after_generate = ShellTask(\n        name=\"delete_after_generate\",\n        command=delete_files_command,\n    )\n\n    read_catalog = Catalog(get=[\"df.csv\", \"data_folder/data.txt\"])\n    read_data_python = PythonTask(\n        name=\"read_data_py\",\n        function=read_files,\n        catalog=read_catalog,\n    )\n\n    delete_local_after_python_get = ShellTask(\n        name=\"delete_after_generate_python\",\n        command=delete_files_command,\n    )\n\n    read_data_shell_command = \"\"\"\n    (ls df.csv &gt;&gt; /dev/null 2&gt;&amp;1 &amp;&amp; echo yes) || exit 1 &amp;&amp; \\\n    (ls data_folder/data.txt &gt;&gt; /dev/null 2&gt;&amp;1 &amp;&amp; echo yes) || exit 1\n    \"\"\"\n    read_data_shell = ShellTask(\n        name=\"read_data_shell\",\n        command=read_data_shell_command,\n        catalog=read_catalog,\n    )\n\n    delete_local_after_shell_get = ShellTask(\n        name=\"delete_after_generate_shell\",\n        command=delete_files_command,\n    )\n\n    read_data_notebook = NotebookTask(\n        notebook=\"examples/common/read_files.ipynb\",\n        name=\"read_data_notebook\",\n        catalog=read_catalog,\n    )\n\n    delete_local_after_notebook_get = ShellTask(\n        name=\"delete_after_generate_notebook\",\n        command=delete_files_command,\n        terminate_with_success=True,\n    )\n\n    pipeline = Pipeline(\n        steps=[\n            generate_data,\n            delete_local_after_generate,\n            read_data_python,\n            delete_local_after_python_get,\n            read_data_shell,\n            delete_local_after_shell_get,\n            read_data_notebook,\n            delete_local_after_notebook_get,\n        ]\n    )\n    _ = pipeline.execute()\n\n    return pipeline\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <pre><code>\n</code></pre>"},{"location":"concepts/job_intro/","title":"Job","text":"<p>Jobs are isolated unit of work which can be python functions, jupyter notebooks or shell scripts.</p> <p>Considering a simple function:</p> <pre><code>def add_numbers(x: int, y: int):\n    # save some data in data.csv\n    return x + y\n</code></pre> <p>The runnable representation of it is:</p> <pre><code>from functions import add_numbers\nfrom runnable import PythonJob, Catalog\n\nwrite_catalog = Catalog(put=[\"data.csv\"])\njob = PythonJob(function=add_numbers,\n                returns[\"sum_of_numbers\"],\n                catalog=write_catalog,\n            )\n</code></pre> <p><code>PythonJob</code> requires a function to call. The input parameters are passed in  from the parameters provided at the time of execution.</p> <p>The return parameters are stored for future reference. Any data object generated in the process can be saved to the catalog.</p>"},{"location":"concepts/job_intro/#python_functions","title":"Python functions","text":"<p>You can use Python functions as jobs in a pipeline, enabling flexible encapsulation of logic, parameter passing, result capturing, and cataloging of outputs.</p> Basic Python Function as a JobWriting Data to the CatalogPassing and Returning Parameters <pre><code>\"\"\"\nYou can execute this pipeline by:\n\n    python examples/01-tasks/python_tasks.py\n\nThe stdout of \"Hello World!\" would be captured as execution\nlog and stored in the catalog.\n\nAn example of the catalog structure:\n\n.catalog\n\u2514\u2500\u2500 baked-heyrovsky-0602\n    \u2514\u2500\u2500 hello.execution.log\n\n2 directories, 1 file\n\n\nThe hello.execution.log has the captured stdout of \"Hello World!\".\n\"\"\"\n\nfrom examples.common.functions import hello\nfrom runnable import PythonJob\n\n\ndef main():\n    job = PythonJob(function=hello)\n\n    job.execute()\n\n    return job\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>The stdout (e.g., \"Hello World!\") and logs are captured and stored in the catalog for traceability.</p> <pre><code>from examples.common.functions import write_files\nfrom runnable import Catalog, PythonJob\n\n\ndef main():\n    write_catalog = Catalog(put=[\"df.csv\", \"data_folder/data.txt\"])\n    job = PythonJob(\n        function=write_files,\n        catalog=write_catalog,\n    )\n\n    job.execute()\n\n    return job\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>The <code>Catalog</code> object specifies which files or data should be saved after job execution.</p> <pre><code>\"\"\"\nThe below example shows how to set/get parameters in python\ntasks of the pipeline.\n\nThe function, set_parameter, returns\n    - JSON serializable types\n    - pydantic models\n    - pandas dataframe, any \"object\" type\n\npydantic models are implicitly handled by runnable\nbut \"object\" types should be marked as \"pickled\".\n\nUse pickled even for python data types is advised for\nreasonably large collections.\n\nRun the below example as:\n    python examples/03-parameters/passing_parameters_python.py\n\n\"\"\"\n\nfrom examples.common.functions import write_parameter\nfrom runnable import PythonJob, metric, pickled\n\n\ndef main():\n    job = PythonJob(\n        function=write_parameter,\n        returns=[\n            pickled(\"df\"),\n            \"integer\",\n            \"floater\",\n            \"stringer\",\n            \"pydantic_param\",\n            metric(\"score\"),\n        ],\n    )\n\n    job.execute()\n\n    return job\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Parameters can be passed at execution time, and returned values can be automatically handled, serialized, and tracked as metrics.</p>"},{"location":"concepts/job_intro/#notebooks","title":"Notebooks","text":"<p>You can also use Jupyter notebooks as jobs in your pipeline. This allows you to encapsulate notebook logic, capture outputs, and integrate notebooks seamlessly into your workflow.</p> Notebook as a Job <p><pre><code>\"\"\"\nYou can execute this pipeline by:\n\n    python examples/11-jobs/notebooks.py\n\nThe output of the notebook will be captured as execution\nlog and stored in the catalog.\n\n\n\"\"\"\n\nfrom runnable import NotebookJob\n\n\ndef main():\n    job = NotebookJob(\n        notebook=\"examples/common/simple_notebook.ipynb\",\n    )\n\n    job.execute()\n\n    return job\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> The output of the notebook will be captured as execution log along with the actual notebook and stored in the catalog for traceability.</p>"},{"location":"concepts/job_intro/#shell_script","title":"Shell script","text":"<p>You can also use shell scripts or commands as jobs in your pipeline. This allows you to execute any shell command, capture its output, and integrate it into your workflow.</p> Shell Script <p><pre><code>\"\"\"\nYou can execute this pipeline by:\n\n    python examples/01-tasks/scripts.py\n\nThe command can be anything that can be executed in a shell.\nThe stdout/stderr of the execution is captured as execution log and stored in the catalog.\n\nFor example:\n\n.catalog\n\u2514\u2500\u2500 seasoned-perlman-1355\n    \u2514\u2500\u2500 hello.execution.log\n\n\"\"\"\n\nfrom runnable import ShellJob\n\n\ndef main():\n    # If this step executes successfully, the pipeline will terminate with success\n    job = ShellJob(command=\"echo 'Hello World!'\")\n\n    job.execute()\n\n    return job\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> The stdout and stderr of the shell command are captured as execution log and stored in the catalog for traceability.</p> <p>For more advanced examples, see the files in <code>examples/11-jobs/</code>.</p>"},{"location":"concepts/map/","title":"Map","text":"<p><code>map</code> nodes in runnable allows to execute a pipeline for all the items in a list.</p>"},{"location":"concepts/map/#concept","title":"Concept","text":"<p>A relatable example from data science would be doing a grid search over hyper parameters where the training pipeline needs to run on every hyper parameter.</p> <pre><code>    flowchart TD\n    gridSearch([Grid Search]):::green\n    success([Success]):::green\n\n    subgraph one[Parameter 1]\n        process_chunk1([Train model]):::yellow\n        success_chunk1([Success]):::yellow\n\n        process_chunk1 --&gt; success_chunk1\n    end\n\n    subgraph two[Parameter ...]\n        process_chunk2([Train model]):::yellow\n        success_chunk2([Success]):::yellow\n\n        process_chunk2 --&gt; success_chunk2\n    end\n\n    subgraph three[Parameter n]\n        process_chunk3([Train model]):::yellow\n        success_chunk3([Success]):::yellow\n\n        process_chunk3 --&gt; success_chunk3\n    end\n\n    reduce([Reduce]):::green\n\n\n    gridSearch --&gt; one --&gt; reduce\n    gridSearch --&gt; two --&gt; reduce\n    gridSearch --&gt; three --&gt; reduce\n    reduce --&gt; success\n\n    classDef yellow stroke:#FFFF00\n    classDef green stroke:#0f0</code></pre> <p>The <code>reduce</code> step is part of the <code>map</code>  state definition.</p> <p>API Documentation</p> <p>Conceptually, map node can be represented in python as:</p> <pre><code>for i in iterable_parameter:\n    # a pipeline of steps\n    x = execute_first_step(i)\n    score = execute_second_step(i, x)\n\nreduce(score) # could be as simple as a list of scores indexed by i or a custom reducer function/lambda\n...\n</code></pre>"},{"location":"concepts/map/#syntax","title":"Syntax","text":"<p>The <code>runnable</code> syntax for the above example:</p> sdkyaml <pre><code>from runnable import PythonTask, Map, Pipeline\n\ndef execute_first_step(i): # (1)\n    ...\n\n    return x # (2)\n\ndef execute_second_step(i, x): # (3)\n    ...\n\ndef get_iterable_branch(): # (4)\n    first_step_task = PythonTask(name=\"execute_first_step\",\n                    function=\"execute_first_step\",\n                    returns=[\"x\"])\n\n    second_step_task = PythonTask(name=\"execute_second_step\",\n                    function=\"execute_second_step\",\n                    terminate_with_success=True)\n\n    pipeline = Pipeline(steps=[first_step_task,second_step_task])\n\ndef main():\n    generate_task = PythonTask(name=\"generate_task\",\n                        function=\"generate\",\n                        returns=[\"iterable_parameter\"]) # (5)\n\n    iterate_task = Map(name=\"iterate\",\n                    branch=get_iterable_branch(),\n                    iterate_on=\"iterable_parameter\", # (6)\n                    iterate_as=\"i\",\n                    terminate_with_success=True) # (7)\n\n    pipeline = Pipeline(steps=[generate_task, iterate_task])\n\n    pipeline.execute()\n    return pipeline\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <ol> <li>Takes in an input parameter <code>i</code>, the current value of the iteration.</li> <li>returns a parameter <code>x</code>.</li> <li><code>i</code> is the current value of iteration, <code>x</code> is the return parameter of function call at iteration <code>i</code>.</li> <li>returns a <code>pipeline</code> whose tasks are dependent on an iterable <code>i</code></li> <li>returns the parameter <code>iterable_parameter</code>.</li> <li>loop over <code>iterable_parameter</code> executing <code>iterable_branch</code> over each value of <code>i</code>.</li> <li>Present <code>i</code> as input argument to all tasks of <code>iterable_branch</code>.</li> </ol> <pre><code>branch: &amp;branch # (1)\nstart_at: execute_first_step\nsteps:\n  execute_first_step: # (2)\n    type: task\n    command: execute_first_step\n    next: execute_second_step\n    returns:\n      - x # (3)\n  execute_second_step:\n    type: task\n    command: execute_second_step # (4)\n    next: success\n\n\ndag:\nstart_at: generate_task\nsteps:\n  generate_task:\n    type: task\n    command: generate\n    returns:\n      - iterable_parameter # (5)\n  iterate_task:\n    type: map\n    branch: *branch # (6)\n    iterate_on: iterable_parameter # (7)\n    iterate_as: i # (8)\n    next: success\n</code></pre> <ol> <li>The pipeline to iterate over an iterable parameter</li> <li>The <code>task</code> expects <code>i</code>, the current value of iteration.</li> <li>The <code>task</code> returns <code>x</code>.</li> <li>The <code>task</code> expects <code>i</code>, the current value of iteration and <code>x</code> at the current iteration.</li> <li>returns a iterable, <code>iterable_parameter</code>.</li> <li>the branch to iterate over</li> <li>the parameter to iterate on, returned by a task <code>generate_task</code>.</li> <li>present the current value of iteration as <code>i</code> to all the tasks of the branch.</li> </ol>"},{"location":"concepts/map/#reduce","title":"Reduce","text":""},{"location":"concepts/map/#default_behavior","title":"Default behavior","text":"<p>The returns of the tasks of the iterable branch are reduced to a list indexed by the order of <code>iterable</code>. In the above example, there would be <code>parameter</code> available for downstream steps of <code>iterate_task</code> that is a list of all <code>x</code>s observed during the iteration.</p> <p>For clarity, the default reducer is: <code>lambda *x: list(x)  # returns a list of the args</code></p>"},{"location":"concepts/map/#custom_reduce","title":"Custom reduce","text":"<p>The <code>map</code> state also accepts a argument <code>reducer</code> which could be a <code>lambda</code> or <code>function</code> that accepts <code>*args</code> (a non-keyword variable length argument list) and returns a reduced value. The downstream steps of <code>iterate_task</code> would use the reduced value.</p>"},{"location":"concepts/map/#traversal","title":"Traversal","text":"<p>A branch of a map step is considered success only if the <code>success</code> step is reached at the end. The steps of the pipeline can fail and be handled by on failure and redirected to <code>success</code> if that is the desired behavior.</p> <p>The map step is considered successful only if all the branches of the step have terminated successfully.</p>"},{"location":"concepts/map/#complete_example","title":"Complete example","text":"Default reducerCustom reducer <p>Uses the default reducer</p> sdkyaml <pre><code>\"\"\"\nYou can execute this pipeline by:\n\n    python examples/07-map/map.py\n\"\"\"\n\nfrom examples.common.functions import (\n    assert_default_reducer,\n    process_chunk,\n    read_processed_chunk,\n)\nfrom runnable import Map, NotebookTask, Pipeline, PythonTask, ShellTask, Stub\n\n\ndef iterable_branch(execute: bool = True):\n    \"\"\"\n    Use the pattern of using \"execute\" to control the execution of the pipeline.\n\n    The same pipeline can be run independently from the command line.\n\n    WARNING: If the execution is not controlled by \"execute\", the pipeline will be executed\n    even during the definition of the branch in parallel steps.\n    \"\"\"\n    # The python function to process a single chunk of data.\n    # In the example, we are multiplying the chunk by 10.\n    process_chunk_task_python = PythonTask(\n        name=\"execute_python\",\n        function=process_chunk,\n        returns=[\"processed_python\"],\n    )\n\n    # return parameters within a map branch have to be unique\n    # The notebook takes in the value of processed_python as an argument.\n    # and returns a new parameter \"processed_notebook\" which is 10*processed_python\n    process_chunk_task_notebook = NotebookTask(\n        name=\"execute_notebook\",\n        notebook=\"examples/common/process_chunk.ipynb\",\n        returns=[\"processed_notebook\"],\n    )\n\n    # following the pattern, the shell takes in the value of processed_notebook as an argument.\n    # and returns a new parameter \"processed_shell\" which is 10*processed_notebook.\n    shell_command = \"\"\"\n    if [ \"$processed_python\" = $( expr 10 '*' \"$chunk\" ) ] \\\n        &amp;&amp; [ \"$processed_notebook\" = $( expr 10 '*' \"$processed_python\" ) ] ; then\n            echo \"yaay\"\n        else\n            echo \"naay\"\n            exit 1;\n    fi\n    export processed_shell=$( expr 10 '*' \"$processed_notebook\")\n    \"\"\"\n\n    process_chunk_task_shell = ShellTask(\n        name=\"execute_shell\",\n        command=shell_command,\n        returns=[\"processed_shell\"],\n    )\n\n    # A downstream step of process_&lt;python, notebook, shell&gt; which reads the parameter \"processed\".\n    # The value of processed is within the context of the branch.\n    # For example, for chunk=1, the value of processed_python is chunk*10 = 10\n    # the value of processed_notebook is processed_python*10 = 100\n    # the value of processed_shell is processed_notebook*10 = 1000\n    read_chunk = PythonTask(\n        name=\"read processed chunk\",\n        function=read_processed_chunk,\n        terminate_with_success=True,\n    )\n\n    pipeline = Pipeline(\n        steps=[\n            process_chunk_task_python,\n            process_chunk_task_notebook,\n            process_chunk_task_shell,\n            read_chunk,\n        ],\n    )\n\n    if execute:\n        pipeline.execute()\n\n    return pipeline\n\n\ndef main():\n    # Create a map state which iterates over a list of chunks.\n    # chunk is the value of the iterable.\n    map_state = Map(\n        name=\"map_state\",\n        iterate_on=\"chunks\",\n        iterate_as=\"chunk\",\n        branch=iterable_branch(execute=False),\n    )\n\n    # Outside of the loop, processed is a list of all the processed chunks.\n    # This is also called as the reduce pattern.\n    # the value of processed_python is [10, 20, 30]\n    # the value of processed_notebook is [100, 200, 300]\n    # the value of processed_shell is [1000, 2000, 3000]\n    collect = PythonTask(\n        name=\"collect\",\n        function=assert_default_reducer,\n    )\n\n    continue_to = Stub(name=\"continue to\")\n\n    pipeline = Pipeline(steps=[map_state, collect])\n\n    pipeline.execute(parameters_file=\"examples/common/initial_parameters.yaml\")\n\n    return pipeline\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <pre><code>branch: &amp;branch\n  start_at: execute_python\n  steps:\n    execute_python:\n      type: task\n      command: examples.common.functions.process_chunk\n      returns:\n        - name: processed_python\n      next: execute_notebook\n    execute_notebook:\n      type: task\n      command_type: notebook\n      command: examples/common/process_chunk.ipynb\n      returns:\n        - name: processed_notebook\n      next: execute_shell\n    execute_shell:\n      type: task\n      command_type: shell\n      command: |\n        if [ \"$processed_python\" = $( expr 10 '*' \"$chunk\" ) ] \\\n        &amp;&amp; [ \"$processed_notebook\" = $( expr 10 '*' \"$processed_python\" ) ] ; then\n            echo \"yaay\"\n        else\n            echo \"naay\"\n            exit 1;\n        fi\n        export processed_shell=$( expr 10 '*' \"$processed_notebook\")\n      returns:\n        - name: processed_shell\n      next: read_chunk\n    read_chunk:\n      type: task\n      command: examples.common.functions.read_processed_chunk\n      next: success\n\n\ndag:\n  description: |\n    map states allows to repeat a branch for each value of an iterable.\n\n    The below example can written, in python, as:\n\n    chunks = [1, 2, 3]\n\n    for chunk in chunks:\n        # Any task within the pipeline can access the value of chunk as an argument.\n        processed = process_chunk(chunk)\n\n        # The value of processed for every iteration is the value returned by the steps\n        # of the current execution. For example, the value of processed\n        # for chunk=1, is chunk*10 = 10 for downstream steps.\n        read_processed_chunk(chunk, processed)\n\n    # Outside of loop, processed is a list of all the processed chunks.\n    # This is also called as the reduce pattern.\n    assert processed == [chunk * 10 for chunk in chunks]\n\n    Run this pipeline as:\n      runnable execute -f examples/07-map/map.yaml \\\n      -p examples/common/initial_parameters.yaml\n\n  start_at: map_state\n  steps:\n    map_state:\n      type: map\n      branch: *branch\n      iterate_on: chunks\n      iterate_as: chunk\n      next: collect\n    collect:\n      type: task\n      command: examples.common.functions.assert_default_reducer\n      next: success\n</code></pre> <p>Differs from default reducer to a <code>lambda *x: max(x)</code> reducer.</p> sdkyaml <pre><code>\"\"\"\nmap states allows to repeat a branch for each value of an iterable.\n\nThe below example can written, in python, as:\n\nchunks = [1, 2, 3]\n\nfor chunk in chunks:\n    # Any task within the pipeline can access the value of chunk as an argument.\n    processed = process_chunk(chunk)\n\n    # The value of processed for every iteration is the value returned by the steps\n    # of the current execution. For example, the value of processed\n    # for chunk=1, is chunk*10 = 10 for downstream steps.\n    read_processed_chunk(chunk, processed)\n\nIt is possible to use a custom reducer, for example, this reducer is a max of the collection.\n# Once the reducer is applied, processed is reduced to a single value.\nassert processed == max(chunk * 10 for chunk in chunks)\n\nYou can execute this pipeline by:\n\n    python examples/07-map/custom_reducer.py\n\"\"\"\n\nfrom examples.common.functions import (\n    assert_custom_reducer,\n    process_chunk,\n    read_processed_chunk,\n)\nfrom runnable import Map, NotebookTask, Pipeline, PythonTask, ShellTask\n\n\ndef iterable_branch(execute: bool = True):\n    \"\"\"\n    Use the pattern of using \"execute\" to control the execution of the pipeline.\n\n    The same pipeline can be run independently from the command line.\n\n    WARNING: If the execution is not controlled by \"execute\", the pipeline will be executed\n    even during the definition of the branch in parallel steps.\n    \"\"\"\n    # The python function to process a single chunk of data.\n    # In the example, we are multiplying the chunk by 10.\n    process_chunk_task_python = PythonTask(\n        name=\"execute_python\",\n        function=process_chunk,\n        returns=[\"processed_python\"],\n    )\n\n    # return parameters within a map branch have to be unique\n    # The notebook takes in the value of processed_python as an argument.\n    # and returns a new parameter \"processed_notebook\" which is 10*processed_python\n    process_chunk_task_notebook = NotebookTask(\n        name=\"execute_notebook\",\n        notebook=\"examples/common/process_chunk.ipynb\",\n        returns=[\"processed_notebook\"],\n    )\n\n    # following the pattern, the shell takes in the value of processed_notebook as an argument.\n    # and returns a new parameter \"processed_shell\" which is 10*processed_notebook.\n    shell_command = \"\"\"\n    if [ \"$processed_python\" = $( expr 10 '*' \"$chunk\" ) ] \\\n        &amp;&amp; [ \"$processed_notebook\" = $( expr 10 '*' \"$processed_python\" ) ] ; then\n            echo \"yaay\"\n        else\n            echo \"naay\"\n            exit 1;\n    fi\n    export processed_shell=$( expr 10 '*' \"$processed_notebook\")\n    \"\"\"\n\n    process_chunk_task_shell = ShellTask(\n        name=\"execute_shell\",\n        command=shell_command,\n        returns=[\"processed_shell\"],\n    )\n\n    # A downstream step of process_&lt;python, notebook, shell&gt; which reads the parameter \"processed\".\n    # The value of processed is within the context of the branch.\n    # For example, for chunk=1, the value of processed_python is chunk*10 = 10\n    # the value of processed_notebook is processed_python*10 = 100\n    # the value of processed_shell is processed_notebook*10 = 1000\n    read_chunk = PythonTask(\n        name=\"read processed chunk\",\n        function=read_processed_chunk,\n        terminate_with_success=True,\n    )\n\n    pipeline = Pipeline(\n        steps=[\n            process_chunk_task_python,\n            process_chunk_task_notebook,\n            process_chunk_task_shell,\n            read_chunk,\n        ],\n    )\n\n    if execute:\n        pipeline.execute()\n\n    return pipeline\n\n\ndef main():\n    # Create a map state which iterates over a list of chunks.\n    # chunk is the value of the iterable.\n    # Upon completion of the map state, all the parameters of the tasks\n    # within the pipeline will be processed by the reducer.\n    # In this case, the reducer is the max of all the processed chunks.\n    map_state = Map(\n        name=\"map state\",\n        iterate_on=\"chunks\",\n        iterate_as=\"chunk\",\n        reducer=\"lambda *x: max(x)\",\n        branch=iterable_branch(execute=False),\n    )\n\n    collect = PythonTask(\n        name=\"collect\",\n        function=assert_custom_reducer,\n        terminate_with_success=True,\n    )\n\n    pipeline = Pipeline(steps=[map_state, collect])\n\n    pipeline.execute(parameters_file=\"examples/common/initial_parameters.yaml\")\n\n    return pipeline\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <pre><code>branch: &amp;branch\n  start_at: execute_python\n  steps:\n    execute_python:\n      type: task\n      command: examples.common.functions.process_chunk\n      returns:\n        - name: processed_python\n      next: execute_notebook\n    execute_notebook:\n      type: task\n      command_type: notebook\n      command: examples/common/process_chunk.ipynb\n      returns:\n        - name: processed_notebook\n      next: execute_shell\n    execute_shell:\n      type: task\n      command_type: shell\n      command: |\n        if [ \"$processed_python\" = $( expr 10 '*' \"$chunk\" ) ] \\\n        &amp;&amp; [ \"$processed_notebook\" = $( expr 10 '*' \"$processed_python\" ) ] ; then\n            echo \"yaay\"\n        else\n            echo \"naay\"\n            exit 1;\n        fi\n        export processed_shell=$( expr 10 '*' \"$processed_notebook\")\n      returns:\n        - name: processed_shell\n      next: read_chunk\n    read_chunk:\n      type: task\n      command: examples.common.functions.read_processed_chunk\n      next: success\n\ndag:\n  description: |\n    map states allows to repeat a branch for each value of an iterable.\n\n    The below example can written, in python, as:\n\n    chunks = [1, 2, 3]\n\n    for chunk in chunks:\n        # Any task within the pipeline can access the value of chunk as an argument.\n        processed = process_chunk(chunk)\n\n        # The value of processed for every iteration is the value returned by the steps\n        # of the current execution. For example, the value of processed\n        # for chunk=1, is chunk*10 = 10 for downstream steps.\n        read_processed_chunk(chunk, processed)\n\n    It is possible to use a custom reducer, for example, this reducer is a max of the collection.\n    # Once the reducer is applied, processed is reduced to a single value.\n    assert processed == max(chunk * 10 for chunk in chunks)\n\n    Run this pipeline as:\n      runnable execute -f examples/07-map/custom_reducer.yaml \\\n      -p examples/common/initial_parameters.yaml\n  start_at: map_state\n  steps:\n    map_state:\n      type: map\n      branch: *branch\n      iterate_on: chunks\n      iterate_as: chunk\n      reducer: \"lambda *x: max(x)\"\n      next: collect\n    collect:\n      type: task\n      command: examples.common.functions.assert_custom_reducer\n      next: success\n</code></pre>"},{"location":"concepts/nesting/","title":"Nesting","text":"<p>As seen from the definitions of parallel or map, the branches are pipelines themselves. This allows for deeply nested workflows in runnable.</p> <p>Technically there is no limit in the depth of nesting but there are some practical considerations.</p> <ul> <li> <p>Not all workflow engines that runnable can transpile the workflow to support deeply nested workflows. AWS Step functions and Argo workflows support them.</p> </li> <li> <p>Deeply nested workflows are complex to understand and debug during errors.</p> </li> </ul>"},{"location":"concepts/nesting/#example","title":"Example","text":"python sdkyaml <p>You can run this pipeline by <code>python examples/06-parallel/nesting.py</code></p> <pre><code>\"\"\"\nExample to show case nesting of parallel steps.\n\nrunnable does not put a limit on the nesting of parallel steps.\nDeeply nested pipelines can be hard to read and not all\nexecutors support it.\n\nRun this pipeline as:\n    python examples/06-parallel/nesting.py\n\"\"\"\n\nfrom examples.common.functions import hello\nfrom runnable import NotebookTask, Parallel, Pipeline, PythonTask, ShellTask, Stub\n\n\ndef traversal(execute: bool = True):\n    \"\"\"\n    Use the pattern of using \"execute\" to control the execution of the pipeline.\n\n    The same pipeline can be run independently from the command line.\n\n    WARNING: If the execution is not controlled by \"execute\", the pipeline will be executed\n    even during the definition of the branch in parallel steps.\n    \"\"\"\n    stub_task = Stub(name=\"hello stub\")\n\n    python_task = PythonTask(\n        name=\"hello python\",\n        function=hello,\n    )\n\n    shell_task = ShellTask(\n        name=\"hello shell\",\n        command=\"echo 'Hello World!'\",\n    )\n\n    notebook_task = NotebookTask(\n        name=\"hello notebook\",\n        notebook=\"examples/common/simple_notebook.ipynb\",\n        terminate_with_success=True,\n    )\n\n    # The pipeline has a mix of tasks.\n    # The order of execution follows the order of the tasks in the list.\n    pipeline = Pipeline(steps=[stub_task, python_task, shell_task, notebook_task])\n\n    if execute:  # Do not execute the pipeline if we are using it as a branch\n        pipeline.execute()\n\n    return pipeline\n\n\ndef parallel_pipeline(execute: bool = True):\n    parallel_step = Parallel(\n        name=\"parallel step\",\n        terminate_with_success=True,\n        branches={\n            \"branch1\": traversal(execute=False),\n            \"branch2\": traversal(execute=False),\n        },\n    )\n\n    pipeline = Pipeline(steps=[parallel_step])\n\n    if execute:\n        pipeline.execute()\n    return pipeline\n\n\ndef main():\n    # Create a parallel step with parallel steps as branches.\n    parallel_step = Parallel(\n        name=\"nested_parallel\",\n        terminate_with_success=True,\n        branches={\n            \"branch1\": parallel_pipeline(execute=False),\n            \"branch2\": parallel_pipeline(execute=False),\n        },\n    )\n\n    pipeline = Pipeline(steps=[parallel_step])\n    pipeline.execute()\n    return pipeline\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>You can run this pipeline by <code>runnable execute examples/parallel/nesting.yaml</code></p> <pre><code>branch: &amp;simple_branch\n  description: |\n    Use this pattern to define repeatable branch\n\n    This pipeline is similar to one defined in:\n      examples/02-sequential/traversal.yaml\n  start_at: hello stub\n  steps:\n    hello stub:\n      type: stub\n      next: hello python\n    hello python:\n      type: task\n      command_type: python\n      command: examples.common.functions.hello # dotted path to the function.\n      next: hello shell\n    hello shell:\n      type: task\n      command_type: shell\n      command: echo \"Hello World!\" # Command to run\n      next: hello notebook\n    hello notebook:\n      type: task\n      command_type: notebook\n      command: examples/common/simple_notebook.ipynb # The path is relative to the root of the project.\n      next: success\n\n\n# This branch is similar to a branch parallel.yaml\nnested_branch: &amp;nested_branch\n  start_at: parallel_step\n  steps:\n    parallel_step:\n      type: parallel\n      next: success\n      branches:\n        branch1: *simple_branch\n        branch2: *simple_branch\n\n\n# The pipeline of nested parallel branches\ndag:\n  start_at: parallel_step\n  steps:\n    parallel_step:\n      type: parallel\n      next: success\n      branches:\n        branch1: *nested_branch\n        branch2: *nested_branch\n</code></pre>"},{"location":"concepts/parallel/","title":"Parallel","text":"<p><code>parallel</code> node in runnable embed multiple <code>pipeline</code> as branches.</p> <p>API Documentation</p>"},{"location":"concepts/parallel/#concept","title":"Concept","text":"<p>The below diagram shows training a baseline model and CNN model in parallel and picking the best model for inference.</p> <pre><code>    flowchart LR\n\n        getFeatures([Get Features]):::green\n        trainStep(Train Models):::green\n        chooseBest([Evaluate Models]):::green\n        inference([Run Inference]):::green\n        success([Success]):::green\n\n        prepareBase([Prepare for baseline model]):::yellow\n        trainBase([Train XGBoost]):::yellow\n        successBase([success]):::yellow\n        prepareBase --&gt; trainBase --&gt; successBase\n\n        trainCNN([Train CNN model]):::yellow\n        successCNN([CNNModel success]):::yellow\n        trainCNN --&gt; successCNN\n\n\n        getFeatures --&gt; trainStep\n        trainStep --&gt; prepareBase\n        trainStep --&gt; trainCNN\n        successBase --&gt; chooseBest\n        successCNN --&gt; chooseBest\n        chooseBest --&gt; inference\n        inference --&gt; success\n\n\n        classDef yellow stroke:#FFFF00\n        classDef green stroke:#0f0</code></pre> <p>The branch for training the <code>baseline</code> and <code>cnn</code> are pipelines themselves are defined as any other pipeline.</p> <p>The step <code>Train Models</code> is a parallel step that has the <code>branches</code> as the individual pipelines.</p>"},{"location":"concepts/parallel/#syntax","title":"Syntax","text":"sdkyaml <pre><code>from runnable import Pipeline, Parallel\ndef get_baseline_pipeline():\n    ...\n    pipeline = Pipeline(...)\n    return pipeline\n\ndef get_cnn_pipeline():\n    ...\n    pipeline = Pipeline(...)\n    return pipeline\n\ndef main():\n    train_models = Parallel(name=\"train models\",\n                    branches={\n                        'baseline': get_baseline_pipeline,\n                        'cnn': get_cnn_pipeline()\n                    },\n                    terminate_with_success=True)\n    pipeline = Pipeline(steps=[train_models])\n\n    pipeline.execute\n\n    return pipeline\n</code></pre> <pre><code>branch: &amp;baseline\nstart_at: prepare\nsteps:\n    ...\n\nbranch: &amp;cnn\nstart_at: train\nsteps:\n    ...\n\ndag:\ndescription: |\n    This example demonstrates the use of the Parallel step.\n\n    parallel step takes a mapping of branches which are pipelines themselves.\n\nstart_at: parallel_step\nsteps:\n    parallel_step:\n    type: parallel\n    next: success\n    branches:\n      baseline: *baseline\n      cnn: *cnn\n</code></pre> <p>Execution</p> <p>The pipelines of the parallel branch should not execute during the definition of <code>parallel</code> step. In case, you want to execute the individual branches in isolation, use a flag to control it.</p> <p>eg: the functions <code>get_baseline</code> and <code>get_cnn</code> can take a argument <code>execute</code> which is defaulted to True. During the composition of <code>parallel</code> step, pass in execute as False.</p>"},{"location":"concepts/parallel/#traversal","title":"Traversal","text":"<p>A branch of a parallel step is considered success only if the <code>success</code> step is reached at the end. The steps of the pipeline can fail and be handled by on failure and redirected to <code>success</code> if that is the desired behavior.</p> <p>The parallel step is considered successful only if all the branches of the step have terminated successfully.</p>"},{"location":"concepts/parallel/#complete_example","title":"Complete example","text":"sdkyaml <pre><code>\"\"\"\nYou can execute this pipeline by:\n\n    python examples/06-parallel/parallel.py\n\"\"\"\n\nfrom examples.common.functions import hello\nfrom runnable import NotebookTask, Parallel, Pipeline, PythonTask, ShellTask, Stub\n\n\ndef traversal():\n    \"\"\"\n    Use the pattern of using \"execute\" to control the execution of the pipeline.\n\n    The same pipeline can be run independently from the command line.\n\n    WARNING: If the execution is not controlled by \"execute\", the pipeline will be executed\n    even during the definition of the branch in parallel steps.\n    \"\"\"\n    stub_task = Stub(name=\"hello stub\")\n\n    python_task = PythonTask(\n        name=\"hello python\",\n        function=hello,\n    )\n\n    shell_task = ShellTask(\n        name=\"hello shell\",\n        command=\"echo 'Hello World!'\",\n    )\n\n    notebook_task = NotebookTask(\n        name=\"hello notebook\",\n        notebook=\"examples/common/simple_notebook.ipynb\",\n        terminate_with_success=True,\n    )\n\n    # The pipeline has a mix of tasks.\n    # The order of execution follows the order of the tasks in the list.\n    pipeline = Pipeline(steps=[stub_task, python_task, shell_task, notebook_task])\n\n    return pipeline\n\n\ndef main():\n    parallel_step = Parallel(\n        name=\"parallel_step\",\n        branches={\n            \"branch1\": traversal(),\n            \"branch2\": traversal(),\n        },\n    )\n\n    continue_to = Stub(name=\"continue to\")\n\n    pipeline = Pipeline(steps=[parallel_step, continue_to])\n\n    pipeline.execute()\n    return pipeline\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <pre><code>branch: &amp;branch\n  description: |\n    Use this pattern to define repeatable branch\n\n    This pipeline is the same as the one defined in examples/02-sequential/traversal.yaml\n  start_at: hello stub\n  steps:\n    hello stub:\n      type: stub\n      next: hello python\n    hello python:\n      type: task\n      command_type: python\n      command: examples.common.functions.hello # dotted path to the function.\n      next: hello shell\n    hello shell:\n      type: task\n      command_type: shell\n      command: echo \"Hello World!\" # Command to run\n      next: hello notebook\n    hello notebook:\n      type: task\n      command_type: notebook\n      command: examples/common/simple_notebook.ipynb # The path is relative to the root of the project.\n      next: success\n\n\ndag:\n  description: |\n    This example demonstrates the use of the Parallel step.\n\n    parallel step takes a mapping of branches which are pipelines themselves.\n\n    Run this pipeline as:\n      runnable execute -f examples/06-parallel/parallel.yaml\n\n\n  start_at: parallel_step\n  steps:\n    parallel_step:\n      type: parallel\n      next: continue_to\n      branches:\n        branch1: *branch\n        branch2: *branch\n    continue_to:\n      type: stub\n      next: success\n</code></pre>"},{"location":"concepts/parameters/","title":"Parameters","text":"<p><code>parameters</code> are data that can be passed from one <code>task</code> to another.</p>"},{"location":"concepts/parameters/#concept","title":"Concept","text":"<p>For example, in the below snippet, the parameters <code>x</code> and <code>y</code> are passed from <code>generate</code> to <code>consume</code>.</p> <pre><code>x, y = generate() # returns x and y as output\nconsume(x, y) # consumes x, y as input arguments.\n</code></pre> <p>The data types of <code>x</code> and <code>y</code> can be:</p> <ul> <li>JSON serializable: int, string, float, list, dict including pydantic models.</li> <li>Objects: Any dill friendly objects.</li> </ul>"},{"location":"concepts/parameters/#compatibility","title":"Compatibility","text":"<p>Below table summarizes the input/output types of different task types. For ex: notebooks can only take JSON serializable parameters as input but can return json/pydantic/objects.</p> Task Input Output python json, pydantic, object via function arguments json, pydantic, object as <code>returns</code> notebook json via cell tagged with <code>parameters</code> json, pydantic, object  as <code>returns</code> shell json via environment variables json environmental variables as <code>returns</code>"},{"location":"concepts/parameters/#project_parameters","title":"Project parameters","text":"<p>Project parameters can be defined using a <code>yaml</code> file. These parameters can then be over-ridden by tasks of the pipeline.</p> <p>They can also be provided by environment variables prefixed by <code>RUNNABLE_PRM_</code>. Environmental variables over-ride <code>yaml</code> parameters.</p> <p>Type casting</p> <p>Annotating the arguments of python function ensures the right data type of arguments.</p> <p>It is advised to <code>cast</code> the parameters in notebook tasks or shell.</p> yamlenvironment variables <p>Deeply nested yaml objects are supported.</p> <pre><code>integer: 1\nfloater : 3.14\nstringer : hello\npydantic_param:\n  x: 10\n  foo: bar\n\nchunks: [1, 2, 3]\n</code></pre> <p>The yaml formatted parameters can also be defined as:</p> <pre><code>export runnable_PRM_integer=\"1\"\nexport runnable_PRM_floater=\"3.14\"\nexport runnable_PRM_stringer=\"hello\"\nexport runnable_PRM_pydantic_param=\"{'x': 10, 'foo': bar}\"\nexport runnable_PRM_chunks=\"[1, 2, 3]\"\n</code></pre> <p>Parameters defined by environment variables override parameters defined by <code>yaml</code>. This can be useful to do a quick experimentation without changing code.</p>"},{"location":"concepts/parameters/#accessing_parameters","title":"Accessing parameters","text":"pythonnotebook &amp; shell <p>The functions have arguments that correspond to the project parameters.</p> <p>Without annotations for nested params, they are sent in as dictionary.</p> <pre><code>\"\"\"\nThe below example showcases setting up known initial parameters for a pipeline\nof only python tasks\n\nThe initial parameters as defined in the yaml file are:\n    simple: 1\n    complex_param:\n        x: 10\n        y: \"hello world!!\"\n\nrunnable allows using pydantic models for deeply nested parameters and\ncasts appropriately based on annotation. eg: read_initial_params_as_pydantic\n\nIf no annotation is provided, the parameter is assumed to be a dictionary.\neg: read_initial_params_as_json\n\nYou can set the initial parameters from environment variables as well.\neg: Any environment variable prefixed by \"RUNNABLE_PRM_\" will be picked up by runnable\n\nRun this pipeline as:\n    python examples/03-parameters/static_parameters_python.py\n\n\"\"\"\n\nimport os\n\nfrom examples.common.functions import (\n    read_initial_params_as_json,\n    read_initial_params_as_pydantic,\n)\nfrom runnable import Pipeline, PythonTask\n\n\ndef main():\n    \"\"\"\n    Signature of read_initial_params_as_pydantic\n    def read_initial_params_as_pydantic(\n        integer: int,\n        floater: float,\n        stringer: str,\n        pydantic_param: ComplexParams,\n        envvar: str,\n    ):\n    \"\"\"\n    read_params_as_pydantic = PythonTask(\n        function=read_initial_params_as_pydantic,\n        name=\"read_params_as_pydantic\",\n    )\n\n    read_params_as_json = PythonTask(\n        function=read_initial_params_as_json,\n        terminate_with_success=True,\n        name=\"read_params_as_json\",\n    )\n\n    pipeline = Pipeline(\n        steps=[read_params_as_pydantic, read_params_as_json],\n    )\n\n    _ = pipeline.execute(parameters_file=\"examples/common/initial_parameters.yaml\")\n\n    return pipeline\n\n\nif __name__ == \"__main__\":\n    # Any parameter prefixed by \"RUNNABLE_PRM_\" will be picked up by runnable\n    os.environ[\"RUNNABLE_PRM_envvar\"] = \"from env\"\n    main()\n</code></pre> <p>The notebook has cell tagged with <code>parameters</code> which are substituted at run time.</p> <p>The shell script has access to them as environmental variables.</p> <pre><code>\"\"\"\nThe below example showcases setting up known initial parameters for a pipeline\nof notebook and shell based commands.\n\nThe initial parameters as defined in the yaml file are:\n    integer: 1\n    floater : 3.14\n    stringer : hello\n    pydantic_param:\n        x: 10\n        foo: bar\n\nrunnable exposes the nested parameters as dictionary for notebook based tasks\nand as a json string for the shell based tasks.\n\nYou can set the initial parameters from environment variables as well.\neg: Any environment variable prefixed by \"RUNNABLE_PRM_\" will be picked up by runnable\n\n\nRun this pipeline as:\n    python examples/03-parameters/static_parameters_non_python.py\n\"\"\"\n\nfrom runnable import NotebookTask, Pipeline, ShellTask\n\n\ndef main():\n    read_params_in_notebook = NotebookTask(\n        name=\"read_params_in_notebook\",\n        notebook=\"examples/common/read_parameters.ipynb\",\n    )\n\n    shell_command = \"\"\"\n    if [ \"$integer\" = 1 ] \\\n    &amp;&amp; [ \"$floater\" = 3.14 ] \\\n    &amp;&amp; [ \"$stringer\" = \"hello\" ] \\\n    &amp;&amp; [ \"$pydantic_param\" = '{\"x\": 10, \"foo\": \"bar\"}' ]; then\n        echo \"yaay\"\n        exit 0;\n    else\n        echo \"naay\"\n        exit 1;\n    fi\n    \"\"\"\n    read_params_in_shell = ShellTask(\n        name=\"read_params_in_shell\",\n        command=shell_command,\n        terminate_with_success=True,\n    )\n\n    pipeline = Pipeline(\n        steps=[read_params_in_notebook, read_params_in_shell],\n    )\n\n    _ = pipeline.execute(parameters_file=\"examples/common/initial_parameters.yaml\")\n\n    return pipeline\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"concepts/parameters/#access_returns","title":"Access &amp; returns","text":""},{"location":"concepts/parameters/#access","title":"access","text":"<p>The access of parameters returned by upstream tasks is similar to project parameters</p>"},{"location":"concepts/parameters/#returns","title":"returns","text":"<p>Tasks can return parameters which can then be accessed by downstream tasks.</p> <p>The syntax is inspired by:</p> <pre><code>def generate():\n    ...\n    return x, y\n\ndef consume(x, y):\n    ...\n\nx, y = generate() # returns x and y as output\nconsume(x, y) # consumes x, y as input arguments.\n</code></pre> <p>and implemented in <code>runnable</code> as:</p> sdkyaml <pre><code>from runnable import PythonTask\n# The returns syntax can be used for notebook and shell scripts too.\ngenerate_task = PythonTask(function=\"generate\", returns=[\"x\", \"y\"])\nconsume_task = PythonTask(function=\"consume\")\n</code></pre> <pre><code>generate:\ntype: task\ncommand: generate\nnext: consume\nreturns:\n    - name: x\n    - name: y\nconsume:\n...\n</code></pre> <p>order of returns</p> <p>The order of <code>returns</code> should match the order of the python function returning them.</p>"},{"location":"concepts/parameters/#marking_returns_as_metric_or_object","title":"marking returns as <code>metric</code> or <code>object</code>","text":"<p>JSON style parameters can be marked as a <code>metric</code> in python functions, notebook, shell. Metric parameters can be accessed as normal parameters in downstream steps.</p> <p>Returns marked as <code>pickled</code> in python functions, notebook are serialized using <code>dill</code>.</p>"},{"location":"concepts/parameters/#example","title":"Example","text":"<pre><code>import pandas as pd\n\n# Assuming a function return a pandas dataframe and a score\ndef generate():\n    ...\n    return df, score\n\n# Downstream step consuming the df and score\ndef consume(df: pd.Dataframe, score: float):\n    ...\n</code></pre> sdkyaml <pre><code>from runnable import metric, pickled, PythonTask\n\ngenerate_task = PythonTask(function=\"generate\",\n                    returns=[pickled(\"df\"),  # pickle df\n                            metric(\"score\")]) # mark score as metric\n\nconsume_task = PythonTask(function=\"consume\")\n</code></pre> <pre><code>generate:\ntype: task\ncommand: generate\nnext: consume\nreturns:\n    - name: df\n      kind: object\n    - name: score\n      kind: metric\nconsume:\n...\n</code></pre>"},{"location":"concepts/parameters/#complete_example","title":"Complete Example","text":"pythonnotebookshell pythonyaml <pre><code>\"\"\"\nThe below example shows how to set/get parameters in python\ntasks of the pipeline.\n\nThe function, set_parameter, returns\n    - JSON serializable types\n    - pydantic models\n    - pandas dataframe, any \"object\" type\n\npydantic models are implicitly handled by runnable\nbut \"object\" types should be marked as \"pickled\".\n\nUse pickled even for python data types is advised for\nreasonably large collections.\n\nRun the below example as:\n    python examples/03-parameters/passing_parameters_python.py\n\n\"\"\"\n\nfrom examples.common.functions import read_parameter, write_parameter\nfrom runnable import Pipeline, PythonTask, metric, pickled\n\n\ndef main():\n    write_parameters = PythonTask(\n        function=write_parameter,\n        returns=[\n            pickled(\"df\"),\n            \"integer\",\n            \"floater\",\n            \"stringer\",\n            \"pydantic_param\",\n            metric(\"score\"),\n        ],\n        name=\"set_parameter\",\n    )\n\n    read_parameters = PythonTask(\n        function=read_parameter,\n        terminate_with_success=True,\n        name=\"get_parameters\",\n    )\n\n    pipeline = Pipeline(\n        steps=[write_parameters, read_parameters],\n    )\n\n    _ = pipeline.execute()\n\n    return pipeline\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <pre><code>dag:\n  description: |\n    The below example shows how to set/get parameters in python\n    tasks of the pipeline.\n\n    The function, set_parameter, returns\n        - JSON serializable\n        - pydantic models\n        - pandas dataframe, any \"object\" type\n\n    pydantic models are implicitly handled by runnable\n    but \"object\" types should be marked as \"pickled\".\n\n    Use pickled even for python data types is advised for\n    reasonably large collections.\n\n    Run the pipeline as:\n      runnable execute -f examples/03-parameters/passing_parameters_python.yaml\n  start_at: set_parameter\n  steps:\n    set_parameter:\n      type: task\n      command: examples.common.functions.write_parameter\n      returns:\n        - name: df\n          kind: object\n        - name: integer\n        - name: floater\n        - name: stringer\n        - name: pydantic_param\n        - name: score\n          kind: metric\n      next: get_parameters\n    get_parameters:\n      type: task\n      command: examples.common.functions.read_parameter\n      next: success\n</code></pre> <p>To access parameters, the cell should be tagged with <code>parameters</code>. Only JSON style parameters can be injected in.</p> <p>Any python variable defined during the execution of the notebook matching the name in <code>returns</code> is inferred as a parameter. The variable can be either JSON type or objects.</p> pythonyaml <pre><code>\"\"\"\nDemonstrates passing parameters to and from a notebook.\n\nrunnable can extract JSON serializable types, pydantic models, objects from notebook.\neg: write_parameters_from_notebook\n\nBut can only inject JSON type parameters to a notebook.\neg: read_parameters_in_notebook\npydantic parameters are injected as dictionary.\n\nRun the below example as:\n    python examples/03-parameters/passing_parameters_notebook.py\n\n\"\"\"\n\nfrom examples.common.functions import read_parameter\nfrom runnable import NotebookTask, Pipeline, PythonTask, metric, pickled\n\n\ndef main():\n    write_parameters_from_notebook = NotebookTask(\n        notebook=\"examples/common/write_parameters.ipynb\",\n        returns=[\n            pickled(\"df\"),\n            \"integer\",\n            \"floater\",\n            \"stringer\",\n            \"pydantic_param\",\n            metric(\"score\"),\n        ],\n        name=\"set_parameter\",\n    )\n\n    read_parameters = PythonTask(\n        function=read_parameter,\n        name=\"get_parameters\",\n    )\n\n    read_parameters_in_notebook = NotebookTask(\n        notebook=\"examples/common/read_parameters.ipynb\",\n        terminate_with_success=True,\n        name=\"read_parameters_in_notebook\",\n    )\n\n    pipeline = Pipeline(\n        steps=[\n            write_parameters_from_notebook,\n            read_parameters,\n            read_parameters_in_notebook,\n        ],\n    )\n\n    _ = pipeline.execute()\n\n    return pipeline\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <pre><code>dag:\n  description: |\n    Demonstrates passing parameters to and from a notebook.\n\n    runnable can extract JSON serializable types, pydantic models, objects from notebook.\n    eg: write_parameters_from_notebook\n\n    But can only inject JSON type parameters to a notebook.\n    eg: read_parameters_in_notebook\n    pydantic parameters are injected as dictionary.\n\n    Run the below example as:\n        runnable execute examples/03-parameters/passing_parameters_notebook.yaml\n  start_at: set_parameter\n  steps:\n    set_parameter:\n      type: task\n      command_type: notebook\n      command: examples/common/write_parameters.ipynb\n      returns:\n        - name: df\n          kind: object\n        - name: integer\n        - name: floater\n        - name: stringer\n        - name: pydantic_param\n        - name: score\n          kind: metric\n      next: get_parameters\n    get_parameters:\n      type: task\n      command: examples.common.functions.read_parameter\n      next: read_parameters_in_notebook\n    read_parameters_in_notebook:\n      type: task\n      command_type: notebook\n      command: examples/common/read_parameters.ipynb\n      next: success\n</code></pre> <p>Shell tasks can only access/return JSON style parameters</p> pythonyaml <pre><code>\"\"\"\nDemonstrates passing parameters to and from shell scripts.\n\nWe can extract only JSON serializable parameters from shell scripts.\neg: write_parameters_in_shell\n\nWe can only read json style parameters from shell scripts.\neg: read_parameters_in_shell\npydantic parameters are injected as json.\n\nRun the below example as:\n    python examples/03-parameters/passing_parameters_shell.py\n\n\"\"\"\n\nfrom examples.common.functions import read_unpickled_parameter\nfrom runnable import Pipeline, PythonTask, ShellTask, metric\n\n\ndef main():\n    export_env_command = \"\"\"\n    export integer=1\n    export floater=3.14\n    export stringer=\"hello\"\n    export pydantic_param='{\"x\": 10, \"foo\": \"bar\"}'\n    export score=0.9\n    \"\"\"\n    write_parameters_in_shell = ShellTask(\n        command=export_env_command,\n        returns=[\n            \"integer\",\n            \"floater\",\n            \"stringer\",\n            \"pydantic_param\",\n            metric(\"score\"),\n        ],\n        name=\"write_parameter\",\n    )\n\n    read_parameters = PythonTask(\n        function=read_unpickled_parameter,\n        name=\"read_parameters\",\n    )\n\n    read_parameters_command = \"\"\"\n    if [ \"$integer\" = 1 ] \\\n        &amp;&amp; [ \"$floater\" = 3.14 ] \\\n        &amp;&amp; [ \"$stringer\" = \"hello\" ] \\\n        &amp;&amp; [ \"$pydantic_param\" = '{\"x\": 10, \"foo\": \"bar\"}' ]; then\n            echo \"yaay\"\n            exit 0;\n        else\n            echo \"naay\"\n            exit 1;\n    fi\n    \"\"\"\n    read_parameters_in_shell = ShellTask(\n        name=\"read_parameters_in_shell\",\n        command=read_parameters_command,\n        terminate_with_success=True,\n    )\n\n    pipeline = Pipeline(\n        steps=[write_parameters_in_shell, read_parameters, read_parameters_in_shell],\n    )\n\n    _ = pipeline.execute()\n\n    return pipeline\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <pre><code>dag:\n  description: |\n    Demonstrates passing parameters to and from shell scripts.\n\n    We can extract only json style parameters from shell scripts.\n    eg: write_parameters_in_shell\n\n    We can only read json style parameters from shell scripts.\n    eg: read_parameters_in_shell\n    pydantic parameters are injected as json.\n\n    Run the pipeline as:\n      runnable execute -f examples/03-parameters/passing_parameters_shell.yaml\n\n  start_at: write_parameter\n  steps:\n    write_parameter:\n      type: task\n      command_type: shell\n      command: |\n        export integer=1\n        export floater=3.14\n        export stringer=\"hello\"\n        export pydantic_param='{\"x\": 10, \"foo\": \"bar\"}'\n        export score=0.9\n      returns:\n        - name: integer\n        - name: floater\n        - name: stringer\n        - name: pydantic_param\n        - name: score\n      next: read_parameters\n    read_parameters:\n      type: task\n      command: examples.common.functions.read_unpickled_parameter\n      next: read_parameters_in_shell\n    read_parameters_in_shell:\n      type: task\n      command_type: shell\n      command: |\n        if [ \"$integer\" = 1 ] \\\n          &amp;&amp; [ \"$floater\" = 3.14 ] \\\n          &amp;&amp; [ \"$stringer\" = \"hello\" ] \\\n          &amp;&amp; [ \"$pydantic_param\" = '{\"x\": 10, \"foo\": \"bar\"}' ]; then\n              echo \"yaay\"\n              exit 0;\n        else\n              echo \"naay\"\n              exit 1;\n        fi\n      next: success\n</code></pre>"},{"location":"concepts/pipeline/","title":"Pipeline","text":"<p>In runnable, we use the words</p> <ul> <li><code>workflows</code> and <code>pipeline</code> interchangeably.</li> <li><code>nodes</code>, <code>steps</code> interchangeably.</li> </ul> <p>A <code>workflow</code> is a sequence of <code>steps</code> to perform.</p> <p>Composite pipelines</p> <p><code>runnable</code> pipelines are composable. For example, a pipeline can have a parallel node which in itself has many pipelines running in parallel.</p> <p></p>"},{"location":"concepts/pipeline/#concept","title":"Concept","text":"<p>A visual example of a workflow:</p> <pre><code>stateDiagram-v2\n    direction lr\n    state \"hello stub\" as start_at\n    state \"hello python\" as step_2\n    state \"hello notebook\" as step_3\n    state \"hello shell\" as step_4\n    state \"Success\" as success\n\n\n    [*] --&gt; start_at\n    start_at --&gt; step_2 : #9989;\n    step_2 --&gt; step_3 : #9989;\n    step_3 --&gt; step_4 : #9989;\n    step_4 --&gt; success : #9989;\n    success --&gt; [*]</code></pre> Traversal <p>Start at <code>hello stub</code>.</p> <p>If it is successful, go to <code>next</code> step of the pipeline until we reach the success state.</p> <p>Any failure in execution of step would, by default, go to the <code>fail</code> state.</p>"},{"location":"concepts/pipeline/#syntax","title":"Syntax","text":"<p>The above pipeline can be written in runnable as below. It is a mixed bag of python functions, notebook, shell and stub.</p> <p>API Documentation</p> sdkyaml <ul> <li> The first step of the <code>steps</code> is the start of the workflow.</li> <li> The order of execution follows the order of the tasks in the list.</li> <li> The terminal nodes <code>success</code> and <code>fail</code> are added automatically.</li> </ul> <pre><code>\"\"\"\nYou can execute this pipeline by:\n\n    python examples/02-sequential/traversal.py\n\nA pipeline can have any \"tasks\" as part of it. In the\nbelow example, we have a mix of stub, python, shell and notebook tasks.\n\nAs with simpler tasks, the stdout and stderr of each task are captured\nand stored in the catalog.\n\"\"\"\n\nfrom examples.common.functions import hello\nfrom runnable import NotebookTask, Pipeline, PythonTask, ShellTask, Stub\n\n\ndef main():\n    stub_task = Stub(name=\"hello stub\")\n\n    python_task = PythonTask(\n        name=\"hello python\", function=hello, overrides={\"argo\": \"smaller\"}\n    )\n\n    shell_task = ShellTask(\n        name=\"hello shell\",\n        command=\"echo 'Hello World!'\",\n    )\n\n    notebook_task = NotebookTask(\n        name=\"hello notebook\",\n        notebook=\"examples/common/simple_notebook.ipynb\",\n    )\n\n    # The pipeline has a mix of tasks.\n    # The order of execution follows the order of the tasks in the list.\n    pipeline = Pipeline(\n        steps=[  # (2)\n            stub_task,  # (1)\n            python_task,\n            shell_task,\n            notebook_task,\n        ]\n    )\n\n    pipeline.execute()\n\n    return pipeline\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <ol> <li>Start the pipeline.</li> <li>The order of the steps is the execution order</li> </ol> <ul> <li> The first step  is the step corresponding to <code>start_at</code></li> <li> The mapping defined in the steps.</li> <li> The <code>next</code> step after a successful execution of a <code>step</code>.</li> <li> <code>success</code> as <code>next</code> node implies successful execution of the pipeline.</li> </ul> <pre><code>dag:\n  description: |\n    A pipeline can have any \"tasks\" as part of it. In the\n    below example, we have a mix of stub, python, shell and notebook tasks.\n\n    As with simpler tasks, the stdout and stderr of each task are captured\n    and stored in the catalog.\n\n    runnable execute -f examples/02-sequential/traversal.yaml\n\n  start_at: hello stub # (1)\n  steps:\n    hello stub:\n      type: stub\n      next: hello python # (2)\n    hello python:\n      type: task\n      command_type: python\n      command: examples.common.functions.hello # dotted path to the function.\n      next: hello shell\n    hello shell:\n      type: task\n      command_type: shell\n      command: echo \"Hello World!\" # Command to run\n      next: hello notebook\n    hello notebook:\n      type: task\n      command_type: notebook\n      command: examples/common/simple_notebook.ipynb # The path is relative to the root of the project.\n      next: success # (3)\n</code></pre> <ol> <li>Start the pipeline at this step.</li> <li>State the <code>next</code> node, if it succeeds.</li> <li>Add the success and fail nodes.</li> </ol> <p></p>"},{"location":"concepts/pipeline/#on_failure","title":"on failure","text":"<p>By default, any failure during the execution of step will traverse to <code>fail</code> node marking the execution as failed.</p> <p>The <code>fail</code> node is implicitly added to the pipeline.</p> <p>This behavior can be over-ridden to follow a different path based on expected failures.</p>"},{"location":"concepts/pipeline/#on_failure_success","title":"on failure success","text":"<p><code>step 1</code> fails as the function raises an exception.</p> <p><code>step 4</code> is a single node pipeline to execute if <code>step1</code> fails. The failure pipeline can have as many steps as needed.</p> pseudo codesdkyaml <pre><code>try:\n    raise_exception()\nexcept:\n    # suppress exception\n    do_something()\n</code></pre> <pre><code>\"\"\"\nThis pipeline showcases handling failures in a pipeline.\n\nThe path taken if none of the steps failed:\nstep_1 -&gt; step_2 -&gt; step_3 -&gt; success\n\nstep_1 is a python function that raises an exception.\nAnd we can instruct the pipeline to execute step_4 if step_1 fails\nand then eventually succeed too.\nstep_1 -&gt; step_4 -&gt; success\n\nThis pattern is handy when you are expecting a failure of a step\nand have ways to handle it.\n\nCorresponds to:\ntry:\n    step1()  # Raises the exception\n    step2()\n    step3()\nexcept Exception as e:\n    step4()\n\nRun this pipeline:\n    python examples/02-sequential/on_failure_succeed.py\n\"\"\"\n\nfrom examples.common.functions import raise_ex\nfrom runnable import Pipeline, PythonTask, Stub\n\n\ndef main():\n    step_1 = PythonTask(name=\"step_1\", function=raise_ex)  # This will fail\n\n    step_2 = Stub(name=\"step_2\")\n\n    step_3 = Stub(name=\"step_3\")\n\n    on_failure_pipeline = Stub(name=\"step_4\").as_pipeline()\n\n    step_1.on_failure = on_failure_pipeline  # (2)\n\n    pipeline = Pipeline(\n        steps=[step_1, step_2, step_3],\n    )\n    pipeline.execute()\n\n    return pipeline\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <ol> <li><code>terminate_with_success</code> is <code>true</code> traverses to success node.</li> </ol> <pre><code>dag:\n  description: |\n    This pipeline showcases handling failures in a pipeline.\n\n    The path taken if none of the steps failed:\n    step_1 -&gt; step_2 -&gt; step_3 -&gt; success\n\n    step_1 is a python function that raises an exception.\n    And we can instruct the pipeline to execute step_4 if step_1 fails\n    and then eventually fail.\n    step_1 -&gt; step_4 -&gt; success\n\n    This pattern is handy when you are expecting a failure of a step\n    and have ways to handle it.\n\n    Corresponds to:\n    try:\n        step1()  # Raises the exception\n        step2()\n        step3()\n    except Exception as e:\n        step4()\n\n    Run this pipeline as:\n      runnable execute -f examples/02-sequential/on_failure_succeed.yaml\n  start_at: step_1\n  steps:\n    step_1:\n      type: task\n      command_type: shell\n      command: exit 1 # This will fail!\n      next: step_2\n      on_failure: step_4\n    step_2:\n      type: stub\n      next: step_3\n    step_3:\n      type: stub\n      next: success\n    step_4:\n      type: stub\n      next: success\n</code></pre>"},{"location":"concepts/pipeline/#on_failure_fail","title":"On failure fail","text":"<p><code>step 1</code> fails as the function raises an exception.</p> <p><code>step 4</code> is a single node pipeline to execute if <code>step1</code> fails. The failure pipeline can have as many steps as needed.</p> pseudo codesdkyaml <pre><code>try:\n    raise_exception()\nexcept:\n    # raise exception after doing something.\n    do_something()\n    raise\n</code></pre> <pre><code>\"\"\"\nThis pipeline showcases handling failures in a pipeline.\n\nThe path taken if none of the steps failed:\nstep_1 -&gt; step_2 -&gt; step_3 -&gt; success\n\nstep_1 is a python function that raises an exception.\nAnd we can instruct the pipeline to execute step_4 if step_1 fails\nand then eventually fail.\nstep_1 -&gt; step_4 -&gt; fail\n\nThis pattern is handy when you need to do something before eventually\nfailing (eg: sending a notification, updating status, etc...)\n\nCorresponds to:\ntry:\n    step1()  # Raises the exception\n    step2()\n    step3()\nexcept Exception as e:\n    step4()\n    raise e\n\n\nRun this pipeline as:\n    python examples/02-sequential/on_failure_fail.py\n\"\"\"\n\nfrom examples.common.functions import raise_ex\nfrom runnable import Pipeline, PythonTask, Stub\n\n\ndef main():\n    step_1 = PythonTask(name=\"step_1\", function=raise_ex)  # This will fail\n\n    step_2 = Stub(name=\"step_2\")\n\n    step_3 = Stub(name=\"step_3\")\n\n    step_4 = Stub(name=\"step_4\", terminate_with_failure=True).as_pipeline()\n\n    step_1.on_failure = step_4  # (2)\n\n    pipeline = Pipeline(\n        steps=[step_1, step_2, step_3],\n    )\n    pipeline.execute()\n\n    return pipeline\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <pre><code>dag:\n  description: |\n    This pipeline showcases handling failures in a pipeline.\n\n    The path taken if none of the steps failed:\n    step_1 -&gt; step_2 -&gt; step_3 -&gt; success\n\n    step_1 is a python function that raises an exception.\n    And we can instruct the pipeline to execute step_4 if step_1 fails\n    and then eventually fail.\n    step_1 -&gt; step_4 -&gt; fail\n\n    This pattern is handy when you need to do something before eventually\n    failing (eg: sending a notification, updating status, etc...)\n\n    Corresponds to:\n    try:\n        step1()  # Raises the exception\n        step2()\n        step3()\n    except Exception as e:\n        step4()\n        raise e\n\n    Run this pipeline as:\n      runnable execute -f examples/02-sequential/default_fail.yaml\n  start_at: step_1\n  steps:\n    step_1:\n      type: task\n      command_type: shell\n      command: exit 1 # This will fail!\n      next: step_2\n      on_failure: step_4\n    step_2:\n      type: stub\n      next: step_3\n    step_3:\n      type: stub\n      next: success\n    step_4:\n      type: stub\n      next: fail\n</code></pre>"},{"location":"concepts/pipeline_intro/","title":"Pipeline","text":""},{"location":"concepts/pipeline_intro/#pipeline","title":"Pipeline","text":"<p>Without any orchestrator, the simplest pipeline could be the below functions:</p> <pre><code>def generate():\n    ...\n    # write some files, data.csv\n    ...\n    # return objects or simple python data types.\n    return x, y\n\ndef consume(x, y):\n    ...\n    # read from data.csv\n    # do some computation with x and y\n\n\n# Stich the functions together\n# This is the driver pattern.\nx, y = generate()\nconsume(x, y)\n</code></pre>"},{"location":"concepts/pipeline_intro/#runnable_representation","title":"Runnable representation","text":"<p>The workflow in <code>runnable</code> would be:</p> <pre><code>from runnable import PythonTask, pickled, catalog, Pipeline\n\ngenerate_task = PythonTask(name=\"generate\", function=generate,\n                returns=[pickled(\"x\"), y],\n                catalog=Catalog(put=[\"data.csv\"])\n\nconsume_task = PythonTask(name=\"consume\", function=consume,\n                catalog=Catalog(get=[\"data.csv\"])\n\npipeline = Pipeline(steps=[generate_task, consume_task])\npipeline.execute()\n</code></pre> <ul> <li><code>runnable</code> wraps the functions <code>generate</code> and <code>consume</code> as tasks.</li> <li>Tasks can access and return parameters.</li> <li>Tasks can also share files between them using catalog.</li> <li>Tasks are stitched together as pipeline</li> <li>The execution environment is configured via</li> </ul>"},{"location":"concepts/pipeline_intro/#todo_figure_this_link","title":"TODO: figure this link","text":""},{"location":"concepts/pipeline_intro/#examples","title":"Examples","text":"<p>All the concepts are accompanied by examples.</p>"},{"location":"concepts/run-log/","title":"Reproducibility","text":"<p><code>runnable</code> makes reproducibility easy without any intervention from the developer.</p> <p>Execution of the pipeline, in any environment, generates a <code>run log</code> which stores meta information of the execution.</p> <p>Executions also stores the <code>stdout</code> and <code>stderr</code> of the task at the log level used during the run.</p>"},{"location":"concepts/run-log/#code_identity","title":"Code identity","text":"<p>For non-container based executions, the <code>git</code> sha is captured along with other necessary attributes.</p>"},{"location":"concepts/run-log/#example","title":"Example","text":"<pre><code>\"steps\": {\n    \"step_name\": {\n    ...\n    \"code_identities\": [\n        {\n            \"code_identifier\": \"259904152753ccb326ab71804ac6b2f343ee6182\",\n            \"code_identifier_type\": \"git\",\n            \"code_identifier_dependable\": false, // (1)\n            \"code_identifier_url\": \"https://github.com/AstraZeneca/runnable.git\",\n            \"code_identifier_message\": \"changes found in docs/concepts/run-log.md\" // (2)\n        }\n    }\n}\n</code></pre> <ol> <li>Implies that the branch is not clean.</li> <li>Emits all the files that are different from the HEAD.</li> </ol> <p>For container based executions, the container digest and name is captured.</p>"},{"location":"concepts/run-log/#parameters","title":"Parameters","text":"<p>The input and output parameters at the point of execution of all the tasks is captured.</p>"},{"location":"concepts/run-log/#example_1","title":"Example","text":"<pre><code>\"steps\": {\n    \"step_name\":{\n        ...\n        \"attempts\":[\n            {\n                ...\n                \"input_parameters\": {\n                    \"X\": {\n                        \"kind\": \"object\",\n                        \"value\": \"X\",\n                        \"reduced\": true,\n                        \"description\": \"Pickled object stored in catalog as: X\"\n                    },\n                    \"Y\": {\n                        \"kind\": \"object\",\n                        \"value\": \"Y\",\n                        \"reduced\": true,\n                        \"description\": \"Pickled object stored in catalog as: Y\"\n                    }\n                },\n                \"output_parameters\": {\n                    \"logreg\": {\n                        \"kind\": \"object\",\n                        \"value\": \"logreg\",\n                        \"reduced\": true,\n                        \"description\": \"Pickled object stored in catalog as: logreg\"\n                    }\n                }\n            }\n        ]\n    }\n</code></pre>"},{"location":"concepts/run-log/#metrics","title":"Metrics","text":"<p>Any parameters marked as <code>metrics</code> are stored too.</p>"},{"location":"concepts/run-log/#example_2","title":"Example","text":"<pre><code>\"steps\": {\n    \"step_name\":{\n        ...\n        \"attempts\":[\n            {\n                ...\n                \"user_defined_metrics\": {\n                    \"score\": {\n                        \"kind\": \"metric\",\n                        \"value\": 0.6,\n                        \"reduced\": true,\n                        \"description\": 0.6\n                    }\n                }\n            }\n        ]\n    }\n</code></pre>"},{"location":"concepts/run-log/#data","title":"Data","text":"<p>Any <code>files</code> moved between tasks are stored in the <code>catalog</code> along with meta information stored in the <code>run log</code>.</p>"},{"location":"concepts/run-log/#example_3","title":"Example","text":"<pre><code>\"steps\": {\n    \"step_name\":{\n        ...\n        \"data_catalog\": [\n            {\n                \"name\": \"iris_logistic.png\",\n                \"data_hash\": \"1a119ee3496f72d7cdd379b658aa79dc0eee38923d270ef7adf61dcb8f033f06\",\n                \"catalog_relative_path\": \"best-hamilton-0300/iris_logistic.png\",\n                \"catalog_handler_location\": \".catalog\",\n                \"stage\": \"put\"\n            }\n        ]\n    }\n}\n</code></pre>"},{"location":"concepts/run-log/#retrying_failures","title":"Retrying failures","text":"<p>The structure of the run log remains the same independent of the <code>executor</code> used to execute. This enables to debug failures during the execution in complex environments to be easily reproduced in local environments and fixed.</p> <p>Make the <code>catalog</code> and <code>run log</code> generated during the failed execution accessible to the <code>retry</code> executor and the execution starts from the failed step.</p> <p>Refer to retry for more information.</p>"},{"location":"concepts/secrets/","title":"Overview","text":"<p>Opt out</p> <p>Pipelines need not use the <code>secrets</code> if the preferred tools of choice is not implemented in runnable. The default configuration of <code>do-nothing</code> is no-op by design. We kindly request to raise a feature request to make us aware of the eco-system.</p> <p>Most complex pipelines require secrets to hold sensitive information during task execution. They could be database credentials, API keys or any information that need to present at the run-time but invisible at all other times.</p> <p>The secrets are always exposed as environmental variables.</p> <p>A typical example would be a task requiring the database connection string to connect to a database.</p> Using the secrets API<pre><code>class CustomObject:\n\n    @property\n    def connection_object(self):\n        import os\n        connection_string = os.environ.get(\"connection_string\")\n        # Do something with the secrets\n</code></pre> <p>Please refer to configurations for available implementations.</p>"},{"location":"concepts/secrets/#example","title":"Example","text":"dotenv formatExample configuration <p>The dotenv format for providing secrets. Ideally, this file should not be part of the version control but present during development phase.</p> <p>The file is assumed to be present in <code>examples/secrets.env</code> for this example.</p> <p>It follows the same format as python-dotenv</p> <p>Configuration to use the dotenv format file.</p> <pre><code>secrets:\n  type: dotenv # (1)\n  config:\n    location: examples/secrets.env # (2)\n</code></pre> <ol> <li>Use dotenv secrets manager.</li> <li>Location of the dotenv file, defaults to <code>.env</code> in project root.</li> </ol>"},{"location":"concepts/task/","title":"Tasks","text":"<p>Task nodes are the execution units of the pipeline.</p> <p>They can be python functions, notebooks, shell scripts or stubs</p> <p>In the below examples, highlighted lines of the code are the relevant bits while the rest of the python code (or yaml) defines and executes a pipeline that executes the python function/notebook/shell script/stubs.</p>"},{"location":"concepts/task/#python_functions","title":"Python functions","text":"<p>Uses python functions as tasks.</p> <p>API Documentation</p>"},{"location":"concepts/task/#example","title":"Example","text":"sdkyaml <p>Structuring</p> <p>It is best to keep the application specific functions in a different module than the pipeline definition, if you are using Python SDK.</p> <pre><code>\"\"\"\nYou can execute this pipeline by:\n\n    python examples/01-tasks/python_tasks.py\n\nThe stdout of \"Hello World!\" would be captured as execution\nlog and stored in the catalog.\n\"\"\"\n\nfrom examples.common.functions import hello\nfrom runnable import Pipeline, PythonTask\n\n\ndef main():\n    # Create a tasks which calls the function \"hello\"\n    # If this step executes successfully,\n    # the pipeline will terminate with success\n    hello_task = PythonTask(\n        name=\"hello\",\n        function=hello,\n    )\n\n    # The pipeline has only one step.\n    pipeline = Pipeline(steps=[hello_task])\n\n    pipeline.execute()\n    return pipeline\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Dotted path</p> <p>Assuming the below project structure:</p> <ul> <li> <p>The <code>command</code> for the <code>outer_function</code> should be <code>outer_functions.outer_function</code></p> </li> <li> <p>The <code>command</code> for <code>inner_function</code> should be <code>module_inner.inner_functions.inner_function</code></p> <pre><code>..\n\u251c\u2500\u2500 outer_functions.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 outer_function()\n\u251c\u2500\u2500 module_inner\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 inner_functions.py\n\u2502\u00a0\u00a0 |    \u251c\u2500\u2500 inner_function()\n..\n</code></pre> </li> </ul> <pre><code>dag:\n  description: |\n    You can run this pipeline by:\n       runnable execute -f examples/01-tasks/python_tasks.yaml\n\n       The stdout of \"Hello World!\" would be captured as\n       execution log and stored in the catalog.\n  start_at: hello\n  steps:\n    hello:\n      type: task\n      command: examples.common.functions.hello # dotted path to the function.\n      next: success\n</code></pre>"},{"location":"concepts/task/#notebook","title":"Notebook","text":"<p>Jupyter notebooks are supported as tasks. We internally use Ploomber engine for executing notebooks.</p> <p>The output is saved to the same location as the input notebook but with <code>_out</code> post-fixed to the name of the notebook and is also saved in the <code>catalog</code> for logging and ease of debugging.</p> <p>API Documentation</p>"},{"location":"concepts/task/#example_1","title":"Example","text":"sdkyaml <pre><code>\"\"\"\nYou can execute this pipeline by:\n\n    python examples/01-tasks/notebook.py\n\nThe notebook is executed in the same environment\nso any installed packages are available for the\nnotebook.\n\nUpon successful execution, the output notebook with\ncell outputs is stored in the catalog.\n\n\"\"\"\n\nfrom runnable import NotebookTask, Pipeline\n\n\ndef main():\n    # Execute the notebook present in examples/common/simple_notebook.ipynb.\n    # The path is relative to the project root.\n    # If this step executes successfully, the pipeline will terminate with success\n    hello_task = NotebookTask(\n        name=\"hello\",\n        notebook=\"examples/common/simple_notebook.ipynb\",\n    )\n\n    # The pipeline has only one step.\n    pipeline = Pipeline(steps=[hello_task])\n\n    pipeline.execute()\n\n    return pipeline\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <pre><code>dag:\n  description: |\n    This is a sample pipeline with one step that executes a notebook.\n\n    The notebook is executed in the same environment so any installed\n    packages are available for the notebook.\n\n    Upon successful execution, the output notebook with cell outputs\n    is stored in the catalog.\n\n    You can run this pipeline as:\n      runnable execute -f examples/01-tasks/notebook.yaml\n\n  start_at: hello\n  steps:\n    hello:\n      type: task\n      command_type: notebook\n      command: examples/common/simple_notebook.ipynb # The path is relative to the root of the project.\n      next: success\n</code></pre>"},{"location":"concepts/task/#shell","title":"Shell","text":"<p>Python functions and Jupyter notebooks provide a rich interface to the python ecosystem while shell provides a interface to non-python executables.</p> <p>API Documentation</p>"},{"location":"concepts/task/#example_2","title":"Example","text":"sdkyaml <pre><code>\"\"\"\nYou can execute this pipeline by:\n\n    python examples/01-tasks/scripts.py\n\nThe command can be anything that can be\nexecuted in a shell.\nThe stdout/stderr of the execution is\ncaptured as execution log and stored in the catalog.\n\n\"\"\"\n\nfrom runnable import Pipeline, ShellTask\n\n\ndef main():\n    # If this step executes successfully, the pipeline will terminate with success\n    hello_task = ShellTask(\n        name=\"hello\",\n        command=\"echo 'Hello World!'\",\n    )\n\n    # The pipeline has only one step.\n    pipeline = Pipeline(steps=[hello_task])\n\n    pipeline.execute()\n\n    return pipeline\n\n\nif __name__ == \"__main__\":\n    main()\n    main()\n</code></pre> <pre><code>dag:\n  description: |\n    This is a sample pipeline with one step that\n    executes a shell command.\n\n    You can run this pipeline by:\n    runnable execute -f examples/01-tasks/scripts.yaml\n\n  start_at: shell\n  steps:\n    shell:\n      type: task\n      command_type: shell\n      command: echo \"hello world!!\"\n      next: success\n</code></pre>"},{"location":"concepts/task/#stub","title":"Stub","text":"<p>Stub nodes in runnable are just like <code>pass</code> or <code>...</code> in python code. It is a placeholder and useful when you want to debug ordesign your pipeline.</p> <p>Stub nodes can take arbitrary number of parameters and is always a success.</p> <p>API Documentation</p>"},{"location":"concepts/task/#example_3","title":"Example","text":"<p>Intuition</p> <p>Designing a pipeline is similar to writing a modular program. Stub nodes are handy to create a placeholder for some step that will be implemented in the future.</p> <p>During debugging, changing a node to <code>stub</code> will let you focus on the actual bug without having to execute the additional steps.</p> sdkyaml <pre><code>\"\"\"\nThis is a simple pipeline that does 3 steps in sequence.\n\n    step 1 &gt;&gt; step 2 &gt;&gt; step 3 &gt;&gt; success\n\n    All the steps are stubbed and they will just pass through.\n    Use this pattern to define the skeleton of your pipeline\n    and flesh out the steps later.\n\n    Note that you can give any arbitrary keys to the steps\n    (like step 2).\n    This is handy to mock steps within mature pipelines.\n\n    You can run this pipeline by:\n       python examples/01-tasks/stub.py\n\nYou can execute this pipeline by:\n\n    python examples/01-tasks/stub.py\n\"\"\"\n\nfrom runnable import Pipeline, Stub\n\n\ndef main():\n    # this will always succeed\n    step1 = Stub(name=\"step1\")\n\n    # It takes arbitrary arguments\n    # Useful for temporarily silencing steps within\n    # mature pipelines\n    step2 = Stub(name=\"step2\", what=\"is this thing\")\n\n    step3 = Stub(name=\"step3\")\n\n    pipeline = Pipeline(steps=[step1, step2, step3])\n\n    pipeline.execute()\n\n    # A function that creates pipeline should always return a\n    # Pipeline object\n    return pipeline\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <pre><code>dag:\n  description: |\n    This is a simple pipeline that does 3 steps in sequence.\n\n    step 1 &gt;&gt; step 2 &gt;&gt; step 3 &gt;&gt; success\n\n    All the steps are stubbed and they will just pass through.\n    Use this pattern to define the skeleton of your pipeline\n    and flesh out the steps later.\n\n    Note that you can give any arbitrary keys to the steps\n    (like step 2).\n    This is handy to mock steps within mature pipelines.\n\n    You can run this pipeline by:\n       runnable execute -f examples/01-tasks/stub.yaml\n  start_at: step 1\n  steps:\n    step 1:\n      type: stub # This will always succeed\n      next: step 2\n    step 2:\n      type: stub\n      what: is this thing? # It takes arbitrary keys\n      It: does not matter!!\n      next: step 3\n    step 3:\n      type: stub\n      next: success\n</code></pre>"},{"location":"configurations/catalog/","title":"Catalog","text":"<p>Catalog provides a way to store and retrieve data generated by the individual steps of the dag to downstream steps of the dag. Please refer to concepts for more detailed information.</p>"},{"location":"configurations/catalog/#do-nothing","title":"do-nothing","text":"<p>A noop implementation which does nothing.</p>"},{"location":"configurations/catalog/#configuration","title":"Configuration","text":"<pre><code>catalog:\n  type: do-nothing\n</code></pre>"},{"location":"configurations/catalog/#file-system","title":"file-system","text":"<p>In this configuration, the local folder is used a catalog store. The default location is <code>.catalog</code>. Every execution of the pipeline will create a new directory by the <code>run_id</code> to store all the generated artifacts.</p>"},{"location":"configurations/catalog/#configuration_1","title":"Configuration","text":"<pre><code>catalog:\n  type: file-system\n  config:\n    catalog_location: .catalog # default value\n</code></pre>"},{"location":"configurations/catalog/#example","title":"Example","text":"ConfigurationPipelineCatalog structureRun log entry <ol> <li>Use local file-system as catalog, default location is <code>.catalog</code></li> </ol> <pre><code>\n</code></pre> <p>The files suffixed by <code>.execution.log</code> are stdout and stderr of the command.</p> <pre><code>.catalog\n\u2514\u2500\u2500 juicy-blackwell-0625\n    \u251c\u2500\u2500 Create_Content.execution.log\n    \u251c\u2500\u2500 Setup.execution.log\n    \u2514\u2500\u2500 data\n        \u2514\u2500\u2500 hello.txt\n\n3 directories, 3 files\n</code></pre> <p>All the execution logs of steps along with files are stored in the catalog. Please look at the highlighted lines in the run log.</p> <pre><code>{\n    \"run_id\": \"juicy-blackwell-0625\",\n    \"dag_hash\": \"\",\n    \"use_cached\": false,\n    \"tag\": \"\",\n    \"original_run_id\": \"\",\n    \"status\": \"SUCCESS\",\n    \"steps\": {\n        \"Setup\": {\n            \"name\": \"Setup\",\n            \"internal_name\": \"Setup\",\n            \"status\": \"SUCCESS\",\n            \"step_type\": \"task\",\n            \"message\": \"\",\n            \"mock\": false,\n            \"code_identities\": [\n                {\n                    \"code_identifier\": \"39cd98770cb2fd6994d8ac08ae4c5506e5ce694a\",\n                    \"code_identifier_type\": \"git\",\n                    \"code_identifier_dependable\": true,\n                    \"code_identifier_url\": \"https://github.com/AstraZeneca/runnable-core.git\",\n                    \"code_identifier_message\": \"\"\n                }\n            ],\n            \"attempts\": [\n                {\n                    \"attempt_number\": 1,\n                    \"start_time\": \"2024-02-04 06:25:26.014967\",\n                    \"end_time\": \"2024-02-04 06:25:26.026029\",\n                    \"duration\": \"0:00:00.011062\",\n                    \"status\": \"SUCCESS\",\n                    \"message\": \"\",\n                    \"parameters\": {}\n                }\n            ],\n            \"user_defined_metrics\": {},\n            \"branches\": {},\n            \"data_catalog\": [\n                {\n                    \"name\": \"Setup.execution.log\",\n                    \"data_hash\": \"b38eb7b5290ff433276a75fdd7a3935335aedff3ab5ee8714f6ea735d9c9492c\",\n                    \"catalog_relative_path\": \"juicy-blackwell-0625/Setup.execution.log\",\n                    \"catalog_handler_location\": \".catalog\",\n                    \"stage\": \"put\"\n                }\n            ]\n        },\n        \"Create Content\": {\n            \"name\": \"Create Content\",\n            \"internal_name\": \"Create Content\",\n            \"status\": \"SUCCESS\",\n            \"step_type\": \"task\",\n            \"message\": \"\",\n            \"mock\": false,\n            \"code_identities\": [\n                {\n                    \"code_identifier\": \"39cd98770cb2fd6994d8ac08ae4c5506e5ce694a\",\n                    \"code_identifier_type\": \"git\",\n                    \"code_identifier_dependable\": true,\n                    \"code_identifier_url\": \"https://github.com/AstraZeneca/runnable-core.git\",\n                    \"code_identifier_message\": \"\"\n                }\n            ],\n            \"attempts\": [\n                {\n                    \"attempt_number\": 1,\n                    \"start_time\": \"2024-02-04 06:25:26.092282\",\n                    \"end_time\": \"2024-02-04 06:25:26.100095\",\n                    \"duration\": \"0:00:00.007813\",\n                    \"status\": \"SUCCESS\",\n                    \"message\": \"\",\n                    \"parameters\": {}\n                }\n            ],\n            \"user_defined_metrics\": {},\n            \"branches\": {},\n            \"data_catalog\": [\n                {\n                    \"name\": \"Create_Content.execution.log\",\n                    \"data_hash\": \"b38eb7b5290ff433276a75fdd7a3935335aedff3ab5ee8714f6ea735d9c9492c\",\n                    \"catalog_relative_path\": \"juicy-blackwell-0625/Create_Content.execution.log\",\n                    \"catalog_handler_location\": \".catalog\",\n                    \"stage\": \"put\"\n                },\n                {\n                    \"name\": \"data/hello.txt\",\n                    \"data_hash\": \"50e75c30352e8ef442b2b5be37dd19533f9334faaf8c4e41f2b528df57d3c20c\",\n                    \"catalog_relative_path\": \"juicy-blackwell-0625/data/hello.txt\",\n                    \"catalog_handler_location\": \".catalog\",\n                    \"stage\": \"put\"\n                }\n            ]\n        },\n        \"success\": {\n            \"name\": \"success\",\n            \"internal_name\": \"success\",\n            \"status\": \"SUCCESS\",\n            \"step_type\": \"success\",\n            \"message\": \"\",\n            \"mock\": false,\n            \"code_identities\": [\n                {\n                    \"code_identifier\": \"39cd98770cb2fd6994d8ac08ae4c5506e5ce694a\",\n                    \"code_identifier_type\": \"git\",\n                    \"code_identifier_dependable\": true,\n                    \"code_identifier_url\": \"https://github.com/AstraZeneca/runnable-core.git\",\n                    \"code_identifier_message\": \"\"\n                }\n            ],\n            \"attempts\": [\n                {\n                    \"attempt_number\": 1,\n                    \"start_time\": \"2024-02-04 06:25:26.165278\",\n                    \"end_time\": \"2024-02-04 06:25:26.165355\",\n                    \"duration\": \"0:00:00.000077\",\n                    \"status\": \"SUCCESS\",\n                    \"message\": \"\",\n                    \"parameters\": {}\n                }\n            ],\n            \"user_defined_metrics\": {},\n            \"branches\": {},\n            \"data_catalog\": []\n        }\n    },\n    \"parameters\": {},\n    \"run_config\": {\n        \"executor\": {\n            \"service_name\": \"local\",\n            \"service_type\": \"executor\",\n            \"enable_parallel\": false,\n            \"overrides\": {}\n        },\n        \"run_log_store\": {\n            \"service_name\": \"buffered\",\n            \"service_type\": \"run_log_store\"\n        },\n        \"secrets_handler\": {\n            \"service_name\": \"do-nothing\",\n            \"service_type\": \"secrets\"\n        },\n        \"catalog_handler\": {\n            \"service_name\": \"file-system\",\n            \"service_type\": \"catalog\",\n            \"catalog_location\": \".catalog\"\n        },\n        \"experiment_tracker\": {\n            \"service_name\": \"do-nothing\",\n            \"service_type\": \"experiment_tracker\"\n        },\n        \"pipeline_file\": \"\",\n        \"parameters_file\": \"\",\n        \"configuration_file\": \"examples/configs/fs-catalog.yaml\",\n        \"tag\": \"\",\n        \"run_id\": \"juicy-blackwell-0625\",\n        \"use_cached\": false,\n        \"original_run_id\": \"\",\n        \"dag\": {\n            \"start_at\": \"Setup\",\n            \"name\": \"\",\n            \"description\": \"\",\n            \"steps\": {\n                \"Setup\": {\n                    \"type\": \"task\",\n                    \"name\": \"Setup\",\n                    \"next\": \"Create Content\",\n                    \"on_failure\": \"\",\n                    \"overrides\": {},\n                    \"catalog\": null,\n                    \"max_attempts\": 1,\n                    \"command\": \"mkdir -p data\",\n                    \"command_type\": \"shell\",\n                    \"node_name\": \"Setup\"\n                },\n                \"Create Content\": {\n                    \"type\": \"task\",\n                    \"name\": \"Create Content\",\n                    \"next\": \"success\",\n                    \"on_failure\": \"\",\n                    \"overrides\": {},\n                    \"catalog\": {\n                        \"get\": [],\n                        \"put\": [\n                            \"data/hello.txt\"\n                        ]\n                    },\n                    \"max_attempts\": 1,\n                    \"command\": \"echo \\\"Hello from runnable\\\" &gt;&gt; data/hello.txt\",\n                    \"command_type\": \"shell\",\n                    \"node_name\": \"Create Content\"\n                },\n                \"success\": {\n                    \"type\": \"success\",\n                    \"name\": \"success\"\n                },\n                \"fail\": {\n                    \"type\": \"fail\",\n                    \"name\": \"fail\"\n                }\n            }\n        },\n        \"dag_hash\": \"\",\n        \"execution_plan\": \"chained\"\n    }\n}\n</code></pre>"},{"location":"configurations/overview/","title":"Run pipeline","text":"<p>Once a pipeline is defined, <code>runnable</code> can execute the pipeline in different environments by changing a configuration. Neither the pipeline definition or the data science code needs to change at all.</p>"},{"location":"configurations/overview/#concept","title":"Concept","text":"<p>Consider the example:</p> <pre><code>import os\n\ndef generate():\n    ...\n    # write some files, data.csv\n    secret = os.environ[\"secret_key\"]\n    ...\n    # return objects or simple python data types.\n    return x, y\n\ndef consume(x, y):\n    ...\n    # read from data.csv\n    # do some computation with x and y\n\n\n# Stich the functions together\n# This is the driver pattern.\nx, y = generate()\nconsume(x, y)\n</code></pre> <p>To execute the functions, we need:</p> <ul> <li>Compute environment with defined resources (CPU, memory, GPU): configured by <code>executor</code>.</li> <li>Mechanism to make variables, <code>x</code> and <code>y</code>, available to functions: achieved by <code>run_log_store</code>.</li> <li>Mechanism to recreate the file system structure for accessing <code>data</code>:  achieved by <code>catalog</code>.</li> <li>Populate secrets as environment variables: configured by <code>secrets</code>.</li> </ul> <p>By default, <code>runnable</code> uses:</p> <ul> <li>local compute to run the pipeline.</li> <li>local file system for storing the the run log.</li> <li>local file system for cataloging data flowing through the pipeline.</li> <li>wrapper around system environment variables for accessing secrets.</li> </ul> <p>This can be over-ridden by <code>configuration</code>. For example, the below configuration uses</p> <ul> <li>argo workflows as execution engine.</li> <li>mounted pvc for storing the run log.</li> <li>mounted pvc for storing the catalog.</li> <li>kubernetes secrets exposed to the container as secrets provider.</li> </ul> <pre><code>executor:\n  type: argo\n  config:\n    image: image_to_use\n    persistent_volumes: # mount a pvc to every container as /mnt\n      - name: runnable-volume\n        mount_path: /mnt\n    secrets_from_k8s: # expose postgres/connection string to container.\n      - environment_variable: connection_string\n        secret_name: postgres\n        secret_key: connection_string\n\nrun_log_store:\n  type: file-system\n  config:\n    log_folder: /mnt/run_log_store # /mnt is a pvc\n\ncatalog:\n  type: file-system\n  config:\n   catalog_location: /mnt/catalog # /mnt is a pvc\n\nsecrets: # Kubernetes exposes secrets as environment variables\n  type: env-secrets-manager\n</code></pre>"},{"location":"configurations/run-log/","title":"Run log","text":"<p>Along with tracking the progress and status of the execution of the pipeline, run log also keeps a track of parameters, experiment tracking metrics, data flowing through the pipeline and any reproducibility metrics emitted by the tasks of the pipeline.</p> <p>Please refer here for detailed information about run log.</p>"},{"location":"configurations/run-log/#buffered","title":"buffered","text":"<p>Stores all the run log in-memory. The run log is not persisted and destroyed immediately after the execution is complete.</p> <p>Parallel execution</p> <p><code>buffered</code> run log stores suffers from race conditions when two tasks need to update status concurrently.</p>"},{"location":"configurations/run-log/#configuration","title":"Configuration","text":"<pre><code>run_log_store:\n  type: buffered\n</code></pre>"},{"location":"configurations/run-log/#file-system","title":"file-system","text":"<p>Stores the run log as a <code>json</code> file in the file-system accessible by all the steps of the pipeline.</p> <p>Parallel execution</p> <p><code>file-system</code> based run log stores suffers from race conditions when two tasks need to update status concurrently. Use <code>chunked</code> version to avoid this behavior or disable parallelism.</p>"},{"location":"configurations/run-log/#configuration_1","title":"Configuration","text":"<pre><code>run_log_store:\n  type: file-system\n  config:\n    log_folder: # defaults to  \".run_log_store\"\n</code></pre>"},{"location":"configurations/run-log/#example","title":"Example","text":"Configurationsdk pipelineRun logfolder structure <p>Assumed to be present at <code>examples/configs/fs-run_log.yaml</code></p> <pre><code>\n</code></pre> <p>The configuration can be provided dynamically by setting the environment variable <code>runnable_CONFIGURATION_FILE</code>.</p> <p>Executing the pipeline with:</p> <p><code>runnable_CONFIGURATION_FILE=examples/configs/fs-run_log.yaml python examples/concepts/simple.py</code></p> <pre><code>\n</code></pre> <p>The structure of the run log is detailed in concepts.</p> <pre><code>{\n    \"run_id\": \"blocking-shaw-0538\",\n    \"dag_hash\": \"\",\n    \"use_cached\": false,\n    \"tag\": \"\",\n    \"original_run_id\": \"\",\n    \"status\": \"SUCCESS\",\n    \"steps\": {\n        \"simple\": {\n            \"name\": \"simple\",\n            \"internal_name\": \"simple\",\n            \"status\": \"SUCCESS\",\n            \"step_type\": \"task\",\n            \"message\": \"\",\n            \"mock\": false,\n            \"code_identities\": [\n                {\n                    \"code_identifier\": \"39cd98770cb2fd6994d8ac08ae4c5506e5ce694a\",\n                    \"code_identifier_type\": \"git\",\n                    \"code_identifier_dependable\": true,\n                    \"code_identifier_url\": \"https://github.com/AstraZeneca/runnable-core.git\",\n                    \"code_identifier_message\": \"\"\n                }\n            ],\n            \"attempts\": [\n                {\n                    \"attempt_number\": 1,\n                    \"start_time\": \"2024-02-02 05:38:07.973392\",\n                    \"end_time\": \"2024-02-02 05:38:07.977228\",\n                    \"duration\": \"0:00:00.003836\",\n                    \"status\": \"SUCCESS\",\n                    \"message\": \"\",\n                    \"parameters\": {}\n                }\n            ],\n            \"user_defined_metrics\": {},\n            \"branches\": {},\n            \"data_catalog\": [\n                {\n                    \"name\": \"simple.execution.log\",\n                    \"data_hash\": \"03ba204e50d126e4674c005e04d82e84c21366780af1f43bd54a37816b6ab340\",\n                    \"catalog_relative_path\": \"blocking-shaw-0538/simple.execution.log\",\n                    \"catalog_handler_location\": \".catalog\",\n                    \"stage\": \"put\"\n                }\n            ]\n        },\n        \"success\": {\n            \"name\": \"success\",\n            \"internal_name\": \"success\",\n            \"status\": \"SUCCESS\",\n            \"step_type\": \"success\",\n            \"message\": \"\",\n            \"mock\": false,\n            \"code_identities\": [\n                {\n                    \"code_identifier\": \"39cd98770cb2fd6994d8ac08ae4c5506e5ce694a\",\n                    \"code_identifier_type\": \"git\",\n                    \"code_identifier_dependable\": true,\n                    \"code_identifier_url\": \"https://github.com/AstraZeneca/runnable-core.git\",\n                    \"code_identifier_message\": \"\"\n                }\n            ],\n            \"attempts\": [\n                {\n                    \"attempt_number\": 1,\n                    \"start_time\": \"2024-02-02 05:38:08.056864\",\n                    \"end_time\": \"2024-02-02 05:38:08.057359\",\n                    \"duration\": \"0:00:00.000495\",\n                    \"status\": \"SUCCESS\",\n                    \"message\": \"\",\n                    \"parameters\": {}\n                }\n            ],\n            \"user_defined_metrics\": {},\n            \"branches\": {},\n            \"data_catalog\": []\n        }\n    },\n    \"parameters\": {},\n    \"run_config\": {\n        \"executor\": {\n            \"service_name\": \"local\",\n            \"service_type\": \"executor\",\n            \"enable_parallel\": false,\n            \"overrides\": {}\n        },\n        \"run_log_store\": {\n            \"service_name\": \"file-system\",\n            \"service_type\": \"run_log_store\",\n            \"log_folder\": \".run_log_store\"\n        },\n        \"secrets_handler\": {\n            \"service_name\": \"do-nothing\",\n            \"service_type\": \"secrets\"\n        },\n        \"catalog_handler\": {\n            \"service_name\": \"file-system\",\n            \"service_type\": \"catalog\",\n            \"catalog_location\": \".catalog\"\n        },\n        \"experiment_tracker\": {\n            \"service_name\": \"do-nothing\",\n            \"service_type\": \"experiment_tracker\"\n        },\n        \"pipeline_file\": \"\",\n        \"parameters_file\": \"\",\n        \"configuration_file\": \"examples/configs/fs-run_log.yaml\",\n        \"tag\": \"\",\n        \"run_id\": \"blocking-shaw-0538\",\n        \"use_cached\": false,\n        \"original_run_id\": \"\",\n        \"dag\": {\n            \"start_at\": \"simple\",\n            \"name\": \"\",\n            \"description\": \"\",\n            \"steps\": {\n                \"simple\": {\n                    \"type\": \"task\",\n                    \"name\": \"simple\",\n                    \"next\": \"success\",\n                    \"on_failure\": \"\",\n                    \"overrides\": {},\n                    \"catalog\": null,\n                    \"max_attempts\": 1,\n                    \"command\": \"examples.concepts.simple.simple_function\",\n                    \"command_type\": \"python\",\n                    \"node_name\": \"simple\"\n                },\n                \"success\": {\n                    \"type\": \"success\",\n                    \"name\": \"success\"\n                },\n                \"fail\": {\n                    \"type\": \"fail\",\n                    \"name\": \"fail\"\n                }\n            }\n        },\n        \"dag_hash\": \"\",\n        \"execution_plan\": \"chained\"\n    }\n}\n</code></pre> <p>All the run logs are stored in .run_log_store with the filename being the <code>run_id</code>.</p> <pre><code>&gt;&gt;&gt; tree .run_log_store\n.run_log_store\n\u2514\u2500\u2500 blocking-shaw-0538.json\n\n1 directory, 1 file\n</code></pre>"},{"location":"configurations/run-log/#chunked-fs","title":"chunked-fs","text":"<p>Chunked file system is similar to the <code>file-system</code> but stores concents of the run log that have concurrency blocks in separate files.</p>"},{"location":"configurations/run-log/#configuration_2","title":"Configuration","text":"<pre><code>run_log_store:\n  type: chunked-fs\n  config:\n    log_folder: # defaults to  \".run_log_store\"\n</code></pre> Configurationsdk pipelineRun logfolder structure <p>Assumed to be present at <code>examples/configs/chunked-fs-run_log.yaml</code></p> <pre><code>run-log-store:\n  type: chunked-fs\n</code></pre> <p>The configuration can be provided dynamically by setting the environment variable <code>runnable_CONFIGURATION_FILE</code>.</p> <p>Executing the pipeline with:</p> <p><code>runnable_CONFIGURATION_FILE=examples/configs/chunked-fs-run_log.yaml python examples/concepts/simple.py</code></p> <pre><code>\n</code></pre> <p>The structure of the run log is detailed in concepts.</p> RunLog.jsonStepLog-simple-1706852981689005000.json <p>Stores only the metadata of the run log. The contents of this are safe for concurrent executions.</p> <pre><code>{\n    \"run_id\": \"pleasant-lamarr-0549\",\n    \"dag_hash\": \"\",\n    \"use_cached\": false,\n    \"tag\": \"\",\n    \"original_run_id\": \"\",\n    \"status\": \"SUCCESS\",\n    \"steps\": {},\n    \"parameters\": {},\n    \"run_config\": {\n        \"executor\": {\n            \"service_name\": \"local\",\n            \"service_type\": \"executor\",\n            \"enable_parallel\": false,\n            \"overrides\": {}\n        },\n        \"run_log_store\": {\n            \"service_name\": \"chunked-fs\",\n            \"service_type\": \"run_log_store\",\n            \"log_folder\": \".run_log_store\"\n        },\n        \"secrets_handler\": {\n            \"service_name\": \"do-nothing\",\n            \"service_type\": \"secrets\"\n        },\n        \"catalog_handler\": {\n            \"service_name\": \"file-system\",\n            \"service_type\": \"catalog\",\n            \"catalog_location\": \".catalog\"\n        },\n        \"experiment_tracker\": {\n            \"service_name\": \"do-nothing\",\n            \"service_type\": \"experiment_tracker\"\n        },\n        \"pipeline_file\": \"\",\n        \"parameters_file\": \"\",\n        \"configuration_file\": \"examples/configs/chunked-fs-run_log.yaml\",\n        \"tag\": \"\",\n        \"run_id\": \"pleasant-lamarr-0549\",\n        \"use_cached\": false,\n        \"original_run_id\": \"\",\n        \"dag\": {\n            \"start_at\": \"simple\",\n            \"name\": \"\",\n            \"description\": \"\",\n            \"steps\": {\n                \"simple\": {\n                    \"type\": \"task\",\n                    \"name\": \"simple\",\n                    \"next\": \"success\",\n                    \"on_failure\": \"\",\n                    \"overrides\": {},\n                    \"catalog\": null,\n                    \"max_attempts\": 1,\n                    \"command\": \"examples.concepts.simple.simple_function\",\n                    \"command_type\": \"python\",\n                    \"node_name\": \"simple\"\n                },\n                \"success\": {\n                    \"type\": \"success\",\n                    \"name\": \"success\"\n                },\n                \"fail\": {\n                    \"type\": \"fail\",\n                    \"name\": \"fail\"\n                }\n            }\n        },\n        \"dag_hash\": \"\",\n        \"execution_plan\": \"chained\"\n    }\n}\n</code></pre> <p>Contains only the information of the single step <code>simple</code>. The name of the file follows the pattern:</p> <p><code>StepLog-&lt;Step name&gt;-&lt;timestamp&gt;.json</code>. The timestamp allows runnable to infer the order of execution of the steps.</p> <pre><code>{\n    \"name\": \"simple\",\n    \"internal_name\": \"simple\",\n    \"status\": \"SUCCESS\",\n    \"step_type\": \"task\",\n    \"message\": \"\",\n    \"mock\": false,\n    \"code_identities\": [\n        {\n            \"code_identifier\": \"39cd98770cb2fd6994d8ac08ae4c5506e5ce694a\",\n            \"code_identifier_type\": \"git\",\n            \"code_identifier_dependable\": true,\n            \"code_identifier_url\": \"https://github.com/AstraZeneca/runnable-core.git\",\n            \"code_identifier_message\": \"\"\n        }\n    ],\n    \"attempts\": [\n        {\n            \"attempt_number\": 1,\n            \"start_time\": \"2024-02-02 05:49:41.697142\",\n            \"end_time\": \"2024-02-02 05:49:41.702983\",\n            \"duration\": \"0:00:00.005841\",\n            \"status\": \"SUCCESS\",\n            \"message\": \"\",\n            \"parameters\": {}\n        }\n    ],\n    \"user_defined_metrics\": {},\n    \"branches\": {},\n    \"data_catalog\": [\n        {\n            \"name\": \"simple.execution.log\",\n            \"data_hash\": \"03ba204e50d126e4674c005e04d82e84c21366780af1f43bd54a37816b6ab340\",\n            \"catalog_relative_path\": \"pleasant-lamarr-0549/simple.execution.log\",\n            \"catalog_handler_location\": \".catalog\",\n            \"stage\": \"put\"\n        }\n    ]\n}\n</code></pre> <p>All the run logs are stored in .run_log_store with the directory name being the <code>run_id</code>.</p> <p>Instead of storing a single <code>json</code> file, the contents are stored in the folder by the name of the <code>`run_id</code>.</p> <pre><code>.run_log_store\n\u2514\u2500\u2500 pleasant-lamarr-0549\n    \u251c\u2500\u2500 RunLog.json\n    \u251c\u2500\u2500 StepLog-simple-1706852981689005000.json\n    \u2514\u2500\u2500 StepLog-success-1706852981779002000.json\n\n2 directories, 3 files\n</code></pre>"},{"location":"configurations/secrets/","title":"Secrets","text":"<p>Secrets are exposed as environmental variables in <code>runnable</code>.</p>"},{"location":"configurations/secrets/#do-nothing","title":"do-nothing","text":"<p>A no-op implementation of a secret manager. This is useful when you do not have need for secrets in your application.</p>"},{"location":"configurations/secrets/#configuration","title":"configuration","text":"<pre><code>secrets:\n  type: do-nothing\n</code></pre> <p>Note that this is the default configuration if nothing is specified.</p>"},{"location":"configurations/secrets/#environment_secret_manager","title":"Environment Secret Manager","text":"<p>A secrets manager to access secrets from environment variables. Many cloud based executors, especially K8's, have capabilities to send in secrets as environment variables and this secrets provider could used in those environments.</p>"},{"location":"configurations/secrets/#configuration_1","title":"Configuration","text":"<pre><code>secrets:\n  type: env-secrets-manager\n  config:\n    prefix: \"\" # default value\n    suffix: \"\" # default value\n</code></pre> <p>Use <code>suffix</code> and <code>prefix</code> the uniquely identify the secrets. The actual key while calling the secrets manager via the API, <code>get_secret(secret_key)</code> is <code>&lt;prefix&gt;&lt;secret_key&gt;&lt;suffix&gt;</code>.</p>"},{"location":"configurations/secrets/#example","title":"Example","text":"PipelineDefault ConfigurationPrefixed and Suffixed Configuration <p>Below is a simple pipeline to demonstrate the use of secrets.</p> <p>The configuration file to use can be dynamically specified via the environment variable <code>runnable_CONFIGURATION_FILE</code>.</p> <p>The example can be found in <code>examples/secrets_env.py</code></p> <pre><code>\n</code></pre> <p>We can execute the pipeline using this configuration by: <code>secret=\"secret_value\" runnable_CONFIGURATION_FILE=examples/configs/secrets-env-default.yaml python examples/secrets_env.py</code></p> <p>The configuration file is located at <code>examples/configs/secrets-env-default.yaml</code></p> <pre><code>\n</code></pre> <p>We can execute the pipeline using this configuration by: <code>runnable_secret=\"secret_value\" runnable_CONFIGURATION_FILE=examples/configs/secrets-env-ps.yaml python examples/secrets_env.py</code></p> <p>The configuration file is located at <code>examples/configs/secrets-env-ps.yaml</code></p> <pre><code>\n</code></pre>"},{"location":"configurations/secrets/#dotenv","title":"dotenv","text":"<p><code>.env</code> files are routinely used to provide configuration parameters and secrets during development phase. runnable can dotenv files as a secret store and can surface them to tasks.</p>"},{"location":"configurations/secrets/#configuration_2","title":"Configuration","text":"<pre><code>secrets:\n  type: dotenv\n  config:\n    location: .env # default value\n</code></pre> <p>The format of the <code>.env</code> file is <code>key=value</code> pairs. Any content after <code>#</code> is considered as a comment and will be ignored. Using <code>export</code> or <code>set</code>, case insensitive, as used for shell scripts are allowed.</p>"},{"location":"configurations/secrets/#example_1","title":"Example","text":".env fileExample configurationPipeline in python <p>Assumed to be present at <code>examples/secrets.env</code></p> <pre><code>\n</code></pre> <ol> <li>Shell scripts style are supported.</li> <li>Key value based format is also supported.</li> </ol> <p>Configuration to use the dotenv format file.</p> <p>Assumed to be present at <code>examples/configs/dotenv.yaml</code></p> <pre><code>secrets:\n  type: dotenv # (1)\n  config:\n    location: examples/secrets.env # (2)\n</code></pre> <ol> <li>Use dotenv secrets manager.</li> <li>Location of the dotenv file, defaults to <code>.env</code> in project root.</li> </ol> <p>The example is present in <code>examples/secrets.py</code></p> <pre><code>\n</code></pre> <ol> <li>The key of the secret that you want to retrieve.</li> </ol>"},{"location":"configurations/executors/argo/","title":"argo workflows","text":"<p>Argo workflows is a powerful container orchestration framework for Kubernetes and it can run on any Kubernetes environment.</p> <p>runnable will transpile pipeline definition to argo specification during the pipeline execution which you can then upload to the cluster either manually or via CICD (recommended).</p> <ul> <li> Execute the pipeline in any cloud environment.</li> <li> Massively scalable.</li> <li> Ability to provide specialized compute environments for different steps of the pipeline.</li> <li> Expects a mature cloud kubernetes environment and expertise.</li> </ul> <p>runnable provides sensible defaults to most of the configuration variables but it is highly advised to get inputs from infrastructure teams or ML engineers in defining the configuration.</p>"},{"location":"configurations/executors/argo/#configuration","title":"Configuration","text":"<p>Executes the pipeline using Argo Workflows.</p> <p>The defaults configuration is kept similar to the Argo Workflow spec.</p> <p>Configuration:</p> <pre><code>pipeline-executor:\n  type: argo\n  config:\n    pvc_for_runnable: \"my-pvc\"\n    custom_volumes:\n      - mount_path: \"/tmp\"\n        persistent_volume_claim:\n          claim_name: \"my-pvc\"\n          read_only: false/true\n    expose_parameters_as_inputs: true/false\n    secrets_from_k8s:\n      - key1\n      - key2\n      - ...\n    output_file: \"argo-pipeline.yaml\"\n    log_level: \"DEBUG\"/\"INFO\"/\"WARNING\"/\"ERROR\"/\"CRITICAL\"\n    defaults:\n      image: \"my-image\"\n      activeDeadlineSeconds: 86400\n      failFast: true\n      nodeSelector:\n        label: value\n      parallelism: 1\n      retryStrategy:\n        backoff:\n        duration: \"2m\"\n        factor: 2\n        maxDuration: \"1h\"\n        limit: 0\n        retryPolicy: \"Always\"\n      timeout: \"1h\"\n      tolerations:\n      imagePullPolicy: \"Always\"/\"IfNotPresent\"/\"Never\"\n      resources:\n        limits:\n          memory: \"1Gi\"\n          cpu: \"250m\"\n          gpu: 0\n        requests:\n          memory: \"1Gi\"\n          cpu: \"250m\"\n      env:\n        - name: \"MY_ENV\"\n        value: \"my-value\"\n        - name: secret_env\n        secretName: \"my-secret\"\n        secretKey: \"my-key\"\n    overrides:\n      key1:\n        ... similar structure to defaults\n\n    argoWorkflow:\n      metadata:\n        annotations:\n          key1: value1\n          key2: value2\n        generateName: \"my-workflow\"\n        labels:\n          key1: value1\n</code></pre> <p>As of now, <code>runnable</code> needs a pvc to store the logs and the catalog; provided by <code>pvc_for_runnable</code>. - <code>custom_volumes</code> can be used to mount additional volumes to the container.</p> <ul> <li><code>expose_parameters_as_inputs</code> can be used to expose the initial parameters as inputs to the workflow.</li> <li><code>secrets_from_k8s</code> can be used to expose the secrets from the k8s secret store.</li> <li><code>output_file</code> is the file where the argo pipeline will be dumped.</li> <li><code>log_level</code> is the log level for the containers.</li> <li><code>defaults</code> is the default configuration for all the containers.</li> </ul>"},{"location":"configurations/executors/argo/#example","title":"Example:","text":"<p>The following adds the volume <code>runnable-volume</code> to every container of the workflow at <code>/mnt</code></p> <pre><code>persistent_volumes:\n  - name: runnable-volume\n    mount_path: /mnt\n</code></pre> <ul> <li><code>secrets_from_k8s</code>: List of secrets from the Kubernetes cluster to be exposed as environment variables.</li> </ul> <p>Secrets</p> <p>As the secrets are exposed as environment variables, the application can then be configured using <code>env-secrets-manager</code> as a convenient way to access K8's secrets.</p>"},{"location":"configurations/executors/argo/#example_1","title":"Example:","text":"<p>In the example below, the secret <code>connection_string</code> from <code>postgres</code> secret of K8's is exposed as <code>connection_string</code> to the container.</p> <pre><code>secrets_from_k8s:\n  - environment_variable: connection_string\n    secret_name: postgres\n    secret_key: connection_string\n</code></pre> <ul> <li><code>expose_parameters_as_inputs</code>: Expose parameters of simple python data types (str, int, float) as inputs to the workflow. This allows for changing the parameters at runtime.</li> </ul>"},{"location":"configurations/executors/argo/#example_2","title":"Example:","text":"Initial Parameterspipelineargo workflowRun SubmissionStep Log <p>Assumed to present at <code>examples/concepts/parameters.yaml</code></p> <pre><code>\n</code></pre> <p>Execute the pipeline as: <code>runnable execute -f examples/concepts/task_shell_parameters.yaml  -p examples/concepts/parameters.yaml -c examples/configs/argo-config.yaml</code></p> <pre><code>\n</code></pre> <p>The initial parameter of <code>spam</code> is exposed and defaulted to <code>Hello</code> as per the parameters file. The <code>run_id</code> is also a configurable run time parameter.</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: runnable-dag-\n  annotations: {}\n  labels: {}\nspec:\n  activeDeadlineSeconds: 172800\n  entrypoint: runnable-dag\n  podGC:\n    strategy: OnPodCompletion\n  retryStrategy:\n    limit: '0'\n    retryPolicy: Always\n    backoff:\n      duration: '120'\n      factor: 2\n      maxDuration: '3600'\n  serviceAccountName: default-editor\n  templates:\n    - name: runnable-dag\n      failFast: true\n      dag:\n        tasks:\n          - name: access-initial-task-cybkoa\n            template: access-initial-task-cybkoa\n            depends: ''\n          - name: modify-initial-task-6lka8g\n            template: modify-initial-task-6lka8g\n            depends: access-initial-task-cybkoa.Succeeded\n          - name: display-again-task-6d1ofy\n            template: display-again-task-6d1ofy\n            depends: modify-initial-task-6lka8g.Succeeded\n          - name: success-success-igw6ct\n            template: success-success-igw6ct\n            depends: display-again-task-6d1ofy.Succeeded\n    - name: access-initial-task-cybkoa\n      container:\n        image: harbor.csis.astrazeneca.net/mlops/runnable:latest\n        command:\n          - runnable\n          - execute_single_node\n          - '{{workflow.parameters.run_id}}'\n          - access%initial\n          - --log-level\n          - WARNING\n          - --file\n          - examples/concepts/task_shell_parameters.yaml\n          - --config-file\n          - examples/configs/argo-config.yaml\n          - --parameters-file\n          - examples/concepts/parameters.yaml\n        volumeMounts:\n          - name: executor-0\n            mountPath: /mnt\n        imagePullPolicy: ''\n        resources:\n          limits:\n            memory: 1Gi\n            cpu: 250m\n          requests:\n            memory: 1Gi\n            cpu: 250m\n        env:\n          - name: runnable_PRM_spam\n            value: '{{workflow.parameters.spam}}'\n    - name: modify-initial-task-6lka8g\n      container:\n        image: harbor.csis.astrazeneca.net/mlops/runnable:latest\n        command:\n          - runnable\n          - execute_single_node\n          - '{{workflow.parameters.run_id}}'\n          - modify%initial\n          - --log-level\n          - WARNING\n          - --file\n          - examples/concepts/task_shell_parameters.yaml\n          - --config-file\n          - examples/configs/argo-config.yaml\n          - --parameters-file\n          - examples/concepts/parameters.yaml\n        volumeMounts:\n          - name: executor-0\n            mountPath: /mnt\n        imagePullPolicy: ''\n        resources:\n          limits:\n            memory: 1Gi\n            cpu: 250m\n          requests:\n            memory: 1Gi\n            cpu: 250m\n    - name: display-again-task-6d1ofy\n      container:\n        image: harbor.csis.astrazeneca.net/mlops/runnable:latest\n        command:\n          - runnable\n          - execute_single_node\n          - '{{workflow.parameters.run_id}}'\n          - display%again\n          - --log-level\n          - WARNING\n          - --file\n          - examples/concepts/task_shell_parameters.yaml\n          - --config-file\n          - examples/configs/argo-config.yaml\n          - --parameters-file\n          - examples/concepts/parameters.yaml\n        volumeMounts:\n          - name: executor-0\n            mountPath: /mnt\n        imagePullPolicy: ''\n        resources:\n          limits:\n            memory: 1Gi\n            cpu: 250m\n          requests:\n            memory: 1Gi\n            cpu: 250m\n    - name: success-success-igw6ct\n      container:\n        image: harbor.csis.astrazeneca.net/mlops/runnable:latest\n        command:\n          - runnable\n          - execute_single_node\n          - '{{workflow.parameters.run_id}}'\n          - success\n          - --log-level\n          - WARNING\n          - --file\n          - examples/concepts/task_shell_parameters.yaml\n          - --config-file\n          - examples/configs/argo-config.yaml\n          - --parameters-file\n          - examples/concepts/parameters.yaml\n        volumeMounts:\n          - name: executor-0\n            mountPath: /mnt\n        imagePullPolicy: ''\n        resources:\n          limits:\n            memory: 1Gi\n            cpu: 250m\n          requests:\n            memory: 1Gi\n            cpu: 250m\n  templateDefaults:\n    activeDeadlineSeconds: 7200\n    timeout: 10800s\n  arguments:\n    parameters:\n      - name: spam\n        value: Hello\n      - name: run_id\n        value: '{{workflow.uid}}'\n  volumes:\n    - name: executor-0\n      persistentVolumeClaim:\n        claimName: runnable-volume\n</code></pre> <p> argo workflows UI exposing the parameters </p> <p>The <code>step log</code> of the first step, <code>access initial</code> receives the value of the parameter <code>spam</code> as <code>No-Hello</code> from the UI submission.</p> <pre><code>{\n  \"name\": \"access initial\",\n  \"internal_name\": \"access initial\",\n  \"status\": \"SUCCESS\",\n  \"step_type\": \"task\",\n  \"message\": \"\",\n  \"mock\": false,\n  \"code_identities\": [\n      {\n          \"code_identifier\": \"39cd98770cb2fd6994d8ac08ae4c5506e5ce694a\",\n          \"code_identifier_type\": \"git\",\n          \"code_identifier_dependable\": true,\n          \"code_identifier_url\": \"https://github.com/AstraZeneca/runnable-core.git\",\n          \"code_identifier_message\": \"\"\n      }\n  ],\n  \"attempts\": [\n      {\n          \"attempt_number\": 1,\n          \"start_time\": \"2024-02-01 14:44:06.023052\",\n          \"end_time\": \"2024-02-01 14:44:06.031187\",\n          \"duration\": \"0:00:00.008135\",\n          \"status\": \"SUCCESS\",\n          \"message\": \"\",\n          \"parameters\": {\n              \"spam\": \"No-Hello\",\n              \"eggs\": {\n                  \"ham\": \"Yes, please!!\"\n              }\n          }\n      }\n  ],\n  \"user_defined_metrics\": {},\n  \"branches\": {},\n  \"data_catalog\": []\n}\n</code></pre>"},{"location":"configurations/executors/argo/#the_following_parameters_can_be_configured_at_step_level_using_overrides","title":"The following parameters can be configured at step level using overrides:","text":"<ul> <li>parallelism: Controls the number of parallel tasks that can happen at once. By default, there is no limit either for <code>parallel</code> or <code>map</code> nodes. To control the parallelism of a <code>map</code> or <code>parallel</code>, provide an <code>override</code> in the overrides section.</li> </ul> <p>The parallelism constraint only applies to the step, any nested steps within the step have the <code>default</code> parallelism.</p>"},{"location":"configurations/executors/argo/#example_3","title":"Example:","text":"Without OverrideWith Override <p>By default, there is no limit on the number of parallel tasks that can be run.</p> ConfigurationPipelineWorkflow execution <p>The argo config is a very basic configuration.</p> <pre><code>pipeline-executor:\n  type: \"argo\" # (1)\n  config:\n    pvc_for_runnable: runnable\n    defaults:\n      image: $docker_image # (3)\n      resources:\n        limits:\n          cpu: \"1\"\n          memory: 1Gi\n        requests:\n          cpu: \"0.5\"\n          memory: 500Mi\n      env:\n        - name: argo_env\n          value: \"argo\"\n    argoWorkflow:\n      metadata:\n        generateName: \"argo-\" # (2)\n        namespace: enterprise-mlops\n      spec:\n        serviceAccountName: \"default-editor\"\n\n\nrun-log-store: # (4)\n  type: chunked-fs\n  config:\n    log_folder: /mnt/run_log_store\n</code></pre> <p>This example is the same as detailed in map.</p> <pre><code>\n</code></pre> <p>From the <code>gant</code> chart representation of the workflow execution, we can see that all the <code>execute_task</code> tasks execute simultaneously.</p> <p> argo workflows UI exposing the parameters </p> ConfigurationPipelineWorkflow execution <p>While the global configuration has no limit on parallelism, any task using <code>sequential</code> override would run sequentially.</p> <pre><code>\n</code></pre> <p>The pipeline defined here is nearly the same as detailed in map with the only exception in lines 25-26 which use the <code>sequential</code> override.</p> <pre><code>dag:\n  description: |\n    This pipeline demonstrates the usage of map state to dynamically\n    execute workflows in parallel.\n\n    The step \"chunk files\" identifies the total number of batches to\n    execute in parallel and sets the parameters\n     - start_index of every batch to process, chunks\n     - number of files to process per batch, stride.\n\n    The step \"iterate and execute\" iterates on \"chunks\" and the\n    parameter name per chunk is set to be \"start_index\".\n  start_at: chunk files\n  steps:\n    chunk files:\n      type: task\n      command_type: python\n      command: \"examples.concepts.map.chunk_files\"\n      next: iterate and execute\n    iterate and execute:\n      type: map\n      iterate_on: chunks\n      iterate_as: start_index\n      next: success\n      overrides:\n        argo: sequential\n      branch:\n        start_at: execute\n        steps:\n          execute:\n            type: task\n            command_type: python\n            command: \"examples.concepts.map.process_chunk\"\n            next: success\n          success:\n            type: success\n          fail:\n            type: fail\n    success:\n      type: success\n    fail:\n      type: fail\n</code></pre> <p>The workflow execution from the <code>gant</code> chart shows the execution of <code>execute task</code> is sequential instead of parallel as seen in the default.</p> <p> argo workflows UI exposing the parameters </p> <ul> <li> <p><code>node_selector</code> and <code>tolerations</code>: Gives you the ability to selectively choose a node to run your task. See more information about node selector and tolerations for more information.</p> </li> <li> <p>resources: Has the same structure as K8's manifest. To use a GPU, you can mention the maximum number of GPUs in <code>limits</code> section. The default value is 1Gi of memory and 250m of cpu with no GPU. To override the resources for a specific task, use <code>overrides</code> section.</p> </li> </ul> <p>Example:</p> Configurationpipeline Argo configuration with override<pre><code>executor:\ntype: argo\nconfig:\n    image: &lt;required&gt;\noverrides:\n    BiggerMachine:\n      requests:\n        memory: 4Gi\n        cpu: 4\n</code></pre> <p>In this example, the <code>run on bigger machine</code> will run on a node that can provide 4 CPU cores and 4GB memory.</p> <pre><code>dag:\n  steps:\n    run on bigger machine:\n      type: task\n      override:\n        argo: BiggerMachine\n</code></pre> <ul> <li> <p><code>max_step_duration_in_seconds</code>: Defines the maximum amount of time a task can take for completion. The default value is 2 hours and an additional 1 hour is given for <code>timeout</code>.</p> </li> <li> <p><code>retry_strategy</code>: Defines the strategy to retry in case of failure. The default retry policy is <code>Always</code>, i.e in case of failure in execution of task or any other infrastructure failures. Please see argo workflows documentation for more information. As with other parameters, this can be overridden for individual task nodes.</p> </li> <li> <p><code>image_pull_policy</code>: Defaults to not setting the field. This behavior does not pull the image for any tag other than <code>latest</code></p> </li> </ul>"},{"location":"configurations/executors/argo/#compatibility","title":"Compatibility","text":"<p>As argo workflows is a cloud based executor, not all the services are compatible with it.</p> <ul> <li> <p>Run log: All steps of the workflow need access to the run log as such <code>buffered</code> run log store would not be compatible. <code>file-system</code> based run log store is compatible by using volumes that are available for all the steps of the workflow, eg. persistent volumes.</p> </li> <li> <p>catalog: Any catalog service that is available for all the steps of the workflow is compatible. <code>file-system</code> is compatible as long as the catalog store is mounted as a volume similar to the run log store.</p> </li> <li> <p>secrets: It is possible to use <code>dotenv</code> secrets manager as long as the file is available during the execution of the task. We highly recommend <code>.env</code> files to be excluded from the code versioning tools. We recommend using <code>secrets_from_k8s</code> in the configuration.</p> </li> </ul>"},{"location":"configurations/executors/argo/#example_4","title":"Example","text":"configurationpython SDKyamlArgo workflow definitionScreenshotsRun Log <p>Assumed to be present at <code>examples/configs/argo-config.yaml</code></p> <pre><code>pipeline-executor:\n  type: \"argo\" # (1)\n  config:\n    pvc_for_runnable: runnable\n    defaults:\n      image: $docker_image # (3)\n      resources:\n        limits:\n          cpu: \"1\"\n          memory: 1Gi\n        requests:\n          cpu: \"0.5\"\n          memory: 500Mi\n      env:\n        - name: argo_env\n          value: \"argo\"\n    argoWorkflow:\n      metadata:\n        generateName: \"argo-\" # (2)\n        namespace: enterprise-mlops\n      spec:\n        serviceAccountName: \"default-editor\"\n\n\nrun-log-store: # (4)\n  type: chunked-fs\n  config:\n    log_folder: /mnt/run_log_store\n</code></pre> <ol> <li>Use <code>argo</code> executor type to execute the pipeline.</li> <li>By default, all the tasks are executed in the docker image.</li> <li>Mount the persistent volume <code>runnable-volume</code> to all the containers as <code>/mnt</code>.</li> <li>Store the run logs in the file-system. As all containers have access to <code>runnable-volume</code> as <code>/mnt</code>. We use that to mounted folder as run log store.</li> </ol> <ol> <li>Generate the <code>yaml</code> definition file by: <code>runnable_CONFIGURATION_FILE=examples/configs/argo-config.yaml python examples/concepts/simple.py</code></li> <li>Build the docker image with yaml definition in it, called runnable:latest in current example.</li> <li>Execute the pipeline via the runnable CLI, <code>runnable_VAR_argo_docker_image=runnable:latest  runnable execute -f runnable-pipeline.yaml -c examples/configs/argo-config.yaml</code></li> </ol> <pre><code>\n</code></pre> <ol> <li>You can provide a configuration file dynamically by using the environment variable <code>runnable_CONFIGURATION_FILE</code>. Please see SDK for more details.</li> </ol> <pre><code>\n</code></pre> <pre><code>\n</code></pre> <p> argo workflows UI showing the pipeline </p> <p> argo workflows UI showing the logs </p> <p>The run log structure is the same as any other executor. Any failed executions in the workflow can be executed in <code>local</code> by providing this run log and any catalog files.</p> <pre><code>{\n  \"run_id\": \"bb96359d-74f0-4837-90e3-94aed85dbb8f\",\n  \"dag_hash\": \"d467805d7f743d459a6abce95bedbfc6c1ecab67\",\n  \"use_cached\": false,\n  \"tag\": \"\",\n  \"original_run_id\": \"\",\n  \"status\": \"SUCCESS\",\n  \"steps\": {\n      \"simple\": {\n          \"name\": \"simple\",\n          \"internal_name\": \"simple\",\n          \"status\": \"SUCCESS\",\n          \"step_type\": \"task\",\n          \"message\": \"\",\n          \"mock\": false,\n          \"code_identities\": [\n              {\n                  \"code_identifier\": \"39cd98770cb2fd6994d8ac08ae4c5506e5ce694a\",\n                  \"code_identifier_type\": \"git\",\n                  \"code_identifier_dependable\": true,\n                  \"code_identifier_url\": \"https://github.com/AstraZeneca/runnable-core.git\",\n                  \"code_identifier_message\": \"\"\n              }\n          ],\n          \"attempts\": [\n              {\n                  \"attempt_number\": 1,\n                  \"start_time\": \"2024-01-31 06:43:01.937309\",\n                  \"end_time\": \"2024-01-31 06:43:01.940862\",\n                  \"duration\": \"0:00:00.003553\",\n                  \"status\": \"SUCCESS\",\n                  \"message\": \"\",\n                  \"parameters\": {}\n              }\n          ],\n          \"user_defined_metrics\": {},\n          \"branches\": {},\n          \"data_catalog\": []\n      },\n      \"success\": {\n          \"name\": \"success\",\n          \"internal_name\": \"success\",\n          \"status\": \"SUCCESS\",\n          \"step_type\": \"success\",\n          \"message\": \"\",\n          \"mock\": false,\n          \"code_identities\": [\n              {\n                  \"code_identifier\": \"39cd98770cb2fd6994d8ac08ae4c5506e5ce694a\",\n                  \"code_identifier_type\": \"git\",\n                  \"code_identifier_dependable\": false,\n                  \"code_identifier_url\": \"https://github.com/AstraZeneca/runnable-core.git\",\n                  \"code_identifier_message\": \"\"\n              }\n          ],\n          \"attempts\": [\n              {\n                  \"attempt_number\": 1,\n                  \"start_time\": \"2024-01-31 06:43:26.537710\",\n                  \"end_time\": \"2024-01-31 06:43:26.544461\",\n                  \"duration\": \"0:00:00.006751\",\n                  \"status\": \"SUCCESS\",\n                  \"message\": \"\",\n                  \"parameters\": {}\n              }\n          ],\n          \"user_defined_metrics\": {},\n          \"branches\": {},\n          \"data_catalog\": []\n      }\n  },\n  \"parameters\": {},\n  \"run_config\": {\n      \"executor\": {\n          \"service_name\": \"argo\",\n          \"service_type\": \"executor\",\n          \"enable_parallel\": false,\n          \"overrides\": {},\n          \"image\": \"$argo_docker_image\",\n          \"expose_parameters_as_inputs\": true,\n          \"output_file\": \"argo-pipeline.yaml\",\n          \"name\": \"runnable-dag-\",\n          \"annotations\": {},\n          \"labels\": {},\n          \"namespace\": null,\n          \"activeDeadlineSeconds\": 172800,\n          \"nodeSelector\": null,\n          \"parallelism\": null,\n          \"branch_parallelism\": 0,\n          \"retryStrategy\": {\n              \"limit\": \"0\",\n              \"retryPolicy\": \"Always\",\n              \"backoff\": {\n                  \"duration\": \"120\",\n                  \"factor\": 2,\n                  \"maxDuration\": \"3600\"\n              }\n          },\n          \"max_step_duration_in_seconds\": 7200,\n          \"tolerations\": null,\n          \"image_pull_policy\": \"\",\n          \"service_account_name\": null,\n          \"secrets_from_k8s\": [],\n          \"persistent_volumes\": [\n              {\n                  \"name\": \"runnable-volume\",\n                  \"mount_path\": \"/mnt\"\n              }\n          ],\n          \"step_timeout\": 14400\n      },\n      \"run_log_store\": {\n          \"service_name\": \"file-system\",\n          \"service_type\": \"run_log_store\",\n          \"log_folder\": \"/mnt/run_log_store\"\n      },\n      \"secrets_handler\": {\n          \"service_name\": \"do-nothing\",\n          \"service_type\": \"secrets\"\n      },\n      \"catalog_handler\": {\n          \"service_name\": \"do-nothing\",\n          \"service_type\": \"catalog\"\n      },\n      \"experiment_tracker\": {\n          \"service_name\": \"do-nothing\",\n          \"service_type\": \"experiment_tracker\"\n      },\n      \"pipeline_file\": \"examples/concepts/simple.yaml\",\n      \"parameters_file\": null,\n      \"configuration_file\": \"examples/configs/argo-config.yaml\",\n      \"tag\": \"\",\n      \"run_id\": \"bb96359d-74f0-4837-90e3-94aed85dbb8f\",\n      \"variables\": {},\n      \"use_cached\": false,\n      \"original_run_id\": \"\",\n      \"dag\": {\n          \"start_at\": \"simple\",\n          \"name\": \"\",\n          \"description\": null,\n          \"steps\": {\n              \"simple\": {\n                  \"type\": \"task\",\n                  \"name\": \"simple\",\n                  \"next\": \"success\",\n                  \"on_failure\": \"\",\n                  \"overrides\": {},\n                  \"catalog\": null,\n                  \"max_attempts\": 1,\n                  \"command\": \"examples.concepts.simple.simple_function\",\n                  \"command_type\": \"python\",\n                  \"node_name\": \"simple\"\n              },\n              \"success\": {\n                  \"type\": \"success\",\n                  \"name\": \"success\"\n              },\n              \"fail\": {\n                  \"type\": \"fail\",\n                  \"name\": \"fail\"\n              }\n          }\n      },\n      \"dag_hash\": \"d467805d7f743d459a6abce95bedbfc6c1ecab67\",\n      \"execution_plan\": \"chained\"\n  }\n}\n</code></pre>"},{"location":"configurations/executors/argo/#nesting","title":"Nesting","text":"<p>runnable compiled argo workflows support deeply nested workflows.</p>"},{"location":"configurations/executors/argo/#example_5","title":"Example","text":"Nested workflowConfigurationArgo workflowIn argo UI <p>This is the same example as shown in nested.</p> <p>Assumed to be present at <code>examples/configs/argo-config.yaml</code></p> <pre><code>pipeline-executor:\n  type: \"argo\" # (1)\n  config:\n    pvc_for_runnable: runnable\n    defaults:\n      image: $docker_image # (3)\n      resources:\n        limits:\n          cpu: \"1\"\n          memory: 1Gi\n        requests:\n          cpu: \"0.5\"\n          memory: 500Mi\n      env:\n        - name: argo_env\n          value: \"argo\"\n    argoWorkflow:\n      metadata:\n        generateName: \"argo-\" # (2)\n        namespace: enterprise-mlops\n      spec:\n        serviceAccountName: \"default-editor\"\n\n\nrun-log-store: # (4)\n  type: chunked-fs\n  config:\n    log_folder: /mnt/run_log_store\n</code></pre> <ol> <li>Use <code>argo</code> executor type to execute the pipeline.</li> <li>By default, all the tasks are executed in the docker image.</li> <li>Mount the persistent volume <code>runnable-volume</code> to all the containers as <code>/mnt</code>.</li> <li>Store the run logs in the file-system. As all containers have access to <code>runnable-volume</code> as <code>/mnt</code>. We use that to mounted folder as run log store.</li> </ol> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: runnable-dag-\n  annotations: {}\n  labels: {}\nspec:\n  activeDeadlineSeconds: 172800\n  entrypoint: runnable-dag\n  retryStrategy:\n    limit: \"0\"\n    retryPolicy: Always\n    backoff:\n      duration: \"120\"\n      factor: 2\n      maxDuration: \"3600\"\n  serviceAccountName: default-editor\n  templates:\n    - name: inner-most-map-map-yeslqe-map\n      inputs:\n        parameters:\n          - name: xarg\n          - name: yarg\n      failFast: true\n      dag:\n        tasks:\n          - name: executable-stub-blnf25\n            template: executable-stub-blnf25\n            depends: \"\"\n            arguments:\n              parameters:\n                - name: xarg\n                  value: \"{{inputs.parameters.xarg}}\"\n                - name: yarg\n                  value: \"{{inputs.parameters.yarg}}\"\n          - name: success-success-trvgst\n            template: success-success-trvgst\n            depends: executable-stub-blnf25.Succeeded\n            arguments:\n              parameters:\n                - name: xarg\n                  value: \"{{inputs.parameters.xarg}}\"\n                - name: yarg\n                  value: \"{{inputs.parameters.yarg}}\"\n    - name: inner-most-map-map-yeslqe\n      inputs:\n        parameters:\n          - name: xarg\n      failFast: true\n      dag:\n        tasks:\n          - name: inner-most-map-map-yeslqe-fan-out\n            template: inner-most-map-map-yeslqe-fan-out\n            depends: \"\"\n            arguments:\n              parameters:\n                - name: xarg\n                  value: \"{{inputs.parameters.xarg}}\"\n          - name: inner-most-map-map-yeslqe-map\n            template: inner-most-map-map-yeslqe-map\n            depends: inner-most-map-map-yeslqe-fan-out.Succeeded\n            arguments:\n              parameters:\n                - name: xarg\n                  value: \"{{inputs.parameters.xarg}}\"\n                - name: yarg\n                  value: \"{{item}}\"\n            withParam: \"{{tasks.inner-most-map-map-yeslqe-fan-out.outputs.parameters.iterate-on}}\"\n          - name: inner-most-map-map-yeslqe-fan-in\n            template: inner-most-map-map-yeslqe-fan-in\n            depends: inner-most-map-map-yeslqe-map.Succeeded || inner-most-map-map-yeslqe-map.Failed\n            arguments:\n              parameters:\n                - name: xarg\n                  value: \"{{inputs.parameters.xarg}}\"\n    - name: nested-parallel-parallel-wje1o4-outer-most-map-map-variable-placeholder-nested-parallel-a\n      inputs:\n        parameters:\n          - name: xarg\n      failFast: true\n      dag:\n        tasks:\n          - name: inner-most-map-map-yeslqe\n            template: inner-most-map-map-yeslqe\n            depends: \"\"\n            arguments:\n              parameters:\n                - name: xarg\n                  value: \"{{inputs.parameters.xarg}}\"\n          - name: success-success-y1yr7v\n            template: success-success-y1yr7v\n            depends: inner-most-map-map-yeslqe.Succeeded\n            arguments:\n              parameters:\n                - name: xarg\n                  value: \"{{inputs.parameters.xarg}}\"\n    - name: inner-most-map-map-b206p5-map\n      inputs:\n        parameters:\n          - name: xarg\n          - name: yarg\n      failFast: true\n      dag:\n        tasks:\n          - name: executable-stub-8ui1yv\n            template: executable-stub-8ui1yv\n            depends: \"\"\n            arguments:\n              parameters:\n                - name: xarg\n                  value: \"{{inputs.parameters.xarg}}\"\n                - name: yarg\n                  value: \"{{inputs.parameters.yarg}}\"\n          - name: success-success-h4j0k9\n            template: success-success-h4j0k9\n            depends: executable-stub-8ui1yv.Succeeded\n            arguments:\n              parameters:\n                - name: xarg\n                  value: \"{{inputs.parameters.xarg}}\"\n                - name: yarg\n                  value: \"{{inputs.parameters.yarg}}\"\n    - name: inner-most-map-map-b206p5\n      inputs:\n        parameters:\n          - name: xarg\n      failFast: true\n      dag:\n        tasks:\n          - name: inner-most-map-map-b206p5-fan-out\n            template: inner-most-map-map-b206p5-fan-out\n            depends: \"\"\n            arguments:\n              parameters:\n                - name: xarg\n                  value: \"{{inputs.parameters.xarg}}\"\n          - name: inner-most-map-map-b206p5-map\n            template: inner-most-map-map-b206p5-map\n            depends: inner-most-map-map-b206p5-fan-out.Succeeded\n            arguments:\n              parameters:\n                - name: xarg\n                  value: \"{{inputs.parameters.xarg}}\"\n                - name: yarg\n                  value: \"{{item}}\"\n            withParam: \"{{tasks.inner-most-map-map-b206p5-fan-out.outputs.parameters.iterate-on}}\"\n          - name: inner-most-map-map-b206p5-fan-in\n            template: inner-most-map-map-b206p5-fan-in\n            depends: inner-most-map-map-b206p5-map.Succeeded || inner-most-map-map-b206p5-map.Failed\n            arguments:\n              parameters:\n                - name: xarg\n                  value: \"{{inputs.parameters.xarg}}\"\n    - name: nested-parallel-parallel-wje1o4-outer-most-map-map-variable-placeholder-nested-parallel-b\n      inputs:\n        parameters:\n          - name: xarg\n      failFast: true\n      dag:\n        tasks:\n          - name: inner-most-map-map-b206p5\n            template: inner-most-map-map-b206p5\n            depends: \"\"\n            arguments:\n              parameters:\n                - name: xarg\n                  value: \"{{inputs.parameters.xarg}}\"\n          - name: success-success-dvma7h\n            template: success-success-dvma7h\n            depends: inner-most-map-map-b206p5.Succeeded\n            arguments:\n              parameters:\n                - name: xarg\n                  value: \"{{inputs.parameters.xarg}}\"\n    - name: nested-parallel-parallel-wje1o4\n      inputs:\n        parameters:\n          - name: xarg\n      failFast: true\n      dag:\n        tasks:\n          - name: nested-parallel-parallel-wje1o4-fan-out\n            template: nested-parallel-parallel-wje1o4-fan-out\n            depends: \"\"\n            arguments:\n              parameters:\n                - name: xarg\n                  value: \"{{inputs.parameters.xarg}}\"\n          - name: nested-parallel-parallel-wje1o4-outer-most-map-map-variable-placeholder-nested-parallel-a\n            template: nested-parallel-parallel-wje1o4-outer-most-map-map-variable-placeholder-nested-parallel-a\n            depends: nested-parallel-parallel-wje1o4-fan-out.Succeeded\n            arguments:\n              parameters:\n                - name: xarg\n                  value: \"{{inputs.parameters.xarg}}\"\n          - name: nested-parallel-parallel-wje1o4-outer-most-map-map-variable-placeholder-nested-parallel-b\n            template: nested-parallel-parallel-wje1o4-outer-most-map-map-variable-placeholder-nested-parallel-b\n            depends: nested-parallel-parallel-wje1o4-fan-out.Succeeded\n            arguments:\n              parameters:\n                - name: xarg\n                  value: \"{{inputs.parameters.xarg}}\"\n          - name: nested-parallel-parallel-wje1o4-fan-in\n            template: nested-parallel-parallel-wje1o4-fan-in\n            depends:\n              nested-parallel-parallel-wje1o4-outer-most-map-map-variable-placeholder-nested-parallel-a.Succeeded\n              ||\n              nested-parallel-parallel-wje1o4-outer-most-map-map-variable-placeholder-nested-parallel-a.Failed\n              ||\n              nested-parallel-parallel-wje1o4-outer-most-map-map-variable-placeholder-nested-parallel-b.Succeeded\n              ||\n              nested-parallel-parallel-wje1o4-outer-most-map-map-variable-placeholder-nested-parallel-b.Failed\n            arguments:\n              parameters:\n                - name: xarg\n                  value: \"{{inputs.parameters.xarg}}\"\n    - name: outer-most-map-map-0ukhr5-map\n      inputs:\n        parameters:\n          - name: xarg\n      failFast: true\n      dag:\n        tasks:\n          - name: nested-parallel-parallel-wje1o4\n            template: nested-parallel-parallel-wje1o4\n            depends: \"\"\n            arguments:\n              parameters:\n                - name: xarg\n                  value: \"{{inputs.parameters.xarg}}\"\n          - name: success-success-e4lb2k\n            template: success-success-e4lb2k\n            depends: nested-parallel-parallel-wje1o4.Succeeded\n            arguments:\n              parameters:\n                - name: xarg\n                  value: \"{{inputs.parameters.xarg}}\"\n    - name: outer-most-map-map-0ukhr5\n      failFast: true\n      dag:\n        tasks:\n          - name: outer-most-map-map-0ukhr5-fan-out\n            template: outer-most-map-map-0ukhr5-fan-out\n            depends: \"\"\n          - name: outer-most-map-map-0ukhr5-map\n            template: outer-most-map-map-0ukhr5-map\n            depends: outer-most-map-map-0ukhr5-fan-out.Succeeded\n            arguments:\n              parameters:\n                - name: xarg\n                  value: \"{{item}}\"\n            withParam: \"{{tasks.outer-most-map-map-0ukhr5-fan-out.outputs.parameters.iterate-on}}\"\n          - name: outer-most-map-map-0ukhr5-fan-in\n            template: outer-most-map-map-0ukhr5-fan-in\n            depends: outer-most-map-map-0ukhr5-map.Succeeded || outer-most-map-map-0ukhr5-map.Failed\n    - name: runnable-dag\n      failFast: true\n      dag:\n        tasks:\n          - name: generate-list-task-s7za4e\n            template: generate-list-task-s7za4e\n            depends: \"\"\n          - name: outer-most-map-map-0ukhr5\n            template: outer-most-map-map-0ukhr5\n            depends: generate-list-task-s7za4e.Succeeded\n          - name: success-success-2v62uq\n            template: success-success-2v62uq\n            depends: outer-most-map-map-0ukhr5.Succeeded\n    - name: generate-list-task-s7za4e\n      container:\n        image: harbor.csis.astrazeneca.net/mlops/runnable:latest\n        command:\n          - runnable\n          - execute_single_node\n          - \"{{workflow.parameters.run_id}}\"\n          - generate_list\n          - --log-level\n          - WARNING\n          - --file\n          - examples/concepts/nesting.yaml\n          - --config-file\n          - examples/configs/argo-config.yaml\n        volumeMounts:\n          - name: executor-0\n            mountPath: /mnt\n        imagePullPolicy: \"\"\n        resources:\n          limits:\n            memory: 1Gi\n            cpu: 250m\n          requests:\n            memory: 1Gi\n            cpu: 250m\n    - name: outer-most-map-map-0ukhr5-fan-out\n      container:\n        image: harbor.csis.astrazeneca.net/mlops/runnable:latest\n        command:\n          - runnable\n          - fan\n          - \"{{workflow.parameters.run_id}}\"\n          - outer%most%map\n          - --mode\n          - out\n          - --file\n          - examples/concepts/nesting.yaml\n          - --log-level\n          - WARNING\n          - --config-file\n          - examples/configs/argo-config.yaml\n        volumeMounts:\n          - name: executor-0\n            mountPath: /mnt\n        imagePullPolicy: \"\"\n        resources:\n          limits:\n            memory: 1Gi\n            cpu: 250m\n          requests:\n            memory: 1Gi\n            cpu: 250m\n      outputs:\n        parameters:\n          - name: iterate-on\n            valueFrom:\n              path: /tmp/output.txt\n    - name: outer-most-map-map-0ukhr5-fan-in\n      container:\n        image: harbor.csis.astrazeneca.net/mlops/runnable:latest\n        command:\n          - runnable\n          - fan\n          - \"{{workflow.parameters.run_id}}\"\n          - outer%most%map\n          - --mode\n          - in\n          - --file\n          - examples/concepts/nesting.yaml\n          - --log-level\n          - WARNING\n          - --config-file\n          - examples/configs/argo-config.yaml\n        volumeMounts:\n          - name: executor-0\n            mountPath: /mnt\n        imagePullPolicy: \"\"\n        resources:\n          limits:\n            memory: 1Gi\n            cpu: 250m\n          requests:\n            memory: 1Gi\n            cpu: 250m\n    - name: nested-parallel-parallel-wje1o4-fan-out\n      container:\n        image: harbor.csis.astrazeneca.net/mlops/runnable:latest\n        command:\n          - runnable\n          - fan\n          - \"{{workflow.parameters.run_id}}\"\n          - outer%most%map.map_variable_placeholder.nested%parallel\n          - --mode\n          - out\n          - --file\n          - examples/concepts/nesting.yaml\n          - --log-level\n          - WARNING\n          - --config-file\n          - examples/configs/argo-config.yaml\n          - --map-variable\n          - '{\"xarg\": \"{{inputs.parameters.xarg}}\"}'\n        volumeMounts:\n          - name: executor-0\n            mountPath: /mnt\n        imagePullPolicy: \"\"\n        resources:\n          limits:\n            memory: 1Gi\n            cpu: 250m\n          requests:\n            memory: 1Gi\n            cpu: 250m\n      inputs:\n        parameters:\n          - name: xarg\n    - name: nested-parallel-parallel-wje1o4-fan-in\n      container:\n        image: harbor.csis.astrazeneca.net/mlops/runnable:latest\n        command:\n          - runnable\n          - fan\n          - \"{{workflow.parameters.run_id}}\"\n          - outer%most%map.map_variable_placeholder.nested%parallel\n          - --mode\n          - in\n          - --file\n          - examples/concepts/nesting.yaml\n          - --log-level\n          - WARNING\n          - --config-file\n          - examples/configs/argo-config.yaml\n          - --map-variable\n          - '{\"xarg\": \"{{inputs.parameters.xarg}}\"}'\n        volumeMounts:\n          - name: executor-0\n            mountPath: /mnt\n        imagePullPolicy: \"\"\n        resources:\n          limits:\n            memory: 1Gi\n            cpu: 250m\n          requests:\n            memory: 1Gi\n            cpu: 250m\n      inputs:\n        parameters:\n          - name: xarg\n    - name: inner-most-map-map-yeslqe-fan-out\n      container:\n        image: harbor.csis.astrazeneca.net/mlops/runnable:latest\n        command:\n          - runnable\n          - fan\n          - \"{{workflow.parameters.run_id}}\"\n          - outer%most%map.map_variable_placeholder.nested%parallel.a.inner%most%map\n          - --mode\n          - out\n          - --file\n          - examples/concepts/nesting.yaml\n          - --log-level\n          - WARNING\n          - --config-file\n          - examples/configs/argo-config.yaml\n          - --map-variable\n          - '{\"xarg\": \"{{inputs.parameters.xarg}}\"}'\n        volumeMounts:\n          - name: executor-0\n            mountPath: /mnt\n        imagePullPolicy: \"\"\n        resources:\n          limits:\n            memory: 1Gi\n            cpu: 250m\n          requests:\n            memory: 1Gi\n            cpu: 250m\n      outputs:\n        parameters:\n          - name: iterate-on\n            valueFrom:\n              path: /tmp/output.txt\n      inputs:\n        parameters:\n          - name: xarg\n    - name: inner-most-map-map-yeslqe-fan-in\n      container:\n        image: harbor.csis.astrazeneca.net/mlops/runnable:latest\n        command:\n          - runnable\n          - fan\n          - \"{{workflow.parameters.run_id}}\"\n          - outer%most%map.map_variable_placeholder.nested%parallel.a.inner%most%map\n          - --mode\n          - in\n          - --file\n          - examples/concepts/nesting.yaml\n          - --log-level\n          - WARNING\n          - --config-file\n          - examples/configs/argo-config.yaml\n          - --map-variable\n          - '{\"xarg\": \"{{inputs.parameters.xarg}}\"}'\n        volumeMounts:\n          - name: executor-0\n            mountPath: /mnt\n        imagePullPolicy: \"\"\n        resources:\n          limits:\n            memory: 1Gi\n            cpu: 250m\n          requests:\n            memory: 1Gi\n            cpu: 250m\n      inputs:\n        parameters:\n          - name: xarg\n    - name: executable-stub-blnf25\n      container:\n        image: harbor.csis.astrazeneca.net/mlops/runnable:latest\n        command:\n          - runnable\n          - execute_single_node\n          - \"{{workflow.parameters.run_id}}\"\n          - outer%most%map.map_variable_placeholder.nested%parallel.a.inner%most%map.map_variable_placeholder.executable\n          - --log-level\n          - WARNING\n          - --file\n          - examples/concepts/nesting.yaml\n          - --map-variable\n          - '{\"xarg\": \"{{inputs.parameters.xarg}}\", \"yarg\": \"{{inputs.parameters.yarg}}\"}'\n          - --config-file\n          - examples/configs/argo-config.yaml\n        volumeMounts:\n          - name: executor-0\n            mountPath: /mnt\n        imagePullPolicy: \"\"\n        resources:\n          limits:\n            memory: 1Gi\n            cpu: 250m\n          requests:\n            memory: 1Gi\n            cpu: 250m\n      inputs:\n        parameters:\n          - name: xarg\n          - name: yarg\n    - name: success-success-trvgst\n      container:\n        image: harbor.csis.astrazeneca.net/mlops/runnable:latest\n        command:\n          - runnable\n          - execute_single_node\n          - \"{{workflow.parameters.run_id}}\"\n          - outer%most%map.map_variable_placeholder.nested%parallel.a.inner%most%map.map_variable_placeholder.success\n          - --log-level\n          - WARNING\n          - --file\n          - examples/concepts/nesting.yaml\n          - --map-variable\n          - '{\"xarg\": \"{{inputs.parameters.xarg}}\", \"yarg\": \"{{inputs.parameters.yarg}}\"}'\n          - --config-file\n          - examples/configs/argo-config.yaml\n        volumeMounts:\n          - name: executor-0\n            mountPath: /mnt\n        imagePullPolicy: \"\"\n        resources:\n          limits:\n            memory: 1Gi\n            cpu: 250m\n          requests:\n            memory: 1Gi\n            cpu: 250m\n      inputs:\n        parameters:\n          - name: xarg\n          - name: yarg\n    - name: success-success-y1yr7v\n      container:\n        image: harbor.csis.astrazeneca.net/mlops/runnable:latest\n        command:\n          - runnable\n          - execute_single_node\n          - \"{{workflow.parameters.run_id}}\"\n          - outer%most%map.map_variable_placeholder.nested%parallel.a.success\n          - --log-level\n          - WARNING\n          - --file\n          - examples/concepts/nesting.yaml\n          - --map-variable\n          - '{\"xarg\": \"{{inputs.parameters.xarg}}\"}'\n          - --config-file\n          - examples/configs/argo-config.yaml\n        volumeMounts:\n          - name: executor-0\n            mountPath: /mnt\n        imagePullPolicy: \"\"\n        resources:\n          limits:\n            memory: 1Gi\n            cpu: 250m\n          requests:\n            memory: 1Gi\n            cpu: 250m\n      inputs:\n        parameters:\n          - name: xarg\n    - name: inner-most-map-map-b206p5-fan-out\n      container:\n        image: harbor.csis.astrazeneca.net/mlops/runnable:latest\n        command:\n          - runnable\n          - fan\n          - \"{{workflow.parameters.run_id}}\"\n          - outer%most%map.map_variable_placeholder.nested%parallel.b.inner%most%map\n          - --mode\n          - out\n          - --file\n          - examples/concepts/nesting.yaml\n          - --log-level\n          - WARNING\n          - --config-file\n          - examples/configs/argo-config.yaml\n          - --map-variable\n          - '{\"xarg\": \"{{inputs.parameters.xarg}}\"}'\n        volumeMounts:\n          - name: executor-0\n            mountPath: /mnt\n        imagePullPolicy: \"\"\n        resources:\n          limits:\n            memory: 1Gi\n            cpu: 250m\n          requests:\n            memory: 1Gi\n            cpu: 250m\n      outputs:\n        parameters:\n          - name: iterate-on\n            valueFrom:\n              path: /tmp/output.txt\n      inputs:\n        parameters:\n          - name: xarg\n    - name: inner-most-map-map-b206p5-fan-in\n      container:\n        image: harbor.csis.astrazeneca.net/mlops/runnable:latest\n        command:\n          - runnable\n          - fan\n          - \"{{workflow.parameters.run_id}}\"\n          - outer%most%map.map_variable_placeholder.nested%parallel.b.inner%most%map\n          - --mode\n          - in\n          - --file\n          - examples/concepts/nesting.yaml\n          - --log-level\n          - WARNING\n          - --config-file\n          - examples/configs/argo-config.yaml\n          - --map-variable\n          - '{\"xarg\": \"{{inputs.parameters.xarg}}\"}'\n        volumeMounts:\n          - name: executor-0\n            mountPath: /mnt\n        imagePullPolicy: \"\"\n        resources:\n          limits:\n            memory: 1Gi\n            cpu: 250m\n          requests:\n            memory: 1Gi\n            cpu: 250m\n      inputs:\n        parameters:\n          - name: xarg\n    - name: executable-stub-8ui1yv\n      container:\n        image: harbor.csis.astrazeneca.net/mlops/runnable:latest\n        command:\n          - runnable\n          - execute_single_node\n          - \"{{workflow.parameters.run_id}}\"\n          - outer%most%map.map_variable_placeholder.nested%parallel.b.inner%most%map.map_variable_placeholder.executable\n          - --log-level\n          - WARNING\n          - --file\n          - examples/concepts/nesting.yaml\n          - --map-variable\n          - '{\"xarg\": \"{{inputs.parameters.xarg}}\", \"yarg\": \"{{inputs.parameters.yarg}}\"}'\n          - --config-file\n          - examples/configs/argo-config.yaml\n        volumeMounts:\n          - name: executor-0\n            mountPath: /mnt\n        imagePullPolicy: \"\"\n        resources:\n          limits:\n            memory: 1Gi\n            cpu: 250m\n          requests:\n            memory: 1Gi\n            cpu: 250m\n      inputs:\n        parameters:\n          - name: xarg\n          - name: yarg\n    - name: success-success-h4j0k9\n      container:\n        image: harbor.csis.astrazeneca.net/mlops/runnable:latest\n        command:\n          - runnable\n          - execute_single_node\n          - \"{{workflow.parameters.run_id}}\"\n          - outer%most%map.map_variable_placeholder.nested%parallel.b.inner%most%map.map_variable_placeholder.success\n          - --log-level\n          - WARNING\n          - --file\n          - examples/concepts/nesting.yaml\n          - --map-variable\n          - '{\"xarg\": \"{{inputs.parameters.xarg}}\", \"yarg\": \"{{inputs.parameters.yarg}}\"}'\n          - --config-file\n          - examples/configs/argo-config.yaml\n        volumeMounts:\n          - name: executor-0\n            mountPath: /mnt\n        imagePullPolicy: \"\"\n        resources:\n          limits:\n            memory: 1Gi\n            cpu: 250m\n          requests:\n            memory: 1Gi\n            cpu: 250m\n      inputs:\n        parameters:\n          - name: xarg\n          - name: yarg\n    - name: success-success-dvma7h\n      container:\n        image: harbor.csis.astrazeneca.net/mlops/runnable:latest\n        command:\n          - runnable\n          - execute_single_node\n          - \"{{workflow.parameters.run_id}}\"\n          - outer%most%map.map_variable_placeholder.nested%parallel.b.success\n          - --log-level\n          - WARNING\n          - --file\n          - examples/concepts/nesting.yaml\n          - --map-variable\n          - '{\"xarg\": \"{{inputs.parameters.xarg}}\"}'\n          - --config-file\n          - examples/configs/argo-config.yaml\n        volumeMounts:\n          - name: executor-0\n            mountPath: /mnt\n        imagePullPolicy: \"\"\n        resources:\n          limits:\n            memory: 1Gi\n            cpu: 250m\n          requests:\n            memory: 1Gi\n            cpu: 250m\n      inputs:\n        parameters:\n          - name: xarg\n    - name: success-success-e4lb2k\n      container:\n        image: harbor.csis.astrazeneca.net/mlops/runnable:latest\n        command:\n          - runnable\n          - execute_single_node\n          - \"{{workflow.parameters.run_id}}\"\n          - outer%most%map.map_variable_placeholder.success\n          - --log-level\n          - WARNING\n          - --file\n          - examples/concepts/nesting.yaml\n          - --map-variable\n          - '{\"xarg\": \"{{inputs.parameters.xarg}}\"}'\n          - --config-file\n          - examples/configs/argo-config.yaml\n        volumeMounts:\n          - name: executor-0\n            mountPath: /mnt\n        imagePullPolicy: \"\"\n        resources:\n          limits:\n            memory: 1Gi\n            cpu: 250m\n          requests:\n            memory: 1Gi\n            cpu: 250m\n      inputs:\n        parameters:\n          - name: xarg\n    - name: success-success-2v62uq\n      container:\n        image: harbor.csis.astrazeneca.net/mlops/runnable:latest\n        command:\n          - runnable\n          - execute_single_node\n          - \"{{workflow.parameters.run_id}}\"\n          - success\n          - --log-level\n          - WARNING\n          - --file\n          - examples/concepts/nesting.yaml\n          - --config-file\n          - examples/configs/argo-config.yaml\n        volumeMounts:\n          - name: executor-0\n            mountPath: /mnt\n        imagePullPolicy: \"\"\n        resources:\n          limits:\n            memory: 1Gi\n            cpu: 250m\n          requests:\n            memory: 1Gi\n            cpu: 250m\n  templateDefaults:\n    activeDeadlineSeconds: 7200\n    timeout: 10800s\n  arguments:\n    parameters:\n      - name: run_id\n        value: \"{{workflow.uid}}\"\n  volumes:\n    - name: executor-0\n      persistentVolumeClaim:\n        claimName: runnable-volume\n</code></pre> <p> argo workflows UI showing the deeply nested workflows. </p>"},{"location":"configurations/executors/argo/#kubeflow","title":"Kubeflow","text":"<p>Kubeflow pipelines compiles workflows defined in SDK to Argo workflows and thereby has support for uploading argo workflows. Below is a screenshot of the map pipeline uploaded to Kubeflow.</p> <p> </p> argo workflows UI showing the map workflow definition. <p> </p> argo workflows UI showing the map workflow execution."},{"location":"configurations/executors/local-container/","title":"local-container","text":"<p>Execute all the steps of the pipeline in containers.</p> <ul> <li> Provides a way to test the containers and the execution of the pipeline in local environment.</li> <li> Any failure in cloud native container environments can be replicated in local environments.</li> <li> Ability to provide specialized compute environments for different steps of the pipeline.</li> <li> The scalability is still constrained by the resources in local environment.</li> </ul>"},{"location":"configurations/executors/local-container/#configuration","title":"Configuration","text":"<p>In the mode of local-container, we execute all the commands in a container.</p> <p>Ensure that the local compute has enough resources to finish all your jobs.</p> <p>Configuration options:</p> <pre><code>pipeline-executor:\n  type: local-container\n  config:\n    docker_image: &lt;required&gt;\n    auto_remove_container: true/false\n    environment:\n      key: value\n    overrides:\n      alternate_config:\n        docker_image: &lt;required&gt;\n        auto_remove_container: true/false\n        environment:\n          key: value\n</code></pre> <ul> <li><code>docker_image</code>: The default docker image to use for all the steps.</li> <li><code>auto_remove_container</code>: Remove container after execution</li> <li><code>environment</code>: Environment variables to pass to the container</li> </ul> <p>Overrides give you the ability to override the default docker image for a single step. A step can then then refer to the alternate_config in the task definition.</p> <p>Example:</p> <pre><code>from runnable import PythonTask\n\ntask = PythonTask(\n    name=\"alt_task\",\n    overrides={\n        \"local-container\": \"alternate_config\"\n        }\n    )\n</code></pre> <p>In the above example, <code>alt_task</code> will run in the docker image/configuration as defined in the alternate_config.</p> <p><code>runnable</code> does not build the docker image for you, it is still left for the user to build and ensure that the docker image provided is the correct one.</p> <p>Debugging</p> <p><code>auto_remove_container</code> allows you to run the failed container independently to identify the issue that caused the failure.</p> <p>All the examples in the concepts section can be executed using the below configuration:</p> <pre><code>pipeline-executor:\n  type: local-container\n  config:\n    docker_image: &lt;your docker image&gt;\n</code></pre>"},{"location":"configurations/executors/local-container/#dynamic_docker_image","title":"Dynamic docker image","text":"<p>The docker image can provided at the run time by using environmental variables.</p> <p>For example:</p> <pre><code>pipeline-executor:\n  type: local-container\n  config:\n    docker_image: $docker_image\n</code></pre> <p>The <code>$docker_image</code> will be replaced by the environmental variable <code>RUNNABLE_VAR_docker_image</code> during run time. The same rule applies to overrides too.</p>"},{"location":"configurations/executors/local/","title":"local","text":"<p>All the steps of the pipeline are executed in the local compute environment in the same shell as it was triggered.</p> <ul> <li> Provides the most comfortable environment for experimentation and development.</li> <li> The scalability is constrained by the local compute environment.</li> <li> Not possible to provide specialized compute environments for different steps of the pipeline.</li> <li> All the steps within <code>parallel</code> or <code>map</code> nodes are executed sequentially.</li> </ul>"},{"location":"configurations/executors/local/#configuration","title":"Configuration","text":"<p>In the mode of local execution, we run everything on the local computer.</p> <p>This has some serious implications on the amount of time it would take to complete the run. Also ensure that the local compute is good enough for the compute to happen of all the steps.</p> <p>Example config:</p> <pre><code>pipeline-executor:\n  type: local\n</code></pre> <p>All the examples in the concepts section are executed using <code>local</code> executors.</p>"},{"location":"configurations/executors/mocked/","title":"Mocked","text":"<p>Mocked executors provide a way to control the behavior of <code>task</code> node types to be either pass through or execute a alternate command with modified configurations.</p> <ul> <li> Runs the pipeline only in local environment.</li> <li> Enables unit testing of the pipeline in both yaml and SDK definitions.</li> <li> Isolates specific node(s) from the execution for further analysis.</li> <li> Not meant to be used for production deployments</li> </ul>"},{"location":"configurations/executors/mocked/#options","title":"Options","text":"<pre><code>executor: mocked\nconfig:\n  patches:\n    name of the name:\n      command_configuration:\n</code></pre> <p>By default, all the <code>task</code> steps are passed through without an execution. By providing <code>patches</code>, indexed by the name of the node, gives control on the command to run and the configuration of the command.</p>"},{"location":"configurations/executors/mocked/#command_configuration_for_notebook_nodes","title":"Command configuration for notebook nodes","text":"<p><code>python</code> and <code>shell</code> based tasks have no configuration options apart from the <code>command</code>. Notebook nodes have additional configuration options detailed in concepts. Ploomber engine provides rich options in debugging failed notebooks.</p>"},{"location":"configurations/executors/mocked/#example","title":"Example","text":""},{"location":"configurations/executors/mocked/#mocking_nodes","title":"Mocking nodes","text":"<p>The following example shows the simple case of mocking all the steps of the pipeline.</p> pipeline in yamlpython sdkMocked configurationRun log <p>You can execute the mocked pipeline by: <code>runnable execute -f examples/concepts/simple.yaml -c examples/configs/mocked-config-simple.yaml</code></p> <pre><code>\n</code></pre> <p>You can execute the mocked pipeline by:</p> <p><code>runnable_CONFIGURATION_FILE=examples/configs/mocked-config-simple.yaml python examples/concepts/simple.py</code></p> <pre><code>\n</code></pre> <pre><code>\n</code></pre> <p>The flag <code>mock</code> is set to be <code>true</code> for the execution of node simple which denotes that the task was mocked.</p> <pre><code>{\n    \"run_id\": \"minty-goodall-0528\",\n    \"dag_hash\": \"\",\n    \"use_cached\": false,\n    \"tag\": \"\",\n    \"original_run_id\": \"\",\n    \"status\": \"SUCCESS\",\n    \"steps\": {\n        \"simple\": {\n            \"name\": \"simple\",\n            \"internal_name\": \"simple\",\n            \"status\": \"SUCCESS\",\n            \"step_type\": \"task\",\n            \"message\": \"\",\n            \"mock\": true,\n            \"code_identities\": [\n                {\n                    \"code_identifier\": \"d76cf865af2f8e03b6c1205403351cbe42e6cdc4\",\n                    \"code_identifier_type\": \"git\",\n                    \"code_identifier_dependable\": true,\n                    \"code_identifier_url\": \"https://github.com/AstraZeneca/runnable-core.git\",\n                    \"code_identifier_message\": \"\"\n                }\n            ],\n            \"attempts\": [\n                {\n                    \"attempt_number\": 1,\n                    \"start_time\": \"2024-02-11 05:28:40.812597\",\n                    \"end_time\": \"2024-02-11 05:28:40.812627\",\n                    \"duration\": \"0:00:00.000030\",\n                    \"status\": \"SUCCESS\",\n                    \"message\": \"\",\n                    \"parameters\": {}\n                }\n            ],\n            \"user_defined_metrics\": {},\n            \"branches\": {},\n            \"data_catalog\": []\n        },\n        \"success\": {\n            \"name\": \"success\",\n            \"internal_name\": \"success\",\n            \"status\": \"SUCCESS\",\n            \"step_type\": \"success\",\n            \"message\": \"\",\n            \"mock\": false,\n            \"code_identities\": [\n                {\n                    \"code_identifier\": \"d76cf865af2f8e03b6c1205403351cbe42e6cdc4\",\n                    \"code_identifier_type\": \"git\",\n                    \"code_identifier_dependable\": true,\n                    \"code_identifier_url\": \"https://github.com/AstraZeneca/runnable-core.git\",\n                    \"code_identifier_message\": \"\"\n                }\n            ],\n            \"attempts\": [\n                {\n                    \"attempt_number\": 1,\n                    \"start_time\": \"2024-02-11 05:28:40.883909\",\n                    \"end_time\": \"2024-02-11 05:28:40.884310\",\n                    \"duration\": \"0:00:00.000401\",\n                    \"status\": \"SUCCESS\",\n                    \"message\": \"\",\n                    \"parameters\": {}\n                }\n            ],\n            \"user_defined_metrics\": {},\n            \"branches\": {},\n            \"data_catalog\": []\n        }\n    },\n    \"parameters\": {},\n    \"run_config\": {\n        \"executor\": {\n            \"service_name\": \"mocked\",\n            \"service_type\": \"executor\",\n            \"enable_parallel\": false,\n            \"overrides\": {},\n            \"patches\": {}\n        },\n        \"run_log_store\": {\n            \"service_name\": \"file-system\",\n            \"service_type\": \"run_log_store\",\n            \"log_folder\": \".run_log_store\"\n        },\n        \"secrets_handler\": {\n            \"service_name\": \"do-nothing\",\n            \"service_type\": \"secrets\"\n        },\n        \"catalog_handler\": {\n            \"service_name\": \"file-system\",\n            \"service_type\": \"catalog\",\n            \"catalog_location\": \".catalog\"\n        },\n        \"experiment_tracker\": {\n            \"service_name\": \"do-nothing\",\n            \"service_type\": \"experiment_tracker\"\n        },\n        \"pipeline_file\": \"\",\n        \"parameters_file\": \"\",\n        \"configuration_file\": \"examples/configs/mocked-config-simple.yaml\",\n        \"tag\": \"\",\n        \"run_id\": \"minty-goodall-0528\",\n        \"variables\": {},\n        \"use_cached\": false,\n        \"original_run_id\": \"\",\n        \"dag\": {\n            \"start_at\": \"simple\",\n            \"name\": \"\",\n            \"description\": \"\",\n            \"steps\": {\n                \"simple\": {\n                    \"type\": \"task\",\n                    \"name\": \"simple\",\n                    \"next\": \"success\",\n                    \"on_failure\": \"\",\n                    \"overrides\": {},\n                    \"catalog\": null,\n                    \"max_attempts\": 1,\n                    \"command\": \"examples.concepts.simple.simple_function\",\n                    \"command_type\": \"python\",\n                    \"node_name\": \"simple\"\n                },\n                \"success\": {\n                    \"type\": \"success\",\n                    \"name\": \"success\"\n                },\n                \"fail\": {\n                    \"type\": \"fail\",\n                    \"name\": \"fail\"\n                }\n            }\n        },\n        \"dag_hash\": \"\",\n        \"execution_plan\": \"chained\"\n    }\n}\n</code></pre>"},{"location":"configurations/executors/mocked/#patching_nodes_for_unit_testing","title":"Patching nodes for unit testing","text":"<p>Pipelines are themselves code and should be testable. In the below example, we take an example pipeline to test the behavior of the traversal.</p> <p>The below pipeline is designed to follow: <code>step 1 &gt;&gt; step 2 &gt;&gt; step 3</code> in case of no failures and <code>step 1 &gt;&gt; step3</code> in case of failure. The traversal is shown in concepts.</p> <p>Asserting Run log</p> <p>The run log is a simple json file that can be parsed and validated against designed behaviors. You can also create the <code>RunLog</code> object by deserializing <code>runnable.datastore.RunLog</code> from the json.</p> <p>This can be handy when validating complex pipelines.</p> pipeline in yamlpython sdkRun log with no mockingMocked configurationRun log with mocking <pre><code>\n</code></pre> <pre><code>\n</code></pre> <p>The <code>run log</code> has only <code>step 1</code> and <code>step 3</code> as part of the steps (as designed)  showing the behavior of the pipeline in case of failure. The status of <code>step 1</code> is  captured as <code>FAIL</code> due to <code>exit 1</code> command in the pipeline definition.</p> <pre><code>{\n    \"run_id\": \"selfish-pasteur-0559\",\n    \"dag_hash\": \"\",\n    \"use_cached\": false,\n    \"tag\": \"\",\n    \"original_run_id\": \"\",\n    \"status\": \"SUCCESS\",\n    \"steps\": {\n        \"step 1\": {\n            \"name\": \"step 1\",\n            \"internal_name\": \"step 1\",\n            \"status\": \"FAIL\",\n            \"step_type\": \"task\",\n            \"message\": \"\",\n            \"mock\": false,\n            \"code_identities\": [\n                {\n                    \"code_identifier\": \"d76cf865af2f8e03b6c1205403351cbe42e6cdc4\",\n                    \"code_identifier_type\": \"git\",\n                    \"code_identifier_dependable\": true,\n                    \"code_identifier_url\": \"https://github.com/AstraZeneca/runnable-core.git\",\n                    \"code_identifier_message\": \"\"\n                }\n            ],\n            \"attempts\": [\n                {\n                    \"attempt_number\": 1,\n                    \"start_time\": \"2024-02-11 05:59:08.382587\",\n                    \"end_time\": \"2024-02-11 05:59:08.446642\",\n                    \"duration\": \"0:00:00.064055\",\n                    \"status\": \"FAIL\",\n                    \"message\": \"Command failed\",\n                    \"parameters\": {}\n                }\n            ],\n            \"user_defined_metrics\": {},\n            \"branches\": {},\n            \"data_catalog\": [\n                {\n                    \"name\": \"step_1.execution.log\",\n                    \"data_hash\": \"e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\",\n                    \"catalog_relative_path\": \"selfish-pasteur-0559/step_1.execution.log\",\n                    \"catalog_handler_location\": \".catalog\",\n                    \"stage\": \"put\"\n                }\n            ]\n        },\n        \"step 3\": {\n            \"name\": \"step 3\",\n            \"internal_name\": \"step 3\",\n            \"status\": \"SUCCESS\",\n            \"step_type\": \"stub\",\n            \"message\": \"\",\n            \"mock\": false,\n            \"code_identities\": [\n                {\n                    \"code_identifier\": \"d76cf865af2f8e03b6c1205403351cbe42e6cdc4\",\n                    \"code_identifier_type\": \"git\",\n                    \"code_identifier_dependable\": true,\n                    \"code_identifier_url\": \"https://github.com/AstraZeneca/runnable-core.git\",\n                    \"code_identifier_message\": \"\"\n                }\n            ],\n            \"attempts\": [\n                {\n                    \"attempt_number\": 1,\n                    \"start_time\": \"2024-02-11 05:59:08.516318\",\n                    \"end_time\": \"2024-02-11 05:59:08.516333\",\n                    \"duration\": \"0:00:00.000015\",\n                    \"status\": \"SUCCESS\",\n                    \"message\": \"\",\n                    \"parameters\": {}\n                }\n            ],\n            \"user_defined_metrics\": {},\n            \"branches\": {},\n            \"data_catalog\": []\n        },\n        \"success\": {\n            \"name\": \"success\",\n            \"internal_name\": \"success\",\n            \"status\": \"SUCCESS\",\n            \"step_type\": \"success\",\n            \"message\": \"\",\n            \"mock\": false,\n            \"code_identities\": [\n                {\n                    \"code_identifier\": \"d76cf865af2f8e03b6c1205403351cbe42e6cdc4\",\n                    \"code_identifier_type\": \"git\",\n                    \"code_identifier_dependable\": true,\n                    \"code_identifier_url\": \"https://github.com/AstraZeneca/runnable-core.git\",\n                    \"code_identifier_message\": \"\"\n                }\n            ],\n            \"attempts\": [\n                {\n                    \"attempt_number\": 1,\n                    \"start_time\": \"2024-02-11 05:59:08.580478\",\n                    \"end_time\": \"2024-02-11 05:59:08.580555\",\n                    \"duration\": \"0:00:00.000077\",\n                    \"status\": \"SUCCESS\",\n                    \"message\": \"\",\n                    \"parameters\": {}\n                }\n            ],\n            \"user_defined_metrics\": {},\n            \"branches\": {},\n            \"data_catalog\": []\n        }\n    },\n    \"parameters\": {},\n    \"run_config\": {\n        \"executor\": {\n            \"service_name\": \"local\",\n            \"service_type\": \"executor\",\n            \"enable_parallel\": false,\n            \"overrides\": {}\n        },\n        \"run_log_store\": {\n            \"service_name\": \"buffered\",\n            \"service_type\": \"run_log_store\"\n        },\n        \"secrets_handler\": {\n            \"service_name\": \"do-nothing\",\n            \"service_type\": \"secrets\"\n        },\n        \"catalog_handler\": {\n            \"service_name\": \"file-system\",\n            \"service_type\": \"catalog\",\n            \"catalog_location\": \".catalog\"\n        },\n        \"experiment_tracker\": {\n            \"service_name\": \"do-nothing\",\n            \"service_type\": \"experiment_tracker\"\n        },\n        \"pipeline_file\": \"\",\n        \"parameters_file\": \"\",\n        \"configuration_file\": \"\",\n        \"tag\": \"\",\n        \"run_id\": \"selfish-pasteur-0559\",\n        \"variables\": {},\n        \"use_cached\": false,\n        \"original_run_id\": \"\",\n        \"dag\": {\n            \"start_at\": \"step 1\",\n            \"name\": \"\",\n            \"description\": \"\",\n            \"steps\": {\n                \"step 1\": {\n                    \"type\": \"task\",\n                    \"name\": \"step 1\",\n                    \"next\": \"step 2\",\n                    \"on_failure\": \"step 3\",\n                    \"overrides\": {},\n                    \"catalog\": null,\n                    \"max_attempts\": 1,\n                    \"command\": \"exit 1\",\n                    \"command_type\": \"shell\",\n                    \"node_name\": \"step 1\"\n                },\n                \"step 2\": {\n                    \"type\": \"stub\",\n                    \"name\": \"step 2\",\n                    \"next\": \"step 3\",\n                    \"on_failure\": \"\",\n                    \"overrides\": {},\n                    \"catalog\": null,\n                    \"max_attempts\": 1\n                },\n                \"step 3\": {\n                    \"type\": \"stub\",\n                    \"name\": \"step 3\",\n                    \"next\": \"success\",\n                    \"on_failure\": \"\",\n                    \"overrides\": {},\n                    \"catalog\": null,\n                    \"max_attempts\": 1\n                },\n                \"success\": {\n                    \"type\": \"success\",\n                    \"name\": \"success\"\n                },\n                \"fail\": {\n                    \"type\": \"fail\",\n                    \"name\": \"fail\"\n                }\n            }\n        },\n        \"dag_hash\": \"\",\n        \"execution_plan\": \"chained\"\n    }\n}\n</code></pre> <p>We can patch the command of step 1 to be successful to test the behavior of traversal in case of no failures.</p> <p>Running the pipeline with mocked configuration:</p> <p>for yaml: <code>runnable execute -f examples/on-failure.yaml -c examples/configs/mocked-config-unittest.yaml</code></p> <p>for python: <code>runnable_CONFIGURATION_FILE=examples/configs/mocked-config-unittest.yaml python examples/on_failure.py</code></p> <pre><code>\n</code></pre> <p>As seen in the <code>run log</code>, the steps have <code>step 1</code>, <code>step 2</code>, <code>step 3</code> as executed and successful steps. And the status of <code>step 1</code> is <code>SUCCESS</code>.</p> <pre><code>{\n    \"run_id\": \"syrupy-aryabhata-0552\",\n    \"dag_hash\": \"026b36dd2b3507fe586f1f85ba308f817745c465\",\n    \"use_cached\": false,\n    \"tag\": \"\",\n    \"original_run_id\": \"\",\n    \"status\": \"SUCCESS\",\n    \"steps\": {\n        \"step 1\": {\n            \"name\": \"step 1\",\n            \"internal_name\": \"step 1\",\n            \"status\": \"SUCCESS\",\n            \"step_type\": \"task\",\n            \"message\": \"\",\n            \"mock\": false,\n            \"code_identities\": [\n                {\n                    \"code_identifier\": \"d76cf865af2f8e03b6c1205403351cbe42e6cdc4\",\n                    \"code_identifier_type\": \"git\",\n                    \"code_identifier_dependable\": true,\n                    \"code_identifier_url\": \"https://github.com/AstraZeneca/runnable-core.git\",\n                    \"code_identifier_message\": \"\"\n                }\n            ],\n            \"attempts\": [\n                {\n                    \"attempt_number\": 1,\n                    \"start_time\": \"2024-02-11 05:52:19.421358\",\n                    \"end_time\": \"2024-02-11 05:52:19.426678\",\n                    \"duration\": \"0:00:00.005320\",\n                    \"status\": \"SUCCESS\",\n                    \"message\": \"\",\n                    \"parameters\": {}\n                }\n            ],\n            \"user_defined_metrics\": {},\n            \"branches\": {},\n            \"data_catalog\": [\n                {\n                    \"name\": \"step_1.execution.log\",\n                    \"data_hash\": \"e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\",\n                    \"catalog_relative_path\": \"syrupy-aryabhata-0552/step_1.execution.log\",\n                    \"catalog_handler_location\": \".catalog\",\n                    \"stage\": \"put\"\n                }\n            ]\n        },\n        \"step 2\": {\n            \"name\": \"step 2\",\n            \"internal_name\": \"step 2\",\n            \"status\": \"SUCCESS\",\n            \"step_type\": \"stub\",\n            \"message\": \"\",\n            \"mock\": true,\n            \"code_identities\": [\n                {\n                    \"code_identifier\": \"d76cf865af2f8e03b6c1205403351cbe42e6cdc4\",\n                    \"code_identifier_type\": \"git\",\n                    \"code_identifier_dependable\": true,\n                    \"code_identifier_url\": \"https://github.com/AstraZeneca/runnable-core.git\",\n                    \"code_identifier_message\": \"\"\n                }\n            ],\n            \"attempts\": [\n                {\n                    \"attempt_number\": 1,\n                    \"start_time\": \"2024-02-11 05:52:19.500544\",\n                    \"end_time\": \"2024-02-11 05:52:19.500559\",\n                    \"duration\": \"0:00:00.000015\",\n                    \"status\": \"SUCCESS\",\n                    \"message\": \"\",\n                    \"parameters\": {}\n                }\n            ],\n            \"user_defined_metrics\": {},\n            \"branches\": {},\n            \"data_catalog\": []\n        },\n        \"step 3\": {\n            \"name\": \"step 3\",\n            \"internal_name\": \"step 3\",\n            \"status\": \"SUCCESS\",\n            \"step_type\": \"stub\",\n            \"message\": \"\",\n            \"mock\": true,\n            \"code_identities\": [\n                {\n                    \"code_identifier\": \"d76cf865af2f8e03b6c1205403351cbe42e6cdc4\",\n                    \"code_identifier_type\": \"git\",\n                    \"code_identifier_dependable\": true,\n                    \"code_identifier_url\": \"https://github.com/AstraZeneca/runnable-core.git\",\n                    \"code_identifier_message\": \"\"\n                }\n            ],\n            \"attempts\": [\n                {\n                    \"attempt_number\": 1,\n                    \"start_time\": \"2024-02-11 05:52:19.577734\",\n                    \"end_time\": \"2024-02-11 05:52:19.577749\",\n                    \"duration\": \"0:00:00.000015\",\n                    \"status\": \"SUCCESS\",\n                    \"message\": \"\",\n                    \"parameters\": {}\n                }\n            ],\n            \"user_defined_metrics\": {},\n            \"branches\": {},\n            \"data_catalog\": []\n        },\n        \"success\": {\n            \"name\": \"success\",\n            \"internal_name\": \"success\",\n            \"status\": \"SUCCESS\",\n            \"step_type\": \"success\",\n            \"message\": \"\",\n            \"mock\": false,\n            \"code_identities\": [\n                {\n                    \"code_identifier\": \"d76cf865af2f8e03b6c1205403351cbe42e6cdc4\",\n                    \"code_identifier_type\": \"git\",\n                    \"code_identifier_dependable\": true,\n                    \"code_identifier_url\": \"https://github.com/AstraZeneca/runnable-core.git\",\n                    \"code_identifier_message\": \"\"\n                }\n            ],\n            \"attempts\": [\n                {\n                    \"attempt_number\": 1,\n                    \"start_time\": \"2024-02-11 05:52:19.649764\",\n                    \"end_time\": \"2024-02-11 05:52:19.650318\",\n                    \"duration\": \"0:00:00.000554\",\n                    \"status\": \"SUCCESS\",\n                    \"message\": \"\",\n                    \"parameters\": {}\n                }\n            ],\n            \"user_defined_metrics\": {},\n            \"branches\": {},\n            \"data_catalog\": []\n        }\n    },\n    \"parameters\": {},\n    \"run_config\": {\n        \"executor\": {\n            \"service_name\": \"mocked\",\n            \"service_type\": \"executor\",\n            \"enable_parallel\": false,\n            \"overrides\": {},\n            \"patches\": {\n                \"step 1\": {\n                    \"command\": \"exit 0\"\n                }\n            }\n        },\n        \"run_log_store\": {\n            \"service_name\": \"file-system\",\n            \"service_type\": \"run_log_store\",\n            \"log_folder\": \".run_log_store\"\n        },\n        \"secrets_handler\": {\n            \"service_name\": \"do-nothing\",\n            \"service_type\": \"secrets\"\n        },\n        \"catalog_handler\": {\n            \"service_name\": \"file-system\",\n            \"service_type\": \"catalog\",\n            \"catalog_location\": \".catalog\"\n        },\n        \"experiment_tracker\": {\n            \"service_name\": \"do-nothing\",\n            \"service_type\": \"experiment_tracker\"\n        },\n        \"pipeline_file\": \"examples/on-failure.yaml\",\n        \"parameters_file\": null,\n        \"configuration_file\": \"examples/configs/mocked-config-unittest.yaml\",\n        \"tag\": \"\",\n        \"run_id\": \"syrupy-aryabhata-0552\",\n        \"variables\": {},\n        \"use_cached\": false,\n        \"original_run_id\": \"\",\n        \"dag\": {\n            \"start_at\": \"step 1\",\n            \"name\": \"\",\n            \"description\": \"This is a simple pipeline to demonstrate failure in a step.\\n\\nThe default behavior is to traverse to step type fail and mark the run as\nfailed.\\nBut you can control it by providing on_failure.\\n\\nIn this example: step 1 fails and moves to step 3 skipping step 2. The pipeline status\\nis considered to be\nsuccess.\\n\\nstep 1 (FAIL) &gt;&gt; step 3 &gt;&gt; success\\n\\nYou can run this pipeline by runnable execute -f examples/on-failure.yaml\\n\",\n            \"steps\": {\n                \"step 1\": {\n                    \"type\": \"task\",\n                    \"name\": \"step 1\",\n                    \"next\": \"step 2\",\n                    \"on_failure\": \"step 3\",\n                    \"overrides\": {},\n                    \"catalog\": null,\n                    \"max_attempts\": 1,\n                    \"command_type\": \"shell\",\n                    \"command\": \"exit 1\",\n                    \"node_name\": \"step 1\"\n                },\n                \"step 2\": {\n                    \"type\": \"stub\",\n                    \"name\": \"step 2\",\n                    \"next\": \"step 3\",\n                    \"on_failure\": \"\",\n                    \"overrides\": {},\n                    \"catalog\": null,\n                    \"max_attempts\": 1\n                },\n                \"step 3\": {\n                    \"type\": \"stub\",\n                    \"name\": \"step 3\",\n                    \"next\": \"success\",\n                    \"on_failure\": \"\",\n                    \"overrides\": {},\n                    \"catalog\": null,\n                    \"max_attempts\": 1\n                },\n                \"success\": {\n                    \"type\": \"success\",\n                    \"name\": \"success\"\n                },\n                \"fail\": {\n                    \"type\": \"fail\",\n                    \"name\": \"fail\"\n                }\n            }\n        },\n        \"dag_hash\": \"026b36dd2b3507fe586f1f85ba308f817745c465\",\n        \"execution_plan\": \"chained\"\n    }\n}\n</code></pre>"},{"location":"configurations/executors/mocked/#debugging_failed_executions","title":"Debugging failed executions","text":"<p>Using debuggers</p> <p>For pipelines defined by the python SDK, you can create breakpoints at the python function being executed and use debuggers.</p> <p>For <code>notebook</code> based tasks, refer to ploomber engine documentation for rich debuggers.</p> <p>Shell commands can be run in isolation by providing the parameters as environment variables and catalog artifacts present in the <code>compute_data_folder</code> location.</p> <p>To debug a failed execution, we can use the mocked executor to mock all the steps except for the failed step and providing the parameters and data exposed to the step during the failure which are captured by the <code>run log</code> and <code>catalog</code>.</p> Faulty pipelineFaulty run logmocked configurationDebugging failed executions <pre><code>\n</code></pre> <pre><code>{\n    \"run_id\": \"wrong-file-name\",\n    \"dag_hash\": \"7b12d64874eff2072c9dd97912a17149f2c32ed2\",\n    \"use_cached\": false,\n    \"tag\": \"\",\n    \"original_run_id\": \"\",\n    \"status\": \"FAIL\",\n    \"steps\": {\n        \"Setup\": {\n            \"name\": \"Setup\",\n            \"internal_name\": \"Setup\",\n            \"status\": \"SUCCESS\",\n            \"step_type\": \"task\",\n            \"message\": \"\",\n            \"mock\": false,\n            \"code_identities\": [\n                {\n                    \"code_identifier\": \"d76cf865af2f8e03b6c1205403351cbe42e6cdc4\",\n                    \"code_identifier_type\": \"git\",\n                    \"code_identifier_dependable\": true,\n                    \"code_identifier_url\": \"https://github.com/AstraZeneca/runnable-core.git\",\n                    \"code_identifier_message\": \"\"\n                }\n            ],\n            \"attempts\": [\n                {\n                    \"attempt_number\": 1,\n                    \"start_time\": \"2024-02-11 23:03:00.417889\",\n                    \"end_time\": \"2024-02-11 23:03:00.429579\",\n                    \"duration\": \"0:00:00.011690\",\n                    \"status\": \"SUCCESS\",\n                    \"message\": \"\",\n                    \"parameters\": {}\n                }\n            ],\n            \"user_defined_metrics\": {},\n            \"branches\": {},\n            \"data_catalog\": [\n                {\n                    \"name\": \"Setup.execution.log\",\n                    \"data_hash\": \"d2dd9105fa3c62c35d89182c44fbd1ec992d8d408e38f0350d582fa29ed88074\",\n                    \"catalog_relative_path\": \"wrong-file-name/Setup.execution.log\",\n                    \"catalog_handler_location\": \".catalog\",\n                    \"stage\": \"put\"\n                }\n            ]\n        },\n        \"Create Content\": {\n            \"name\": \"Create Content\",\n            \"internal_name\": \"Create Content\",\n            \"status\": \"SUCCESS\",\n            \"step_type\": \"task\",\n            \"message\": \"\",\n            \"mock\": false,\n            \"code_identities\": [\n                {\n                    \"code_identifier\": \"d76cf865af2f8e03b6c1205403351cbe42e6cdc4\",\n                    \"code_identifier_type\": \"git\",\n                    \"code_identifier_dependable\": true,\n                    \"code_identifier_url\": \"https://github.com/AstraZeneca/runnable-core.git\",\n                    \"code_identifier_message\": \"\"\n                }\n            ],\n            \"attempts\": [\n                {\n                    \"attempt_number\": 1,\n                    \"start_time\": \"2024-02-11 23:03:00.507067\",\n                    \"end_time\": \"2024-02-11 23:03:00.514757\",\n                    \"duration\": \"0:00:00.007690\",\n                    \"status\": \"SUCCESS\",\n                    \"message\": \"\",\n                    \"parameters\": {}\n                }\n            ],\n            \"user_defined_metrics\": {},\n            \"branches\": {},\n            \"data_catalog\": [\n                {\n                    \"name\": \"Create_Content.execution.log\",\n                    \"data_hash\": \"d2dd9105fa3c62c35d89182c44fbd1ec992d8d408e38f0350d582fa29ed88074\",\n                    \"catalog_relative_path\": \"wrong-file-name/Create_Content.execution.log\",\n                    \"catalog_handler_location\": \".catalog\",\n                    \"stage\": \"put\"\n                },\n                {\n                    \"name\": \"data/hello.txt\",\n                    \"data_hash\": \"2ac8edfe4eb5d0d9392cb070664c31c45eecca78c43cb99d2d9c6f5a8c813932\",\n                    \"catalog_relative_path\": \"wrong-file-name/data/hello.txt\",\n                    \"catalog_handler_location\": \".catalog\",\n                    \"stage\": \"put\"\n                }\n            ]\n        },\n        \"Retrieve Content\": {\n            \"name\": \"Retrieve Content\",\n            \"internal_name\": \"Retrieve Content\",\n            \"status\": \"FAIL\",\n            \"step_type\": \"task\",\n            \"message\": \"\",\n            \"mock\": false,\n            \"code_identities\": [\n                {\n                    \"code_identifier\": \"d76cf865af2f8e03b6c1205403351cbe42e6cdc4\",\n                    \"code_identifier_type\": \"git\",\n                    \"code_identifier_dependable\": true,\n                    \"code_identifier_url\": \"https://github.com/AstraZeneca/runnable-core.git\",\n                    \"code_identifier_message\": \"\"\n                }\n            ],\n            \"attempts\": [\n                {\n                    \"attempt_number\": 1,\n                    \"start_time\": \"2024-02-11 23:03:00.595992\",\n                    \"end_time\": \"2024-02-11 23:03:00.645752\",\n                    \"duration\": \"0:00:00.049760\",\n                    \"status\": \"FAIL\",\n                    \"message\": \"Command failed\",\n                    \"parameters\": {}\n                }\n            ],\n            \"user_defined_metrics\": {},\n            \"branches\": {},\n            \"data_catalog\": [\n                {\n                    \"name\": \"data/hello.txt\",\n                    \"data_hash\": \"2ac8edfe4eb5d0d9392cb070664c31c45eecca78c43cb99d2d9c6f5a8c813932\",\n                    \"catalog_relative_path\": \"data/hello.txt\",\n                    \"catalog_handler_location\": \".catalog\",\n                    \"stage\": \"get\"\n                },\n                {\n                    \"name\": \"Retrieve_Content.execution.log\",\n                    \"data_hash\": \"e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\",\n                    \"catalog_relative_path\": \"wrong-file-name/Retrieve_Content.execution.log\",\n                    \"catalog_handler_location\": \".catalog\",\n                    \"stage\": \"put\"\n                }\n            ]\n        },\n        \"fail\": {\n            \"name\": \"fail\",\n            \"internal_name\": \"fail\",\n            \"status\": \"SUCCESS\",\n            \"step_type\": \"fail\",\n            \"message\": \"\",\n            \"mock\": false,\n            \"code_identities\": [\n                {\n                    \"code_identifier\": \"d76cf865af2f8e03b6c1205403351cbe42e6cdc4\",\n                    \"code_identifier_type\": \"git\",\n                    \"code_identifier_dependable\": true,\n                    \"code_identifier_url\": \"https://github.com/AstraZeneca/runnable-core.git\",\n                    \"code_identifier_message\": \"\"\n                }\n            ],\n            \"attempts\": [\n                {\n                    \"attempt_number\": 1,\n                    \"start_time\": \"2024-02-11 23:03:00.727316\",\n                    \"end_time\": \"2024-02-11 23:03:00.727911\",\n                    \"duration\": \"0:00:00.000595\",\n                    \"status\": \"SUCCESS\",\n                    \"message\": \"\",\n                    \"parameters\": {}\n                }\n            ],\n            \"user_defined_metrics\": {},\n            \"branches\": {},\n            \"data_catalog\": []\n        }\n    },\n    \"parameters\": {},\n    \"run_config\": {\n        \"executor\": {\n            \"service_name\": \"local\",\n            \"service_type\": \"executor\",\n            \"enable_parallel\": false,\n            \"overrides\": {}\n        },\n        \"run_log_store\": {\n            \"service_name\": \"file-system\",\n            \"service_type\": \"run_log_store\",\n            \"log_folder\": \".run_log_store\"\n        },\n        \"secrets_handler\": {\n            \"service_name\": \"do-nothing\",\n            \"service_type\": \"secrets\"\n        },\n        \"catalog_handler\": {\n            \"service_name\": \"file-system\",\n            \"service_type\": \"catalog\",\n            \"catalog_location\": \".catalog\"\n        },\n        \"experiment_tracker\": {\n            \"service_name\": \"do-nothing\",\n            \"service_type\": \"experiment_tracker\"\n        },\n        \"pipeline_file\": \"examples/retry-fail.yaml\",\n        \"parameters_file\": null,\n        \"configuration_file\": \"examples/configs/fs-catalog-run_log.yaml\",\n        \"tag\": \"\",\n        \"run_id\": \"wrong-file-name\",\n        \"variables\": {},\n        \"use_cached\": false,\n        \"original_run_id\": \"\",\n        \"dag\": {\n            \"start_at\": \"Setup\",\n            \"name\": \"\",\n            \"description\": \"This is a simple pipeline that demonstrates retrying failures.\\n\\n1. Setup: We setup a data folder, we ignore if it is already present\\n2. Create Content: We create a \\\"hello.txt\\\" and \\\"put\\\" the file in catalog\\n3. Retrieve Content: We \\\"get\\\" the file \\\"hello.txt\\\" from the catalog and show the contents\\n5. Cleanup: We remove the data folder. Note that this is stubbed to prevent accidental deletion.\\n\\n\\nYou can run this pipeline by:\\n   runnable execute -f examples/retry-fail.yaml -c examples/configs/fs-catalog-run_log.yaml \\\\\\n    --run-id wrong-file-name\\n\",\n            \"steps\": {\n                \"Setup\": {\n                    \"type\": \"task\",\n                    \"name\": \"Setup\",\n                    \"next\": \"Create Content\",\n                    \"on_failure\": \"\",\n                    \"overrides\": {},\n                    \"catalog\": null,\n                    \"max_attempts\": 1,\n                    \"command_type\": \"shell\",\n                    \"command\": \"mkdir -p data\",\n                    \"node_name\": \"Setup\"\n                },\n                \"Create Content\": {\n                    \"type\": \"task\",\n                    \"name\": \"Create Content\",\n                    \"next\": \"Retrieve Content\",\n                    \"on_failure\": \"\",\n                    \"overrides\": {},\n                    \"catalog\": {\n                        \"get\": [],\n                        \"put\": [\n                            \"data/hello.txt\"\n                        ]\n                    },\n                    \"max_attempts\": 1,\n                    \"command_type\": \"shell\",\n                    \"command\": \"echo \\\"Hello from runnable\\\" &gt;&gt; data/hello.txt\\n\",\n                    \"node_name\": \"Create Content\"\n                },\n                \"Retrieve Content\": {\n                    \"type\": \"task\",\n                    \"name\": \"Retrieve Content\",\n                    \"next\": \"success\",\n                    \"on_failure\": \"\",\n                    \"overrides\": {},\n                    \"catalog\": {\n                        \"get\": [\n                            \"data/hello.txt\"\n                        ],\n                        \"put\": []\n                    },\n                    \"max_attempts\": 1,\n                    \"command_type\": \"shell\",\n                    \"command\": \"cat data/hello1.txt\",\n                    \"node_name\": \"Retrieve Content\"\n                },\n                \"success\": {\n                    \"type\": \"success\",\n                    \"name\": \"success\"\n                },\n                \"fail\": {\n                    \"type\": \"fail\",\n                    \"name\": \"fail\"\n                }\n            }\n        },\n        \"dag_hash\": \"7b12d64874eff2072c9dd97912a17149f2c32ed2\",\n        \"execution_plan\": \"chained\"\n    }\n}\n</code></pre> <pre><code>\n</code></pre> <p>Copy the catalog during the failed execution to the debugging execution and retry the step. We give it a run_id <code>debug-pipeline</code></p> <p>cp .catalog/wrong-file-name debug-pipeline</p> <p>and retry with the fix:</p> <p><code>runnable execute -f examples/retry-fail.yaml -c examples/configs/mocked-config-debug.yaml --run-id debug-pipeline</code></p>"}]}