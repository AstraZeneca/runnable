{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Runnable","text":"Orchestrate your functions, notebooks, scripts anywhere!! <p> Runner icons created by Leremy - Flaticon </p> <p>Transform any Python function into a portable, trackable pipeline in seconds.</p>"},{"location":"#step_1_install","title":"Step 1: Install","text":"<pre><code>pip install runnable\n</code></pre> <p>Optional Features</p> <p>Install optional features as needed: <pre><code>pip install runnable[notebook]    # Jupyter notebook execution\npip install runnable[docker]     # Container execution\npip install runnable[k8s]        # Kubernetes job executors\npip install runnable[s3]         # S3 storage backend\npip install runnable[examples]   # Example dependencies\n</code></pre></p>"},{"location":"#step_2_your_function_unchanged","title":"Step 2: Your Function (unchanged!)","text":"<pre><code># Your existing function - zero changes needed\ndef analyze_sales():\n    total_revenue = 50000\n    best_product = \"widgets\"\n    return total_revenue, best_product\n</code></pre>"},{"location":"#step_3_make_it_runnable","title":"Step 3: Make It Runnable","text":"<pre><code># Add main function \u2192 Make it runnable everywhere\nfrom runnable import PythonJob\n\ndef main():\n    job = PythonJob(function=analyze_sales)\n    job.execute()\n    return job  # REQUIRED: Always return the job object\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"#success","title":"\ud83c\udf89 Success!","text":"<p>You just made your first function runnable and got:</p> <ul> <li>\u2705 Automatic tracking: execution logs, timestamps, results saved</li> <li>\u2705 Reproducible runs: full execution history and metadata</li> <li>\u2705 Environment portability: runs the same on laptop, containers, Kubernetes</li> </ul> <p>Your code now runs anywhere without changes!</p>"},{"location":"#want_to_see_more","title":"Want to See More?","text":""},{"location":"#same_code_different_parameters","title":"\ud83d\udd27 Same Code, Different Parameters","text":"<p>Change parameters without touching your code:</p> <pre><code># Function accepts parameters\ndef forecast_growth(revenue, growth_rate):\n    return revenue * (1 + growth_rate) ** 3\n\nfrom runnable import PythonJob\n\ndef main():\n    job = PythonJob(function=forecast_growth)\n    job.execute()\n    return job  # REQUIRED: Always return the job object\n\nif __name__ == \"__main__\":\n    main()\n\n# Run different scenarios anywhere:\n# Local: RUNNABLE_PRM_revenue=100000 RUNNABLE_PRM_growth_rate=0.05 python forecast.py\n# Container: same command, same results\n# Kubernetes: same command, same results\n\n# \u2728 Every run tracked with parameters - reproducible everywhere\n</code></pre> See complete parameter example examples/11-jobs/passing_parameters_python.py<pre><code>\"\"\"\nThe below example shows how to set/get parameters in python\ntasks of the pipeline.\n\nThe function, set_parameter, returns\n    - JSON serializable types\n    - pydantic models\n    - pandas dataframe, any \"object\" type\n\npydantic models are implicitly handled by runnable\nbut \"object\" types should be marked as \"pickled\".\n\nUse pickled even for python data types is advised for\nreasonably large collections.\n\nRun the below example as:\n    python examples/03-parameters/passing_parameters_python.py\n\n\"\"\"\n\nfrom examples.common.functions import write_parameter\nfrom runnable import PythonJob, metric, pickled\n\n\ndef main():\n    job = PythonJob(\n        function=write_parameter,\n        returns=[\n            pickled(\"df\"),\n            \"integer\",\n            \"floater\",\n            \"stringer\",\n            \"pydantic_param\",\n            metric(\"score\"),\n        ],\n    )\n\n    job.execute()\n\n    return job\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Try it: <code>uv run examples/11-jobs/passing_parameters_python.py</code></p> <p>Why bother? No more \"what parameters gave us those good results?\" - tracked automatically across all environments.</p>"},{"location":"#chain_functions_no_glue_code","title":"\ud83d\udd17 Chain Functions, No Glue Code","text":"<p>Build workflows that run anywhere unchanged:</p> <pre><code># Your existing functions\ndef load_customer_data():\n    customers = {\"count\": 1500, \"segments\": [\"premium\", \"standard\"]}\n    return customers\n\ndef analyze_segments(customer_data):  # Name matches = automatic connection\n    analysis = {\"premium_pct\": 30, \"growth_potential\": \"high\"}\n    return analysis\n\n# What you used to write (glue code):\n# customer_data = load_customer_data()\n# analysis = analyze_segments(customer_data)\n\n# What Runnable needs (same logic, no glue):\nfrom runnable import Pipeline, PythonTask\n\ndef main():\n    pipeline = Pipeline(steps=[\n        PythonTask(function=load_customer_data, returns=[\"customer_data\"]),\n        PythonTask(function=analyze_segments, returns=[\"analysis\"])\n    ])\n    pipeline.execute()\n    return pipeline  # REQUIRED: Always return the pipeline object\n\nif __name__ == \"__main__\":\n    main()\n\n# Same pipeline runs unchanged on:\n# \u2022 Your laptop (development)\n# \u2022 Docker containers (testing)\n# \u2022 Kubernetes (production)\n\n# \u2728 Write once, run anywhere - zero deployment rewrites\n</code></pre> See complete pipeline example examples/02-sequential/traversal.py<pre><code>\"\"\"\nYou can execute this pipeline by:\n\n    python examples/02-sequential/traversal.py\n\nA pipeline can have any \"tasks\" as part of it. In the\nbelow example, we have a mix of stub, python, shell and notebook tasks.\n\nAs with simpler tasks, the stdout and stderr of each task are captured\nand stored in the catalog.\n\"\"\"\n\nfrom examples.common.functions import hello\nfrom runnable import NotebookTask, Pipeline, PythonTask, ShellTask, Stub\n\n\ndef main():\n    stub_task = Stub(name=\"hello stub\")  # [concept:stub-task]\n\n    python_task = PythonTask(  # [concept:python-task]\n        name=\"hello python\", function=hello, overrides={\"argo\": \"smaller\"}\n    )\n\n    shell_task = ShellTask(  # [concept:shell-task]\n        name=\"hello shell\",\n        command=\"echo 'Hello World!'\",\n    )\n\n    notebook_task = NotebookTask(  # [concept:notebook-task]\n        name=\"hello notebook\",\n        notebook=\"examples/common/simple_notebook.ipynb\",\n    )\n\n    # The pipeline has a mix of tasks.\n    # The order of execution follows the order of the tasks in the list.\n    pipeline = Pipeline(  # [concept:pipeline]\n        steps=[  # (2)\n            stub_task,  # (1)\n            python_task,\n            shell_task,\n            notebook_task,\n        ]\n    )\n\n    pipeline.execute()  # [concept:execution]\n\n    return pipeline\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Try it: <code>uv run examples/02-sequential/traversal.py</code></p> <p>Why bother? No more \"it works locally but breaks in production\" - same code, guaranteed same behavior.</p>"},{"location":"#mix_python_notebooks","title":"\ud83d\ude80 Mix Python + Notebooks","text":"<p>Different tools, portable workflows:</p> <pre><code># Python prepares data, notebook analyzes - works everywhere\ndef prepare_dataset():\n    clean_data = {\"sales\": [100, 200, 300], \"regions\": [\"north\", \"south\"]}\n    return clean_data\n\nfrom runnable import Pipeline, PythonTask, NotebookTask\n\ndef main():\n    pipeline = Pipeline(steps=[\n        PythonTask(function=prepare_dataset, returns=[\"dataset\"]),\n        NotebookTask(notebook=\"deep_analysis.ipynb\", returns=[\"insights\"])\n    ])\n    pipeline.execute()\n    return pipeline  # REQUIRED: Always return the pipeline object\n\nif __name__ == \"__main__\":\n    main()\n\n# This exact pipeline runs unchanged on:\n# \u2022 Local Jupyter setup\n# \u2022 Containerized environments\n# \u2022 Cloud Kubernetes clusters\n\n# \u2728 No more environment setup headaches or \"works on my machine\"\n</code></pre> See complete mixed workflow examples/02-sequential/traversal.py<pre><code>\"\"\"\nYou can execute this pipeline by:\n\n    python examples/02-sequential/traversal.py\n\nA pipeline can have any \"tasks\" as part of it. In the\nbelow example, we have a mix of stub, python, shell and notebook tasks.\n\nAs with simpler tasks, the stdout and stderr of each task are captured\nand stored in the catalog.\n\"\"\"\n\nfrom examples.common.functions import hello\nfrom runnable import NotebookTask, Pipeline, PythonTask, ShellTask, Stub\n\n\ndef main():\n    stub_task = Stub(name=\"hello stub\")  # [concept:stub-task]\n\n    python_task = PythonTask(  # [concept:python-task]\n        name=\"hello python\", function=hello, overrides={\"argo\": \"smaller\"}\n    )\n\n    shell_task = ShellTask(  # [concept:shell-task]\n        name=\"hello shell\",\n        command=\"echo 'Hello World!'\",\n    )\n\n    notebook_task = NotebookTask(  # [concept:notebook-task]\n        name=\"hello notebook\",\n        notebook=\"examples/common/simple_notebook.ipynb\",\n    )\n\n    # The pipeline has a mix of tasks.\n    # The order of execution follows the order of the tasks in the list.\n    pipeline = Pipeline(  # [concept:pipeline]\n        steps=[  # (2)\n            stub_task,  # (1)\n            python_task,\n            shell_task,\n            notebook_task,\n        ]\n    )\n\n    pipeline.execute()  # [concept:execution]\n\n    return pipeline\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Try it: <code>uv run examples/02-sequential/traversal.py</code></p> <p>Why bother? Your entire data science workflow becomes truly portable - no environment-specific rewrites.</p>"},{"location":"#complete_working_examples","title":"\ud83d\udd0d Complete Working Examples","text":"<p>All examples in this documentation are fully working code! Every code snippet comes from the <code>examples/</code> directory with complete, tested implementations.</p> <p>Repository Examples</p> <p>\ud83d\udcc1 Browse All Examples</p> <p>Complete, tested examples organized by topic:</p> <ul> <li><code>examples/01-tasks/</code> - Basic task types (Python, notebooks, shell scripts)</li> <li><code>examples/02-sequential/</code> - Multi-step workflows and conditional logic</li> <li><code>examples/03-parameters/</code> - Configuration and parameter passing</li> <li><code>examples/04-catalog/</code> - File storage and data management</li> <li><code>examples/06-parallel/</code> - Parallel execution patterns</li> <li><code>examples/07-map/</code> - Iterative processing over data</li> <li><code>examples/10-loop/</code> - Loop workflows with dynamic iteration</li> <li><code>examples/11-jobs/</code> - Single job execution examples</li> <li><code>examples/configs/</code> - Configuration files for different environments</li> </ul> <p>\ud83d\udccb All examples include:</p> <ul> <li>\u2705 Complete Python code following the correct patterns</li> <li>\u2705 Configuration files for different execution environments</li> <li>\u2705 Instructions on how to run them with <code>uv run</code></li> <li>\u2705 Tested in CI to ensure they always work</li> </ul> <p>\ud83d\ude80 Quick Start: Pick any example and run it immediately: <pre><code>git clone https://github.com/AstraZeneca/runnable.git\ncd runnable\nuv run examples/01-tasks/python_tasks.py\n</code></pre></p>"},{"location":"#batch_processing_experimental_async_proof_of_concept","title":"\ud83d\udd04 Batch Processing + \ud83e\uddea Experimental Async (Proof of Concept)","text":"<p>Runnable provides robust batch processing with experimental async streaming capabilities as a proof of concept.</p> <p>Not Recommended for Production Agentic Frameworks</p> <p>While Runnable includes experimental async capabilities, these are primarily a proof of possibility rather than production-ready features for agentic frameworks.</p> <p>For production agentic applications, we recommend using mature, purpose-built frameworks like:</p> <ul> <li>Pydantic AI - Production-ready async agent framework</li> <li>LangChain - Comprehensive LLM application framework</li> <li>CrewAI - Multi-agent orchestration framework</li> </ul> <p>Runnable's async features are experimental, local-only, and subject to change without notice.</p> <p>Runnable's strength lies in data pipeline orchestration:</p> <p>\ud83d\udd04 Batch Processing - Production-ready data pipelines with full reproducibility <pre><code># Regular batch pipeline - Runnable's core strength\npipeline = Pipeline(steps=[\n    PythonTask(function=process_data, name=\"process\"),\n    PythonTask(function=train_model, name=\"train\")\n])\npipeline.execute()  # Runs to completion\n</code></pre></p> <p>\ud83e\uddea Experimental Async - Proof of concept for streaming (local execution only) <pre><code># Experimental async - NOT recommended for production agents\npipeline = AsyncPipeline(steps=[\n    AsyncPythonTask(function=stream_llm_response, name=\"llm\")\n])\nasync for event in pipeline.execute_streaming():\n    print(event)  # {\"type\": \"chunk\", \"text\": \"Hello\"}\n</code></pre></p> <p>\ud83c\udfaf Runnable is designed for:</p> <ul> <li>Data processing pipelines</li> <li>ML model training workflows</li> <li>ETL operations</li> <li>Batch job orchestration</li> <li>Scientific computing reproducibility</li> </ul> <p>\u274c Not recommended for:</p> <ul> <li>Production agentic frameworks</li> <li>Complex multi-agent systems</li> <li>Real-time conversational AI</li> <li>Advanced LLM orchestration patterns</li> </ul> <p> Learn Async Limitations</p>"},{"location":"#whats_next","title":"What's Next?","text":"<p>You've seen how Runnable transforms your code for portability and tracking. Ready to go deeper?</p> <p>\ud83c\udfaf Master the Concepts \u2192 Jobs vs Pipelines Learn when to use single jobs vs multi-step pipelines</p> <p>\u26a1 Async &amp; Streaming \u2192 Async &amp; Streaming Execution Real-time streaming workflows for LLMs, APIs, and live data processing</p> <p>\ud83d\udcca Handle Your Data \u2192 Task Types Work with returns, parameters, and different data types</p> <p>\ud83d\udc41\ufe0f Visualize Execution \u2192 Pipeline Visualization Interactive timelines showing execution flow and timing</p> <p>\u26a1 See Real Examples \u2192 Browse Repository Examples All working examples with full code in the <code>examples/</code> directory</p> <p>\ud83d\ude80 Deploy Anywhere \u2192 Production Guide Scale from laptop to containers to Kubernetes</p> <p>\ud83d\udd0d Compare Alternatives \u2192 Compare Tools See how Runnable compares to Kedro, Metaflow, and other orchestration tools</p>"},{"location":"#why_choose_runnable","title":"Why Choose Runnable?","text":"<ul> <li> <p> Easy to adopt, its mostly your code</p> <p>Your application code remains as it is. Runnable exists outside of it.</p> <ul> <li>No API's or decorators or any imposed structure.</li> </ul> <p> Getting started</p> </li> <li> <p> Bring your infrastructure</p> <p><code>runnable</code> is not a platform. It works with your platforms.</p> <ul> <li><code>runnable</code> composes pipeline definitions suited to your infrastructure.</li> <li>Extensible plugin architecture: Build custom executors, storage backends, and task types for any platform.</li> </ul> <p> Infrastructure</p> </li> <li> <p> Reproducibility</p> <p>Runnable tracks key information to reproduce the execution. All this happens without any additional code.</p> <p> Run Log</p> </li> <li> <p> Retry failures</p> <p>Debug any failure in your local development environment.</p> <p> Advanced Patterns</p> </li> <li> <p> Testing</p> <p>Unit test your code and pipelines.</p> <ul> <li>mock/patch the steps of the pipeline</li> <li>test your functions as you normally do.</li> </ul> <p> Testing Guide</p> </li> <li> <p> Move on</p> <p>Moving away from runnable is as simple as deleting relevant files.</p> <ul> <li>Your application code remains as it is.</li> </ul> </li> </ul>"},{"location":"hash-optimization/","title":"File Hash Optimization","text":""},{"location":"hash-optimization/#overview","title":"Overview","text":"<p>Runnable uses file hashing to track data integrity in the catalog system. The <code>get_data_hash</code> function in <code>runnable/utils.py</code> computes SHA256 hashes of files to detect changes and ensure reproducibility.</p>"},{"location":"hash-optimization/#optimization_strategy","title":"Optimization Strategy","text":""},{"location":"hash-optimization/#small_files_1gb","title":"Small Files (&lt; 1GB)","text":"<ul> <li>Method: Full SHA256 hash of entire file content</li> <li>Memory Usage: Constant (4KB chunks)</li> <li>Performance: Optimal for small to medium files</li> <li>Accuracy: Complete integrity verification</li> </ul>"},{"location":"hash-optimization/#large_files_1gb","title":"Large Files (\u2265 1GB)","text":"<ul> <li>Method: Fingerprint hash combining:</li> <li>File size (for uniqueness)</li> <li>First 1MB of file content</li> <li>Last 1MB of file content</li> <li>Memory Usage: Constant (2MB maximum)</li> <li>Performance: Dramatically faster than full hash</li> <li>Accuracy: High probability of detecting changes while not being cryptographically complete</li> </ul>"},{"location":"hash-optimization/#configuration","title":"Configuration","text":"<pre><code># runnable/defaults.py\nHASH_ALGORITHM = \"sha256\"  # Algorithm used for hashing\nLARGE_FILE_THRESHOLD_BYTES = 1024 * 1024 * 1024  # 1GB threshold\nHASH_CHUNK_SIZE = 1024 * 1024  # 1MB chunks for fingerprint\n</code></pre>"},{"location":"hash-optimization/#use_cases","title":"Use Cases","text":""},{"location":"hash-optimization/#when_full_hash_is_used","title":"When Full Hash is Used","text":"<ul> <li>Files under 1GB (configurable threshold)</li> <li>Critical integrity verification needed</li> <li>Files change frequently in middle sections</li> </ul>"},{"location":"hash-optimization/#when_fingerprint_hash_is_used","title":"When Fingerprint Hash is Used","text":"<ul> <li>Files 1GB or larger</li> <li>Performance is prioritized over complete verification</li> <li>Files typically change at beginning/end (logs, datasets)</li> </ul>"},{"location":"hash-optimization/#performance_impact","title":"Performance Impact","text":""},{"location":"hash-optimization/#before_optimization","title":"Before Optimization","text":"<ul> <li>10GB file: ~30 seconds, constant memory usage</li> <li>Memory efficient but time inefficient for large files</li> </ul>"},{"location":"hash-optimization/#after_optimization","title":"After Optimization","text":"<ul> <li>10GB file: ~0.1 seconds, constant memory usage</li> <li>Both memory and time efficient</li> <li>Maintains backward compatibility for small files</li> </ul>"},{"location":"hash-optimization/#trade-offs","title":"Trade-offs","text":""},{"location":"hash-optimization/#benefits","title":"Benefits","text":"<ul> <li>Massive performance improvement for large files</li> <li>Constant memory usage regardless of file size</li> <li>Maintains full integrity checking for small files</li> <li>Configurable threshold for different use cases</li> </ul>"},{"location":"hash-optimization/#limitations","title":"Limitations","text":"<ul> <li>Large files get fingerprint hash, not complete verification</li> <li>Small probability of hash collision for files with identical start/end but different middle content</li> <li>Requires understanding of trade-offs when troubleshooting integrity issues</li> </ul>"},{"location":"hash-optimization/#integration","title":"Integration","text":"<p>The optimized <code>get_data_hash</code> function is used automatically by:</p> <ul> <li>File system catalog (<code>extensions/catalog/any_path.py</code>)</li> <li>S3 catalog (when implemented)</li> <li>Any custom catalog implementations</li> </ul> <p>No changes required to existing code - optimization is transparent.</p>"},{"location":"advanced-patterns/async-streaming/","title":"\ud83e\uddea Async &amp; Streaming Execution (Experimental - Proof of Concept)","text":"<p>Execute async functions and stream results in real-time with runnable's experimental async capabilities.</p> <p>Not Recommended for Production Agentic Frameworks</p> <p>These async capabilities are experimental and primarily serve as a proof of possibility. They are NOT recommended for production agentic frameworks or complex LLM applications.</p> <p>For production agentic applications, use mature, purpose-built frameworks:</p> <ul> <li>Pydantic AI - Production-ready async agent framework with robust LLM integration</li> <li>LangChain - Comprehensive framework for LLM applications with extensive tooling</li> <li>CrewAI - Multi-agent orchestration with advanced coordination patterns</li> <li>AutoGen - Multi-agent conversation framework by Microsoft</li> </ul> <p>Runnable's async features are:</p> <ul> <li>\u274c Limited to local execution only (no containers, no Kubernetes)</li> <li>\u274c Experimental and subject to breaking changes</li> <li>\u274c Not optimized for complex agent workflows</li> <li>\u274c Lacking advanced LLM integration patterns</li> </ul> <p>Runnable excels at data pipeline orchestration, not agentic frameworks.</p> <p>Experimental Feature Details</p> <p>Async and streaming capabilities are experimental and under active development. Features may change, and stability is not guaranteed for production use.</p>"},{"location":"advanced-patterns/async-streaming/#overview","title":"Overview","text":"<p>Runnable provides experimental async and streaming support for specialized use cases requiring real-time processing:</p> <ul> <li>Batch Processing: Core production-ready data pipelines with full reproducibility and orchestration</li> <li>Experimental Streaming: AsyncGenerator support for LLM inference, APIs, and streaming data</li> </ul> <p>AsyncPipeline and AsyncPythonTask are experimental features that enable streaming workflows while maintaining runnable's core features: parameter management, catalog system, reproducibility, and multi-environment execution.</p> <p>Local Execution Only</p> <p>Async capabilities are currently only supported for local pipeline execution. AsyncPipeline and AsyncPythonTask cannot be used with containerized (<code>local-container</code>) or Kubernetes (<code>argo</code>) pipeline executors.</p> <pre><code># This works \u2713\npipeline.execute(configuration_file=\"configs/default.yaml\")  # Uses local executor\n\n# This won't work \u2717\npipeline.execute(configuration_file=\"configs/local-container.yaml\")\npipeline.execute(configuration_file=\"configs/argo-config.yaml\")\n</code></pre>"},{"location":"advanced-patterns/async-streaming/#when_to_use_async_execution","title":"When to Use Async Execution","text":"<ul> <li>LLM and AI model inference with streaming responses</li> <li>Real-time data processing that produces intermediate results</li> <li>Long-running async operations that benefit from streaming feedback</li> <li>FastAPI integration with Server-Sent Events (SSE)</li> <li>WebSocket streaming and event-driven architectures</li> </ul>"},{"location":"advanced-patterns/async-streaming/#asyncpythontask","title":"AsyncPythonTask","text":"<p>Execute async functions with native <code>await</code> support and optional streaming:</p>"},{"location":"advanced-patterns/async-streaming/#basic_async_function","title":"Basic Async Function","text":"<pre><code>from runnable import AsyncPythonTask\nimport asyncio\n\nasync def fetch_data():\n    await asyncio.sleep(2)  # Simulate async operation\n    return {\"status\": \"complete\", \"data\": [1, 2, 3]}\n\nasync def main():\n    task = AsyncPythonTask(\n        name=\"fetch_task\",\n        function=fetch_data,\n        returns=[\"result\"]\n    )\n\n    # Convert to pipeline and execute with streaming\n    pipeline = task.as_async_pipeline()\n\n    # Stream events from the async pipeline\n    async for event in pipeline.execute_streaming():\n        print(f\"Event: {event}\")\n\n    return pipeline\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"advanced-patterns/async-streaming/#streaming_with_asyncgenerator","title":"Streaming with AsyncGenerator","text":"<p>For real-time streaming, use <code>AsyncGenerator</code> functions that yield events:</p> <pre><code>from runnable import AsyncPythonTask\nimport asyncio\nfrom typing import AsyncGenerator\n\nasync def mock_llm_stream(prompt: str) -&gt; AsyncGenerator[dict, None]:\n    \"\"\"Stream LLM response token by token.\"\"\"\n\n    # Initial status\n    yield {\"type\": \"status\", \"status\": \"thinking\"}\n    await asyncio.sleep(0.3)\n\n    # Simulate streaming response\n    response = f\"Response to: {prompt}\"\n    words = response.split()\n\n    yield {\"type\": \"status\", \"status\": \"generating\"}\n\n    # Stream word by word\n    for word in words:\n        yield {\"type\": \"chunk\", \"text\": word + \" \"}\n        await asyncio.sleep(0.05)\n\n    # Final event with complete response\n    yield {\"type\": \"done\", \"full_text\": response}\n\nasync def main():\n    task = AsyncPythonTask(\n        name=\"llm_stream\",\n        function=mock_llm_stream,\n        returns=[\"full_text\"],\n        stream_end_type=\"done\"  # Which event contains final values\n    )\n\n    pipeline = task.as_async_pipeline()\n\n    # Stream events in real-time\n    async for event in pipeline.execute_streaming():\n        print(f\"Streaming event: {event}\")\n\n    return pipeline\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"advanced-patterns/async-streaming/#asyncpipeline","title":"AsyncPipeline","text":"<p>Execute sequences of async tasks with streaming support:</p>"},{"location":"advanced-patterns/async-streaming/#multi-step_async_pipeline","title":"Multi-Step Async Pipeline","text":"<pre><code>from runnable import AsyncPipeline, AsyncPythonTask\nimport asyncio\n\nasync def process_input(text: str):\n    await asyncio.sleep(0.5)\n    return text.upper()\n\nasync def analyze_text(processed_text: str):\n    await asyncio.sleep(0.3)\n    word_count = len(processed_text.split())\n    return {\"word_count\": word_count, \"processed\": processed_text}\n\nasync def main():\n    pipeline = AsyncPipeline(\n        name=\"text_processing\",\n        steps=[\n            AsyncPythonTask(\n                name=\"process\",\n                function=process_input,\n                returns=[\"processed_text\"]\n            ),\n            AsyncPythonTask(\n                name=\"analyze\",\n                function=analyze_text,\n                returns=[\"analysis\"]\n            )\n        ]\n    )\n\n    # Stream events from the multi-step pipeline\n    async for event in pipeline.execute_streaming():\n        print(f\"Pipeline event: {event}\")\n\n    return pipeline\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"advanced-patterns/async-streaming/#streaming_execution","title":"Streaming Execution","text":"<p>Access events from AsyncGenerator functions in real-time:</p> <pre><code>from runnable import AsyncPipeline, AsyncPythonTask\n\nasync def streaming_analysis(data: str) -&gt; AsyncGenerator[dict, None]:\n    yield {\"type\": \"status\", \"message\": \"Starting analysis\"}\n\n    # Simulate processing steps\n    steps = [\"tokenizing\", \"parsing\", \"analyzing\", \"summarizing\"]\n    for step in steps:\n        yield {\"type\": \"progress\", \"step\": step}\n        await asyncio.sleep(0.2)\n\n    yield {\"type\": \"done\", \"summary\": f\"Analyzed: {data[:50]}...\"}\n\nasync def main():\n    pipeline = AsyncPipeline(\n        steps=[\n            AsyncPythonTask(\n                name=\"stream_analysis\",\n                function=streaming_analysis,\n                returns=[\"summary\"]\n            )\n        ]\n    )\n\n    # Stream real-time events from AsyncGenerator function\n    async for event in pipeline.execute_streaming():\n        print(f\"Real-time event: {event}\")\n\n    return pipeline\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"advanced-patterns/async-streaming/#fastapi_integration","title":"FastAPI Integration","text":"<p>Create streaming APIs with Server-Sent Events (SSE):</p>"},{"location":"advanced-patterns/async-streaming/#basic_fastapi_setup","title":"Basic FastAPI Setup","text":"<pre><code>from fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse\nfrom pydantic import BaseModel\nimport json\nimport os\n\napp = FastAPI()\n\nclass ChatRequest(BaseModel):\n    prompt: str\n\n@app.post(\"/chat\")\nasync def chat_stream(request: ChatRequest):\n    \"\"\"Stream LLM response via Server-Sent Events.\"\"\"\n\n    async def event_stream():\n        # Set pipeline parameter\n        os.environ[\"RUNNABLE_PRM_prompt\"] = request.prompt\n\n        try:\n            # Your async pipeline from previous examples\n            pipeline = create_chat_pipeline()\n\n            # Stream events directly to client\n            async for event in pipeline.execute_streaming():\n                yield f\"data: {json.dumps(event)}\\n\\n\"\n        finally:\n            os.environ.pop(\"RUNNABLE_PRM_prompt\", None)\n\n    return StreamingResponse(\n        event_stream(),\n        media_type=\"text/event-stream\",\n        headers={\n            \"Cache-Control\": \"no-cache\",\n            \"X-Accel-Buffering\": \"no\"\n        }\n    )\n\ndef create_chat_pipeline():\n    return AsyncPythonTask(\n        name=\"llm_response\",\n        function=mock_llm_stream,\n        returns=[\"full_text\"]\n    ).as_async_pipeline()\n</code></pre>"},{"location":"advanced-patterns/async-streaming/#advanced_patterns","title":"Advanced Patterns","text":""},{"location":"advanced-patterns/async-streaming/#parallel_async_execution","title":"Parallel Async Execution","text":"<p>Combine async tasks with parallel execution:</p> <pre><code>from runnable import AsyncPipeline, AsyncPythonTask, Parallel\n\nasync def translate_spanish(text: str) -&gt; AsyncGenerator[dict, None]:\n    yield {\"type\": \"status\", \"message\": \"Translating to Spanish\"}\n    await asyncio.sleep(0.5)\n    yield {\"type\": \"done\", \"spanish_translation\": f\"ES: {text}\"}\n\nasync def translate_french(text: str) -&gt; AsyncGenerator[dict, None]:\n    yield {\"type\": \"status\", \"message\": \"Translating to French\"}\n    await asyncio.sleep(0.4)\n    yield {\"type\": \"done\", \"french_translation\": f\"FR: {text}\"}\n\nasync def main():\n    pipeline = AsyncPipeline(\n        steps=[\n            # First generate text\n            AsyncPythonTask(\n                name=\"generate\",\n                function=mock_llm_stream,\n                returns=[\"full_text\"]\n            ),\n            # Then translate in parallel\n            Parallel(\n                name=\"translate_parallel\",\n                branches={\n                    \"spanish\": AsyncPythonTask(\n                        name=\"spanish\",\n                        function=translate_spanish,\n                        returns=[\"spanish_translation\"]\n                    ).as_async_pipeline(),\n                    \"french\": AsyncPythonTask(\n                        name=\"french\",\n                        function=translate_french,\n                        returns=[\"french_translation\"]\n                    ).as_async_pipeline()\n                }\n            )\n        ]\n    )\n\n    # Stream events from LLM generation and parallel translations\n    async for event in pipeline.execute_streaming():\n        print(f\"Parallel execution event: {event}\")\n\n    return pipeline\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"advanced-patterns/async-streaming/#map_with_async_streaming","title":"Map with Async Streaming","text":"<p>Process multiple items with async streaming:</p> <pre><code>from runnable import AsyncPipeline, AsyncPythonTask, Map\n\nasync def process_item(text: str, language: str) -&gt; AsyncGenerator[dict, None]:\n    yield {\"type\": \"status\", \"message\": f\"Processing {language}\"}\n    await asyncio.sleep(0.3)\n    yield {\"type\": \"done\", \"result\": f\"[{language.upper()}] {text}\"}\n\nasync def main():\n    # Branch that processes one language\n    process_branch = AsyncPythonTask(\n        name=\"process_language\",\n        function=process_item,\n        returns=[\"result\"]\n    ).as_async_pipeline()\n\n    pipeline = AsyncPipeline(\n        steps=[\n            AsyncPythonTask(\n                name=\"generate_text\",\n                function=mock_llm_stream,\n                returns=[\"full_text\"]\n            ),\n            Map(\n                name=\"process_languages\",\n                iterate_on=\"languages\",  # Parameter: [\"spanish\", \"french\", \"german\"]\n                iterate_as=\"language\",   # Current language in loop\n                branch=process_branch\n            )\n        ]\n    )\n\n    # Stream events from text generation and map processing\n    async for event in pipeline.execute_streaming():\n        print(f\"Map streaming event: {event}\")\n\n    return pipeline\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"advanced-patterns/async-streaming/#event_types_and_stream_configuration","title":"Event Types and Stream Configuration","text":""},{"location":"advanced-patterns/async-streaming/#standard_event_types","title":"Standard Event Types","text":"<p>AsyncGenerator functions should yield events with a <code>type</code> field:</p> <ul> <li><code>status</code>: Processing status updates</li> <li><code>progress</code>: Step-by-step progress information</li> <li><code>chunk</code>: Incremental data (text tokens, partial results)</li> <li><code>done</code>: Final completion with return values</li> <li><code>error</code>: Error information</li> </ul>"},{"location":"advanced-patterns/async-streaming/#stream_end_configuration","title":"Stream End Configuration","text":"<p>Control which event contains the final return values:</p> <pre><code>AsyncPythonTask(\n    name=\"custom_stream\",\n    function=my_async_generator,\n    returns=[\"result\"],\n    stream_end_type=\"complete\"  # Look for type=\"complete\" events\n)\n</code></pre> <p>The framework extracts return values from the specified event type, using all keys except <code>type</code> as the returned data.</p>"},{"location":"advanced-patterns/async-streaming/#best_practices","title":"Best Practices","text":""},{"location":"advanced-patterns/async-streaming/#asyncgenerator_patterns","title":"AsyncGenerator Patterns","text":"<pre><code>async def well_structured_stream(input_data: str) -&gt; AsyncGenerator[dict, None]:\n    \"\"\"Best practice streaming function structure.\"\"\"\n\n    try:\n        # Always start with status\n        yield {\"type\": \"status\", \"status\": \"starting\", \"input\": input_data}\n\n        # Processing with progress updates\n        steps = [\"step1\", \"step2\", \"step3\"]\n        for i, step in enumerate(steps):\n            yield {\"type\": \"progress\", \"step\": step, \"completed\": i, \"total\": len(steps)}\n            await asyncio.sleep(0.1)  # Your actual work here\n\n        # Stream incremental results if applicable\n        result_parts = [\"part1\", \"part2\", \"part3\"]\n        full_result = []\n\n        for part in result_parts:\n            yield {\"type\": \"chunk\", \"data\": part}\n            full_result.append(part)\n            await asyncio.sleep(0.05)\n\n        # Always end with complete data\n        yield {\n            \"type\": \"done\",\n            \"final_result\": \" \".join(full_result),\n            \"metadata\": {\"processing_time\": \"estimated\"}\n        }\n\n    except Exception as e:\n        yield {\"type\": \"error\", \"error\": str(e)}\n        raise\n</code></pre>"},{"location":"advanced-patterns/async-streaming/#error_handling","title":"Error Handling","text":"<p>Handle errors gracefully in async pipelines:</p> <pre><code>async def safe_async_operation(data: str) -&gt; AsyncGenerator[dict, None]:\n    try:\n        yield {\"type\": \"status\", \"status\": \"processing\"}\n\n        if not data:\n            raise ValueError(\"Empty input data\")\n\n        # Your processing\n        await asyncio.sleep(0.5)\n        result = data.upper()\n\n        yield {\"type\": \"done\", \"result\": result}\n\n    except Exception as e:\n        yield {\"type\": \"error\", \"error\": str(e), \"failed_input\": data}\n        # Re-raise to fail the pipeline step\n        raise\n</code></pre>"},{"location":"advanced-patterns/async-streaming/#fastapi_best_practices","title":"FastAPI Best Practices","text":"<pre><code>@app.post(\"/stream-endpoint\")\nasync def stream_endpoint(request: RequestModel):\n    async def event_stream():\n        # Always use try/finally for cleanup\n        os.environ[\"RUNNABLE_PRM_input\"] = request.input\n\n        try:\n            pipeline = create_streaming_pipeline()\n            async for event in pipeline.execute_streaming(\n                run_id=f\"req-{datetime.now().isoformat()}\"\n            ):\n                # Validate events before sending\n                if isinstance(event, dict) and \"type\" in event:\n                    yield f\"data: {json.dumps(event)}\\n\\n\"\n        except Exception as e:\n            # Send error event to client\n            error_event = {\"type\": \"error\", \"error\": str(e)}\n            yield f\"data: {json.dumps(error_event)}\\n\\n\"\n        finally:\n            # Always clean up parameters\n            os.environ.pop(\"RUNNABLE_PRM_input\", None)\n\n    return StreamingResponse(\n        event_stream(),\n        media_type=\"text/event-stream\"\n    )\n</code></pre>"},{"location":"advanced-patterns/async-streaming/#complete_example","title":"Complete Example","text":"<p>See the FastAPI LLM streaming example for a complete working implementation with:</p> <ul> <li>Multiple async pipeline patterns</li> <li>FastAPI SSE integration</li> <li>Parallel and map async execution</li> <li>Conditional async workflows</li> <li>Production-ready error handling</li> </ul> <pre><code># Run the example\ncd examples/fastapi_llm\nuv run uvicorn main:app --reload\n\n# Test streaming\ncurl -N -X POST http://localhost:8000/chat \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"prompt\": \"Hello, how are you?\"}'\n</code></pre>"},{"location":"advanced-patterns/async-streaming/#key_differences_from_sync_execution","title":"Key Differences from Sync Execution","text":"Feature Sync Pipeline AsyncPipeline Execution <code>pipeline.execute()</code> <code>async for event in pipeline.execute_streaming()</code> Streaming Not available \u2705 Native streaming support Task Type <code>PythonTask</code> <code>AsyncPythonTask</code> Functions Regular functions <code>async def</code> or <code>AsyncGenerator</code> Use Cases Batch processing Real-time streaming, LLMs, APIs <p>The async capabilities enable entirely new patterns like real-time LLM streaming, progressive data processing, and seamless FastAPI integration while maintaining runnable's core principles of reproducibility and configuration management.</p>"},{"location":"advanced-patterns/conditional-workflows/","title":"\ud83d\udd00 Conditional Workflows","text":"<p>Make your pipeline choose different paths based on data or results.</p>"},{"location":"advanced-patterns/conditional-workflows/#the_pattern","title":"The pattern","text":"<pre><code>flowchart TD\n    A[Start] --&gt; B[Toss Function]\n    B --&gt; C{toss = ?}\n    C --&gt;|heads| D[Heads Pipeline]\n    C --&gt;|tails| E[Tails Pipeline]\n    D --&gt; F[Continue]\n    E --&gt; F</code></pre> <pre><code>from runnable import Conditional, Pipeline, PythonTask, Stub\n\ndef main():\n    # Step 1: Make a decision\n    toss_task = PythonTask(\n        function=toss_function,    # Returns \"heads\" or \"tails\"\n        returns=[\"toss\"],          # Named return for conditional to use\n        name=\"toss_task\"\n    )\n\n    # Step 2: Branch based on decision\n    conditional = Conditional(\n        parameter=\"toss\",          # Use the \"toss\" value from above\n        branches={\n            \"heads\": create_heads_pipeline(),    # Run this if toss=\"heads\"\n            \"tails\": create_tails_pipeline()     # Run this if toss=\"tails\"\n        },\n        name=\"conditional\"\n    )\n\n    # Step 3: Continue after branching\n    continue_step = Stub(name=\"continue_processing\")\n\n    pipeline = Pipeline(steps=[toss_task, conditional, continue_step])\n    pipeline.execute()\n    return pipeline\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> See complete runnable code examples/02-sequential/conditional.py<pre><code>from runnable import Conditional, Pipeline, PythonTask, Stub\n\n\ndef when_heads_function():\n    print(\"when_heads_function called\")\n\n\ndef when_tails_function():\n    print(\"when_tails_function called\")\n\n\ndef toss_function():\n    import os\n    import random\n\n    if \"FIX_RANDOM_TOSS\" in os.environ:\n        # Use the fixed value for testing\n        toss = os.environ[\"FIX_RANDOM_TOSS\"]\n        print(f\"Using fixed toss result: {toss}\")\n        return toss\n\n    # Simulate a coin toss\n    toss = random.choice([\"heads\", \"tails\"])\n    print(f\"Toss result: {toss}\")\n    return toss\n\n\ndef main():\n    when_heads_pipeline = PythonTask(  # [concept:branch-pipeline]\n        name=\"when_heads_task\",\n        function=when_heads_function,\n    ).as_pipeline()\n\n    when_tails_pipeline = PythonTask(  # [concept:branch-pipeline]\n        name=\"when_tails_task\",\n        function=when_tails_function,\n    ).as_pipeline()\n\n    conditional = Conditional(  # [concept:conditional]\n        name=\"conditional\",\n        branches={\n            \"heads\": when_heads_pipeline,\n            \"tails\": when_tails_pipeline,\n        },\n        parameter=\"toss\",\n    )\n\n    toss_task = PythonTask(  # [concept:task-with-returns]\n        name=\"toss_task\",\n        function=toss_function,\n        returns=[\"toss\"],\n    )\n\n    continue_to = Stub(name=\"continue to\")  # [concept:stub]\n    pipeline = Pipeline(steps=[toss_task, conditional, continue_to])  # [concept:pipeline]\n\n    pipeline.execute()  # [concept:execution]\n\n    return pipeline\n\n\nif __name__ == \"__main__\":\n    main()\n    main()\n</code></pre> <p>Try it now: <pre><code>uv run examples/02-sequential/conditional.py\n</code></pre></p>"},{"location":"advanced-patterns/conditional-workflows/#how_it_works","title":"How it works","text":"<ol> <li><code>toss_task</code> returns a value named <code>\"toss\"</code></li> <li><code>Conditional</code> uses <code>parameter=\"toss\"</code> to check that value</li> <li><code>branches={}</code> maps values to different pipelines</li> <li>Only one branch executes based on the parameter value</li> </ol>"},{"location":"advanced-patterns/conditional-workflows/#the_decision_function","title":"The decision function","text":"<p>Helper function (makes the decision): <pre><code>import random\n\ndef toss_function():\n    # Simulate a coin toss\n    toss = random.choice([\"heads\", \"tails\"])\n    print(f\"Toss result: {toss}\")\n    return toss  # Returns \"heads\" or \"tails\"\n</code></pre></p> <p>Returns <code>\"heads\"</code> or <code>\"tails\"</code> - the conditional uses this to pick a branch.</p>"},{"location":"advanced-patterns/conditional-workflows/#branch_pipelines","title":"Branch pipelines","text":"<p>Helper functions (create the branch pipelines): <pre><code>def create_heads_pipeline():\n    return PythonTask(\n        function=when_heads_function,\n        name=\"when_heads_task\"\n    ).as_pipeline()\n\ndef create_tails_pipeline():\n    return PythonTask(\n        function=when_tails_function,\n        name=\"when_tails_task\"\n    ).as_pipeline()\n</code></pre></p> <p>Each branch is a complete pipeline that runs independently.</p>"},{"location":"advanced-patterns/conditional-workflows/#real-world_examples","title":"Real-world examples","text":"<pre><code>flowchart TD\n    A[Data Input] --&gt; B[Quality Check]\n    B --&gt; C{data_quality}\n    C --&gt;|good| D[Direct Analysis]\n    C --&gt;|needs_cleaning| E[Clean \u2192 Analysis]\n    C --&gt;|invalid| F[Error Handling]\n    D --&gt; G[Results]\n    E --&gt; G\n    F --&gt; G</code></pre> <p>Data validation: <pre><code># Example conditional configuration (partial code)\nconditional = Conditional(\n    parameter=\"data_quality\",  # returns \"good\", \"needs_cleaning\", \"invalid\"\n    branches={\n        \"good\": analysis_pipeline,\n        \"needs_cleaning\": cleanup_then_analysis_pipeline,\n        \"invalid\": error_handling_pipeline\n    }\n)\n</code></pre></p> <p>Model selection: <pre><code># Example conditional configuration (partial code)\nconditional = Conditional(\n    parameter=\"dataset_size\",  # returns \"small\", \"medium\", \"large\"\n    branches={\n        \"small\": simple_model_pipeline,\n        \"medium\": ensemble_pipeline,\n        \"large\": distributed_training_pipeline\n    }\n)\n</code></pre></p> <p>Environment routing: <pre><code># Example conditional configuration (partial code)\nconditional = Conditional(\n    parameter=\"environment\",  # returns \"dev\", \"staging\", \"prod\"\n    branches={\n        \"dev\": fast_testing_pipeline,\n        \"staging\": full_validation_pipeline,\n        \"prod\": production_pipeline\n    }\n)\n</code></pre></p> <p>Conditional tips</p> <ul> <li>Always provide a branch for every possible return value</li> <li>Use meaningful parameter names like <code>\"status\"</code>, <code>\"environment\"</code>, <code>\"model_type\"</code></li> <li>Consider using enums in your decision functions for type safety</li> </ul> <p>Next: Learn about failure handling.</p>"},{"location":"advanced-patterns/failure-handling/","title":"\ud83d\udee1\ufe0f Failure Handling","text":"<p>Control what happens when tasks fail, with graceful recovery and alternative paths.</p>"},{"location":"advanced-patterns/failure-handling/#default_behavior","title":"Default behavior","text":"<p>By default, any task failure stops the entire pipeline:</p> <pre><code>flowchart TD\n    A[Step 1] --&gt; B[Step 2]\n    B --&gt; C[\u274c FAILS]\n    C --&gt; D[Pipeline Stops]\n    E[Step 3] -.-&gt;|Never runs| F[\u274c]</code></pre> <pre><code>from runnable import Pipeline, PythonTask, Stub\nfrom examples.common.functions import hello, raise_ex\n\ndef main():\n    # Normal pipeline - fails on any error\n    step1 = PythonTask(name=\"step_1\", function=hello)\n    step2 = PythonTask(name=\"step_2\", function=raise_ex)  # This will fail!\n    step3 = Stub(name=\"step_3\")                           # Never runs\n\n    pipeline = Pipeline(steps=[step1, step2, step3])\n    pipeline.execute()  # Stops at step2 failure\n    return pipeline\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> See complete runnable code examples/02-sequential/default_fail.py<pre><code>\"\"\"\nWhen defining a Pipeline(), it automatically adds a success node and failure node.\n\nBy default any failure in a step is considered to be a failure in the pipeline.\n\nIn the below example, the progression would be as follows:\n\n  step 1 &gt;&gt; step 2 &gt;&gt; fail\n\nCorresponds to:\n  try:\n      step1()\n      step2() # Raises the exception\n      step3()\n  except Exception as e:\n      raise e\n\nYou can run this example by:\n  python examples/02-sequential/default_fail.py\n\n\"\"\"\n\nfrom examples.common.functions import hello, raise_ex\nfrom runnable import Pipeline, PythonTask, Stub\n\n\ndef main():\n    step1 = PythonTask(name=\"step 1\", function=hello)\n\n    step2 = PythonTask(name=\"step 2\", function=raise_ex)  # This will fail\n\n    step3 = Stub(name=\"step 3\")  # This step will not be executed\n\n    pipeline = Pipeline(steps=[step1, step2, step3])\n\n    pipeline.execute()\n\n    return pipeline\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Try it now: <pre><code>uv run examples/02-sequential/default_fail.py\n</code></pre></p> <p>Flow: <code>step 1</code> \u2192 <code>step 2</code> (fails) \u2192 pipeline stops</p>"},{"location":"advanced-patterns/failure-handling/#custom_failure_handling","title":"Custom failure handling","text":"<p>Specify what to do when a task fails:</p> <pre><code>flowchart TD\n    A[Step 1] --&gt; B[\u274c FAILS]\n    B --&gt; C[Recovery Pipeline]\n    C --&gt; D[\u2705 Success]\n    E[Step 2] -.-&gt;|Skipped| F[\u274c]\n    G[Step 3] -.-&gt;|Skipped| H[\u274c]</code></pre> <pre><code>from runnable import Pipeline, PythonTask, Stub\nfrom examples.common.functions import raise_ex\n\ndef main():\n    # Create recovery pipeline\n    recovery_pipeline = Stub(name=\"recovery_step\").as_pipeline()\n\n    # Set up failure handling\n    step_1 = PythonTask(name=\"step_1\", function=raise_ex)  # This will fail\n    step_1.on_failure = recovery_pipeline                   # Run this instead\n\n    step_2 = Stub(name=\"step_2\")  # Skipped (normal flow interrupted)\n    step_3 = Stub(name=\"step_3\")  # Skipped (normal flow interrupted)\n\n    pipeline = Pipeline(steps=[step_1, step_2, step_3])\n    pipeline.execute()  # Runs: step_1 \u2192 fails \u2192 recovery_pipeline \u2192 success!\n    return pipeline\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> See complete runnable code examples/02-sequential/on_failure_succeed.py<pre><code>\"\"\"\nThis pipeline showcases handling failures in a pipeline.\n\nThe path taken if none of the steps failed:\nstep_1 -&gt; step_2 -&gt; step_3 -&gt; success\n\nstep_1 is a python function that raises an exception.\nAnd we can instruct the pipeline to execute step_4 if step_1 fails\nand then eventually succeed too.\nstep_1 -&gt; step_4 -&gt; success\n\nThis pattern is handy when you are expecting a failure of a step\nand have ways to handle it.\n\nCorresponds to:\ntry:\n    step1()  # Raises the exception\n    step2()\n    step3()\nexcept Exception as e:\n    step4()\n\nRun this pipeline:\n    python examples/02-sequential/on_failure_succeed.py\n\"\"\"\n\nfrom examples.common.functions import raise_ex\nfrom runnable import Pipeline, PythonTask, Stub\n\n\ndef main():\n    step_1 = PythonTask(name=\"step_1\", function=raise_ex)  # [concept:failing-task]\n\n    step_2 = Stub(name=\"step_2\")\n\n    step_3 = Stub(name=\"step_3\")\n\n    on_failure_pipeline = Stub(name=\"step_4\").as_pipeline()  # [concept:failure-pipeline]\n\n    step_1.on_failure = on_failure_pipeline  # [concept:failure-handling]\n\n    pipeline = Pipeline(  # [concept:pipeline]\n        steps=[step_1, step_2, step_3],\n    )\n    pipeline.execute()  # [concept:execution]\n\n    return pipeline\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Try it now: <pre><code>uv run examples/02-sequential/on_failure_succeed.py\n</code></pre></p> <p>Flow: <code>step_1</code> (fails) \u2192 <code>step_4</code> \u2192 success</p>"},{"location":"advanced-patterns/failure-handling/#how_it_works","title":"How it works","text":"<pre><code>step_1.on_failure = recovery_pipeline\n</code></pre> <ol> <li>Task fails \u2192 Normal execution stops</li> <li><code>on_failure</code> pipeline executes instead</li> <li>Recovery pipeline can succeed or fail</li> <li>Pipeline continues if recovery succeeds</li> </ol> <p>This is Alternative Execution, Not Retry</p> <p>The <code>on_failure</code> pattern runs different code when tasks fail - it's graceful recovery, not retry.</p> <p>For actual retry functionality (re-executing the same failed steps), see Retry &amp; Recovery.</p>"},{"location":"advanced-patterns/failure-handling/#real-world_patterns","title":"Real-world patterns","text":"<pre><code>flowchart TD\n    A[Fetch Latest Data] --&gt; B{Success?}\n    B --&gt;|\u2705| C[Process Latest]\n    B --&gt;|\u274c| D[Use Cached Data]\n    C --&gt; E[Results]\n    D --&gt; E</code></pre>"},{"location":"advanced-patterns/failure-handling/#data_fallback","title":"Data fallback","text":"<pre><code># Example failure handling configuration (partial code)\nfetch_latest_data_task.on_failure = use_cached_data_pipeline\n</code></pre>"},{"location":"advanced-patterns/failure-handling/#alternative_approach_on_failure","title":"Alternative approach on failure","text":"<pre><code># Example failure handling configuration (partial code)\nfast_model_task.on_failure = robust_model_pipeline\n</code></pre>"},{"location":"advanced-patterns/failure-handling/#error_reporting","title":"Error reporting","text":"<pre><code># Example failure handling configuration (partial code)\ncritical_task.on_failure = send_alert_pipeline\n</code></pre>"},{"location":"advanced-patterns/failure-handling/#graceful_degradation","title":"Graceful degradation","text":"<pre><code># Example failure handling configuration (partial code)\nfeature_extraction_task.on_failure = use_simple_features_pipeline\n</code></pre>"},{"location":"advanced-patterns/failure-handling/#failure_pipeline_example","title":"Failure pipeline example","text":"<p>Helper function (creates a recovery pipeline): <pre><code>def create_fallback_pipeline():\n    log_error = PythonTask(\n        name=\"log_error\",\n        function=log_failure_details\n    )\n\n    use_backup = PythonTask(\n        name=\"use_backup\",\n        function=load_backup_data,\n        returns=[\"data\"]\n    )\n\n    return Pipeline(steps=[log_error, use_backup])\n\n# Usage example (partial code)\nmain_task.on_failure = create_fallback_pipeline()\n</code></pre></p>"},{"location":"advanced-patterns/failure-handling/#when_to_use_failure_handling","title":"When to use failure handling","text":"<p>Essential for:</p> <ul> <li>Production pipelines that must complete</li> <li>Data pipelines with unreliable sources</li> <li>ML pipelines with multiple model options</li> <li>External API integrations</li> <li>File processing with backup sources</li> </ul> <p>Failure strategy</p> <ul> <li>Fail fast: For development and testing</li> <li>Graceful recovery: For production systems</li> <li>Log everything: Always capture failure details</li> <li>Test failures: Verify your recovery paths work</li> </ul> <p>Next: Learn about mocking and testing.</p>"},{"location":"advanced-patterns/loop-patterns/","title":"Loop Patterns","text":"<p>Execute iterative workflows with conditional termination using runnable's Loop node for dynamic repetition patterns.</p>"},{"location":"advanced-patterns/loop-patterns/#core_loop_concepts","title":"Core Loop Concepts","text":"<p>Loop enables iterative execution of a branch pipeline until a break condition is met or maximum iterations reached:</p> <ul> <li>Branch Pipeline: A complete pipeline executed on each iteration</li> <li>Break Condition: Boolean parameter that terminates the loop when <code>True</code></li> <li>Iteration Index: Environment variable providing current iteration count (0-based)</li> <li>Safety Limit: Maximum iterations to prevent infinite loops</li> </ul>"},{"location":"advanced-patterns/loop-patterns/#loop_flow_visualization","title":"Loop Flow Visualization","text":"<pre><code>flowchart TD\n    A[Start] --&gt; B[Loop Step]\n    B --&gt; C[Execute branch]\n    C --&gt; D{Break condition?&lt;br/&gt;max_iterations OR&lt;br/&gt;parameter=True}\n    D --&gt;|No| C\n    D --&gt;|Yes| E[Continue]</code></pre>"},{"location":"advanced-patterns/loop-patterns/#basic_loop_pattern","title":"Basic Loop Pattern","text":"<pre><code>from runnable.sdk import Pipeline, PythonTask, Loop\n\ndef counter_task():\n    \"\"\"Task that increments until reaching 3.\"\"\"\n    import os\n    counter = int(os.environ.get(\"counter\", 0))\n    should_stop = counter &gt;= 3\n    return should_stop\n\ndef counter_branch():\n    \"\"\"Branch pipeline executed each iteration.\"\"\"\n    task = PythonTask(\n        name=\"count\",\n        function=counter_task,\n        returns=[\"should_stop\"]\n    )\n    return Pipeline(steps=[task])\n\ndef main():\n    loop = Loop(\n        name=\"counter_loop\",\n        branch=counter_branch(),\n        max_iterations=5,         # Safety limit\n        break_on=\"should_stop\",   # Exit condition parameter\n        index_as=\"counter\"        # Environment variable name\n    )\n\n    pipeline = Pipeline(steps=[loop])\n    pipeline.execute()\n    return pipeline\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>How it works:</p> <ol> <li>Iteration 0: Loop sets <code>counter=0</code> in environment and executes branch</li> <li>Iteration 1: Loop sets <code>counter=1</code> in environment and executes branch</li> <li>Iteration N: Continues until <code>should_stop</code> becomes <code>True</code> or <code>max_iterations</code> reached</li> </ol>"},{"location":"advanced-patterns/loop-patterns/#loop_components","title":"Loop Components","text":""},{"location":"advanced-patterns/loop-patterns/#branch_pipeline","title":"Branch Pipeline","text":"<p>The branch is a complete Pipeline executed on each iteration:</p> <pre><code>def processing_branch():\n    \"\"\"Multi-step pipeline executed per iteration.\"\"\"\n    process_task = PythonTask(\n        name=\"process_data\",\n        function=process_iteration_data,\n        returns=[\"processed_count\"]\n    )\n\n    check_task = PythonTask(\n        name=\"check_completion\",\n        function=check_if_done,\n        returns=[\"is_complete\"]\n    )\n\n    return Pipeline(steps=[process_task, check_task])\n</code></pre>"},{"location":"advanced-patterns/loop-patterns/#break_condition","title":"Break Condition","text":"<p>Boolean parameter that controls loop termination:</p> <pre><code>def check_completion():\n    \"\"\"Determine if loop should continue.\"\"\"\n    # Your completion logic here\n    completed = some_condition()\n    return completed  # Return True to exit loop\n\ntask = PythonTask(\n    name=\"checker\",\n    function=check_completion,\n    returns=[\"loop_complete\"]  # This becomes the break_on parameter\n)\n\nloop = Loop(\n    name=\"data_loop\",\n    branch=Pipeline(steps=[task]),\n    break_on=\"loop_complete\",  # Parameter name from returns\n    # ... other config\n)\n</code></pre>"},{"location":"advanced-patterns/loop-patterns/#iteration_index","title":"Iteration Index","text":"<p>Access current iteration count in tasks:</p> <pre><code>def iteration_aware_task():\n    \"\"\"Task that uses iteration count.\"\"\"\n    import os\n    import logging\n    logger = logging.getLogger(__name__)\n\n    iteration = int(os.environ.get(\"iteration_idx\", 0))\n\n    logger.info(f\"Executing iteration {iteration}\")\n\n    # Iteration-specific logic\n    batch_size = 100 + (iteration * 10)  # Increase batch size each iteration\n    data = fetch_data(limit=batch_size)\n\n    completed = len(data) &lt; batch_size  # Stop when less than expected\n    return completed\n\nloop = Loop(\n    name=\"adaptive_loop\",\n    branch=Pipeline(steps=[task]),\n    index_as=\"iteration_idx\",  # Available as environment variable\n    break_on=\"completed\",\n    max_iterations=10\n)\n</code></pre>"},{"location":"advanced-patterns/loop-patterns/#loop_termination_conditions","title":"Loop Termination Conditions","text":"<p>Loops terminate when any condition is met (checked in order):</p>"},{"location":"advanced-patterns/loop-patterns/#1_branch_execution_failure","title":"1. Branch Execution Failure","text":"<p>If any task in the branch fails, loop exits immediately:</p> <pre><code>def risky_task():\n    \"\"\"Task that might fail.\"\"\"\n    import random\n    if random.random() &lt; 0.3:  # 30% chance of failure\n        raise ValueError(\"Simulated processing error\")\n    return False  # Continue loop\n\n# Loop exits immediately on task failure\n# Final status: FAIL\n</code></pre>"},{"location":"advanced-patterns/loop-patterns/#2_break_condition_met","title":"2. Break Condition Met","text":"<p>When break condition parameter becomes <code>True</code>:</p> <pre><code>def convergence_check():\n    \"\"\"Check if algorithm converged.\"\"\"\n    # Your convergence logic\n    error_rate = calculate_error()\n    converged = error_rate &lt; 0.01\n    return converged\n\ntask = PythonTask(\n    name=\"check\",\n    function=convergence_check,\n    returns=[\"converged\"]\n)\n\nloop = Loop(\n    name=\"training_loop\",\n    branch=Pipeline(steps=[task]),\n    break_on=\"converged\",  # Exit when converged=True\n    max_iterations=1000\n)\n</code></pre>"},{"location":"advanced-patterns/loop-patterns/#3_maximum_iterations_reached","title":"3. Maximum Iterations Reached","text":"<p>Safety limit prevents infinite loops:</p> <pre><code>loop = Loop(\n    name=\"bounded_loop\",\n    branch=never_ending_branch(),\n    break_on=\"never_true\",      # This never becomes True\n    max_iterations=5            # Loop stops here (iterations 0,1,2,3,4)\n)\n# Final iteration count: 4 (0-indexed)\n</code></pre>"},{"location":"advanced-patterns/loop-patterns/#advanced_loop_patterns","title":"Advanced Loop Patterns","text":""},{"location":"advanced-patterns/loop-patterns/#data_processing_loop","title":"Data Processing Loop","text":"<p>Process data in batches until complete:</p> <pre><code>def fetch_batch():\n    \"\"\"Fetch next batch of data.\"\"\"\n    import os\n    offset = int(os.environ.get(\"batch_idx\", 0)) * 100\n\n    data = fetch_from_database(limit=100, offset=offset)\n    has_more = len(data) == 100\n\n    # Process the batch\n    results = process_batch(data)\n\n    return len(results), not has_more  # Count processed, stop when no more data\n\ndef main():\n    batch_task = PythonTask(\n        name=\"process_batch\",\n        function=fetch_batch,\n        returns=[\"processed_count\", \"all_done\"]\n    )\n\n    batch_loop = Loop(\n        name=\"data_processing\",\n        branch=Pipeline(steps=[batch_task]),\n        break_on=\"all_done\",\n        index_as=\"batch_idx\",\n        max_iterations=50  # Process max 5000 records (50 * 100)\n    )\n\n    pipeline = Pipeline(steps=[batch_loop])\n    pipeline.execute()\n    return pipeline\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"advanced-patterns/loop-patterns/#convergence_loop","title":"Convergence Loop","text":"<p>Iterative algorithms with convergence criteria:</p> <pre><code>def training_iteration():\n    \"\"\"Single training iteration.\"\"\"\n    import os\n    import logging\n    logger = logging.getLogger(__name__)\n\n    epoch = int(os.environ.get(\"epoch\", 0))\n\n    # Train model for one epoch\n    model_loss = train_epoch(epoch)\n\n    # Check convergence\n    converged = model_loss &lt; 0.001\n\n    logger.info(f\"Epoch {epoch}: loss={model_loss:.6f}, converged={converged}\")\n\n    return model_loss, converged\n\ndef main():\n    train_task = PythonTask(\n        name=\"train_epoch\",\n        function=training_iteration,\n        returns=[\"loss\", \"converged\"]\n    )\n\n    training_loop = Loop(\n        name=\"model_training\",\n        branch=Pipeline(steps=[train_task]),\n        break_on=\"converged\",\n        index_as=\"epoch\",\n        max_iterations=100\n    )\n\n    pipeline = Pipeline(steps=[training_loop])\n    pipeline.execute()\n    return pipeline\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"advanced-patterns/loop-patterns/#retry_with_backoff","title":"Retry with Backoff","text":"<p>Implement retry logic with exponential backoff:</p> <pre><code>def attempt_operation():\n    \"\"\"Attempt operation with increasing delay.\"\"\"\n    import os\n    import time\n    import random\n    import logging\n    logger = logging.getLogger(__name__)\n\n    attempt = int(os.environ.get(\"attempt\", 0))\n\n    # Exponential backoff delay\n    if attempt &gt; 0:\n        delay = 2 ** attempt  # 2, 4, 8, 16 seconds\n        time.sleep(delay)\n\n    # Simulate operation with decreasing failure rate\n    success_rate = 0.3 + (attempt * 0.2)  # 30%, 50%, 70%, 90% success\n    success = random.random() &lt; success_rate\n\n    if not success:\n        logger.warning(f\"Attempt {attempt} failed, retrying...\")\n    else:\n        logger.info(f\"Attempt {attempt} succeeded!\")\n\n    return success\n\ndef main():\n    retry_task = PythonTask(\n        name=\"retry_operation\",\n        function=attempt_operation,\n        returns=[\"success\"]\n    )\n\n    retry_loop = Loop(\n        name=\"retry_with_backoff\",\n        branch=Pipeline(steps=[retry_task]),\n        break_on=\"success\",\n        index_as=\"attempt\",\n        max_iterations=5  # Max 5 attempts\n    )\n\n    pipeline = Pipeline(steps=[retry_loop])\n    pipeline.execute()\n    return pipeline\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"advanced-patterns/loop-patterns/#parameter_flow_in_loops","title":"Parameter Flow in Loops","text":""},{"location":"advanced-patterns/loop-patterns/#iteration_parameter_inheritance","title":"Iteration Parameter Inheritance","text":"<p>Parameters flow between iterations:</p> <ul> <li>Iteration 0: Inherits from parent scope</li> <li>Iteration N: Inherits from iteration N-1</li> <li>Loop Exit: Final iteration parameters copied back to parent scope</li> </ul> <pre><code>def accumulator_task(running_total: int = 0):\n    \"\"\"Task that accumulates values across iterations.\"\"\"\n    import os\n    import logging\n    logger = logging.getLogger(__name__)\n\n    iteration = int(os.environ.get(\"idx\", 0))\n\n    # Add current iteration value\n    running_total += iteration * 10  # 0, 10, 20, 30, ...\n\n    # Stop after 5 iterations\n    done = iteration &gt;= 4\n\n    logger.info(f\"Iteration {iteration}: total={running_total}\")\n\n    return running_total, done\n\ndef main():\n    accumulate_task = PythonTask(\n        name=\"accumulate\",\n        function=accumulator_task,\n        returns=[\"running_total\", \"done\"]\n    )\n\n    loop = Loop(\n        name=\"accumulator\",\n        branch=Pipeline(steps=[accumulate_task]),\n        break_on=\"done\",\n        index_as=\"idx\",\n        max_iterations=10\n    )\n\n    # Task after loop has access to final running_total\n    def show_final(running_total):\n        import logging\n        logger = logging.getLogger(__name__)\n        logger.info(f\"Final total: {running_total}\")\n\n    final_task = PythonTask(\n        name=\"show_final\",\n        function=show_final,\n        returns=[]\n    )\n\n    pipeline = Pipeline(steps=[loop, final_task])\n    pipeline.execute()\n    return pipeline\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"advanced-patterns/loop-patterns/#error_handling_in_loops","title":"Error Handling in Loops","text":""},{"location":"advanced-patterns/loop-patterns/#branch_failure_behavior","title":"Branch Failure Behavior","text":"<p>When any task in the branch fails:</p> <pre><code>def failing_branch():\n    \"\"\"Branch that might fail.\"\"\"\n    failing_task = PythonTask(\n        name=\"might_fail\",\n        function=lambda: 1/0,  # This will fail\n        returns=[]\n    )\n    return Pipeline(steps=[failing_task])\n\n# Loop exits immediately with FAIL status\n# No further iterations executed\n# Parameters from failed iteration are not copied back\n</code></pre>"},{"location":"advanced-patterns/loop-patterns/#graceful_error_handling","title":"Graceful Error Handling","text":"<p>Handle errors within the branch:</p> <pre><code>def robust_task():\n    \"\"\"Task with internal error handling.\"\"\"\n    import logging\n    logger = logging.getLogger(__name__)\n\n    try:\n        # Risky operation\n        result = risky_operation()\n        return result, False  # Success, continue\n    except Exception as e:\n        logger.error(f\"Error handled: {e}\")\n        return None, True  # Error handled, stop loop\n\ntask = PythonTask(\n    name=\"robust\",\n    function=robust_task,\n    returns=[\"result\", \"should_stop\"]\n)\n\n# Loop continues normally, stops gracefully on handled error\n</code></pre>"},{"location":"advanced-patterns/loop-patterns/#loop_configuration_reference","title":"Loop Configuration Reference","text":"<pre><code>Loop(\n    name=\"loop_name\",                 # Required: Unique identifier\n    branch=Pipeline(steps=[...]),     # Required: Pipeline to execute per iteration\n    max_iterations=10,                # Required: Safety limit (prevent infinite loops)\n    break_on=\"done\",                  # Required: Boolean parameter name for exit condition\n    index_as=\"iteration\",             # Required: Environment variable name for iteration count\n\n    # Standard node options\n    next_node=\"next_step\",            # Optional: Next node after loop completion\n    terminate_with_success=False,     # Optional: End pipeline successfully after loop\n    terminate_with_failure=False,     # Optional: End pipeline with failure after loop\n)\n</code></pre> <p>Parameter Types:</p> <ul> <li><code>name</code>: String identifier for the loop node</li> <li><code>branch</code>: Pipeline instance (not executed, passed as template)</li> <li><code>max_iterations</code>: Positive integer (1 or greater)</li> <li><code>break_on</code>: String matching a boolean return parameter from branch tasks</li> <li><code>index_as</code>: String for environment variable name (alphanumeric recommended)</li> </ul> <p>Common Patterns:</p> <ul> <li>Data Processing: <code>break_on=\"no_more_data\"</code>, <code>index_as=\"batch\"</code></li> <li>Training Loops: <code>break_on=\"converged\"</code>, <code>index_as=\"epoch\"</code></li> <li>Retry Logic: <code>break_on=\"success\"</code>, <code>index_as=\"attempt\"</code></li> <li>Polling: <code>break_on=\"ready\"</code>, <code>index_as=\"check\"</code></li> </ul>"},{"location":"advanced-patterns/loop-patterns/#best_practices","title":"Best Practices","text":""},{"location":"advanced-patterns/loop-patterns/#1_always_set_reasonable_max_iterations","title":"1. Always Set Reasonable max_iterations","text":"<p>Prevent infinite loops with conservative limits:</p> <pre><code># Good: Reasonable safety limits\nLoop(max_iterations=100)   # Training: 100 epochs max\nLoop(max_iterations=5)     # Retry: 5 attempts max\nLoop(max_iterations=1000)  # Data processing: 100k records max\n\n# Avoid: Too high or too low\nLoop(max_iterations=999999)  # Too high - potential runaway\nLoop(max_iterations=1)       # Too low - barely iterative\n</code></pre>"},{"location":"advanced-patterns/loop-patterns/#2_use_descriptive_environment_variable_names","title":"2. Use Descriptive Environment Variable Names","text":"<p>Choose clear names for iteration indices:</p> <pre><code># Good: Context-specific names\nLoop(index_as=\"epoch\")        # Machine learning\nLoop(index_as=\"batch_num\")    # Data processing\nLoop(index_as=\"attempt\")      # Retry logic\nLoop(index_as=\"round\")        # Iterative algorithms\n\n# Avoid: Generic names\nLoop(index_as=\"i\")           # Unclear purpose\nLoop(index_as=\"counter\")     # Generic\n</code></pre>"},{"location":"advanced-patterns/loop-patterns/#3_implement_proper_break_conditions","title":"3. Implement Proper Break Conditions","text":"<p>Ensure break conditions are reliable:</p> <pre><code># Good: Clear boolean return\ndef check_done():\n    completed = all_work_finished()\n    return completed  # Always boolean\n\n# Good: Handle edge cases\ndef safe_check():\n    try:\n        return is_complete()\n    except Exception:\n        return True  # Stop on errors\n\n# Avoid: Non-boolean returns\ndef bad_check():\n    return \"maybe\"  # Not boolean!\n</code></pre>"},{"location":"advanced-patterns/loop-patterns/#4_monitor_loop_progress","title":"4. Monitor Loop Progress","text":"<p>Add logging for debugging:</p> <pre><code>def logged_task():\n    import os\n    import logging\n    logger = logging.getLogger(__name__)\n\n    iteration = int(os.environ.get(\"iter\", 0))\n\n    logger.info(f\"Starting iteration {iteration}\")\n\n    # Your processing logic\n    result = process_data()\n\n    done = check_completion()\n    logger.info(f\"Iteration {iteration} complete, done={done}\")\n\n    return done\n</code></pre>"},{"location":"advanced-patterns/loop-patterns/#loop_vs_map_comparison","title":"Loop vs Map Comparison","text":"<p>Use Loop when:</p> <ul> <li>Unknown number of iterations</li> <li>Conditional termination based on results</li> <li>Sequential processing with state accumulation</li> <li>Retry/polling patterns</li> </ul> <p>Use Map when:</p> <ul> <li>Known collection of items to process</li> <li>Each iteration is independent</li> <li>Parallel processing possible</li> <li>Simple iteration over data</li> </ul> <pre><code># Loop: Unknown iterations\nLoop(\n    branch=process_until_converged(),\n    break_on=\"converged\",\n    max_iterations=100\n)\n\n# Map: Known iterations\nMap(\n    branch=process_item(),\n    iterate_on=\"items\",  # List of items\n    iterate_as=\"item\"    # Current item variable\n)\n</code></pre> <p>Next: Learn about Map Patterns for processing collections of data.</p>"},{"location":"advanced-patterns/map-patterns/","title":"\ud83d\udd04 Map Patterns","text":"<p>Process collections of data with the same workflow - like a for loop for pipelines.</p>"},{"location":"advanced-patterns/map-patterns/#the_basic_pattern","title":"The basic pattern","text":"<pre><code>flowchart TD\n    A[Start] --&gt; B[Map Step]\n    B --&gt; C[\"chunks = 1, 2, 3\"]\n    C --&gt; D{For each chunk}\n    D --&gt; E[Process chunk=1]\n    D --&gt; F[Process chunk=2]\n    D --&gt; G[Process chunk=3]\n    E --&gt; H[Result: 10]\n    F --&gt; I[Result: 20]\n    G --&gt; J[Result: 30]\n    H --&gt; K[\"Collect: 10, 20, 30\"]\n    I --&gt; K\n    J --&gt; K\n    K --&gt; L[Continue]</code></pre> <pre><code>from runnable import Map, Pipeline, PythonTask\n\ndef main():\n    # Create a map step that processes each chunk\n    map_state = Map(\n        name=\"process_chunks\",\n        iterate_on=\"chunks\",           # Parameter name containing [1, 2, 3]\n        iterate_as=\"chunk\",           # Each iteration gets chunk=1, chunk=2, chunk=3\n        branch=create_processing_workflow()  # Same workflow runs for each chunk\n    )\n\n    # Collect results after all iterations\n    collect_results = PythonTask(function=collect_all_results, name=\"collect\")\n\n    pipeline = Pipeline(steps=[map_state, collect_results])\n    pipeline.execute(parameters_file=\"parameters.yaml\")  # Contains chunks: [1, 2, 3]\n    return pipeline\n\n# Helper function to create the processing workflow\ndef create_processing_workflow():\n    # Implementation details in complete example below\n    pass\n\ndef collect_all_results():\n    # Implementation details in complete example below\n    pass\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> See complete runnable code examples/07-map/map.py<pre><code>\"\"\"\nYou can execute this pipeline by:\n\n    python examples/07-map/map.py\n\"\"\"\n\nfrom examples.common.functions import (\n    assert_default_reducer,\n    process_chunk,\n    read_processed_chunk,\n)\nfrom runnable import Map, NotebookTask, Pipeline, PythonTask, ShellTask, Stub\n\n\ndef iterable_branch(execute: bool = True):\n    \"\"\"\n    Use the pattern of using \"execute\" to control the execution of the pipeline.\n\n    The same pipeline can be run independently from the command line.\n\n    WARNING: If the execution is not controlled by \"execute\", the pipeline will be executed\n    even during the definition of the branch in parallel steps.\n    \"\"\"\n    # The python function to process a single chunk of data.\n    # In the example, we are multiplying the chunk by 10.\n    process_chunk_task_python = PythonTask(\n        name=\"execute_python\",\n        function=process_chunk,\n        returns=[\"processed_python\"],\n    )\n\n    # return parameters within a map branch have to be unique\n    # The notebook takes in the value of processed_python as an argument.\n    # and returns a new parameter \"processed_notebook\" which is 10*processed_python\n    process_chunk_task_notebook = NotebookTask(\n        name=\"execute_notebook\",\n        notebook=\"examples/common/process_chunk.ipynb\",\n        returns=[\"processed_notebook\"],\n    )\n\n    # following the pattern, the shell takes in the value of processed_notebook as an argument.\n    # and returns a new parameter \"processed_shell\" which is 10*processed_notebook.\n    shell_command = \"\"\"\n    if [ \"$processed_python\" = $( expr 10 '*' \"$chunk\" ) ] \\\n        &amp;&amp; [ \"$processed_notebook\" = $( expr 10 '*' \"$processed_python\" ) ] ; then\n            echo \"yaay\"\n        else\n            echo \"naay\"\n            exit 1;\n    fi\n    export processed_shell=$( expr 10 '*' \"$processed_notebook\")\n    \"\"\"\n\n    process_chunk_task_shell = ShellTask(\n        name=\"execute_shell\",\n        command=shell_command,\n        returns=[\"processed_shell\"],\n    )\n\n    # A downstream step of process_&lt;python, notebook, shell&gt; which reads the parameter \"processed\".\n    # The value of processed is within the context of the branch.\n    # For example, for chunk=1, the value of processed_python is chunk*10 = 10\n    # the value of processed_notebook is processed_python*10 = 100\n    # the value of processed_shell is processed_notebook*10 = 1000\n    read_chunk = PythonTask(\n        name=\"read processed chunk\",\n        function=read_processed_chunk,\n        terminate_with_success=True,\n    )\n\n    pipeline = Pipeline(\n        steps=[\n            process_chunk_task_python,\n            process_chunk_task_notebook,\n            process_chunk_task_shell,\n            read_chunk,\n        ],\n    )\n\n    if execute:\n        pipeline.execute()\n\n    return pipeline\n\n\ndef main():\n    # Create a map state which iterates over a list of chunks.\n    # chunk is the value of the iterable.\n    map_state = Map(  # [concept:map]\n        name=\"map_state\",\n        iterate_on=\"chunks\",  # [concept:iteration-parameter]\n        iterate_as=\"chunk\",  # [concept:iterator-variable]\n        branch=iterable_branch(execute=False),  # [concept:branch-pipeline]\n    )\n\n    # Outside of the loop, processed is a list of all the processed chunks.\n    # This is also called as the reduce pattern.\n    # the value of processed_python is [10, 20, 30]\n    # the value of processed_notebook is [100, 200, 300]\n    # the value of processed_shell is [1000, 2000, 3000]\n    collect = PythonTask(  # [concept:collect-step]\n        name=\"collect\",\n        function=assert_default_reducer,\n    )\n\n    continue_to = Stub(name=\"continue to\")\n\n    pipeline = Pipeline(steps=[map_state, collect])  # [concept:pipeline]\n\n    pipeline.execute(parameters_file=\"examples/common/initial_parameters.yaml\")  # [concept:execution]\n\n    return pipeline\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Try it now: <pre><code>uv run examples/07-map/map.py\n</code></pre></p> <p>Like writing: <pre><code>chunks = [1, 2, 3]\nresults = []\nfor chunk in chunks:\n    result = process_chunk(chunk)\n    results.append(result)\n</code></pre></p>"},{"location":"advanced-patterns/map-patterns/#the_branch_workflow","title":"The branch workflow","text":"<p>Each iteration runs this pipeline with different <code>chunk</code> values:</p> <p>Helper function (creates the workflow for each iteration): <pre><code>def create_processing_workflow():\n    from runnable import Pipeline, PythonTask, NotebookTask, ShellTask\n\n    # Process chunk through multiple steps\n    python_step = PythonTask(\n        function=process_chunk,           # chunk=1 \u2192 processed_python=10\n        returns=[\"processed_python\"]\n    )\n\n    notebook_step = NotebookTask(\n        notebook=\"process_chunk.ipynb\",   # processed_python=10 \u2192 processed_notebook=100\n        returns=[\"processed_notebook\"]\n    )\n\n    shell_step = ShellTask(\n        command=\"./process_chunk.sh\",     # processed_notebook=100 \u2192 processed_shell=1000\n        returns=[\"processed_shell\"]\n    )\n\n    return Pipeline(steps=[python_step, notebook_step, shell_step])\n</code></pre></p> <p>Each iteration runs the same pipeline structure:</p> <pre><code>flowchart LR\n    A[chunk] --&gt; B[Python Task]\n    B --&gt; C[Notebook Task]\n    C --&gt; D[Shell Task]\n    D --&gt; E[Read Task]\n    E --&gt; F[returns: processed]</code></pre>"},{"location":"advanced-patterns/map-patterns/#custom_reducers","title":"\ud83d\udd27 Custom reducers","text":"<p>By default, map collects all results into lists. Customize this with reducers:</p> <pre><code>flowchart TD\n    A[\"Results: 10, 20, 30\"] --&gt; B{Reducer}\n    B --&gt; C[\"Default: 10, 20, 30\"]\n    B --&gt; D[Max: 30]\n    B --&gt; E[Sum: 60]\n    B --&gt; F[Count: 3]</code></pre> <pre><code>from runnable import Map, Pipeline\n\ndef main():\n    # Use custom reducer to aggregate results\n    map_state = Map(\n        name=\"process_with_max\",\n        iterate_on=\"chunks\",                    # [1, 2, 3]\n        iterate_as=\"chunk\",\n        reducer=\"lambda *x: max(x)\",           # Take maximum instead of collecting all\n        branch=create_processing_workflow()\n    )\n\n    pipeline = Pipeline(steps=[map_state])\n    pipeline.execute(parameters_file=\"parameters.yaml\")  # Contains chunks: [1, 2, 3]\n    return pipeline\n\n# Results: processed_python = max(10, 20, 30) = 30\n#          processed_notebook = max(100, 200, 300) = 300\n#          processed_shell = max(1000, 2000, 3000) = 3000\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> See complete runnable code examples/07-map/custom_reducer.py<pre><code>\"\"\"\nmap states allows to repeat a branch for each value of an iterable.\n\nThe below example can written, in python, as:\n\nchunks = [1, 2, 3]\n\nfor chunk in chunks:\n    # Any task within the pipeline can access the value of chunk as an argument.\n    processed = process_chunk(chunk)\n\n    # The value of processed for every iteration is the value returned by the steps\n    # of the current execution. For example, the value of processed\n    # for chunk=1, is chunk*10 = 10 for downstream steps.\n    read_processed_chunk(chunk, processed)\n\nIt is possible to use a custom reducer, for example, this reducer is a max of the collection.\n# Once the reducer is applied, processed is reduced to a single value.\nassert processed == max(chunk * 10 for chunk in chunks)\n\nYou can execute this pipeline by:\n\n    python examples/07-map/custom_reducer.py\n\"\"\"\n\nfrom examples.common.functions import (\n    assert_custom_reducer,\n    process_chunk,\n    read_processed_chunk,\n)\nfrom runnable import Map, NotebookTask, Pipeline, PythonTask, ShellTask\n\n\ndef iterable_branch(execute: bool = True):\n    \"\"\"\n    Use the pattern of using \"execute\" to control the execution of the pipeline.\n\n    The same pipeline can be run independently from the command line.\n\n    WARNING: If the execution is not controlled by \"execute\", the pipeline will be executed\n    even during the definition of the branch in parallel steps.\n    \"\"\"\n    # The python function to process a single chunk of data.\n    # In the example, we are multiplying the chunk by 10.\n    process_chunk_task_python = PythonTask(\n        name=\"execute_python\",\n        function=process_chunk,\n        returns=[\"processed_python\"],\n    )\n\n    # return parameters within a map branch have to be unique\n    # The notebook takes in the value of processed_python as an argument.\n    # and returns a new parameter \"processed_notebook\" which is 10*processed_python\n    process_chunk_task_notebook = NotebookTask(\n        name=\"execute_notebook\",\n        notebook=\"examples/common/process_chunk.ipynb\",\n        returns=[\"processed_notebook\"],\n    )\n\n    # following the pattern, the shell takes in the value of processed_notebook as an argument.\n    # and returns a new parameter \"processed_shell\" which is 10*processed_notebook.\n    shell_command = \"\"\"\n    if [ \"$processed_python\" = $( expr 10 '*' \"$chunk\" ) ] \\\n        &amp;&amp; [ \"$processed_notebook\" = $( expr 10 '*' \"$processed_python\" ) ] ; then\n            echo \"yaay\"\n        else\n            echo \"naay\"\n            exit 1;\n    fi\n    export processed_shell=$( expr 10 '*' \"$processed_notebook\")\n    \"\"\"\n\n    process_chunk_task_shell = ShellTask(\n        name=\"execute_shell\",\n        command=shell_command,\n        returns=[\"processed_shell\"],\n    )\n\n    # A downstream step of process_&lt;python, notebook, shell&gt; which reads the parameter \"processed\".\n    # The value of processed is within the context of the branch.\n    # For example, for chunk=1, the value of processed_python is chunk*10 = 10\n    # the value of processed_notebook is processed_python*10 = 100\n    # the value of processed_shell is processed_notebook*10 = 1000\n    read_chunk = PythonTask(\n        name=\"read processed chunk\",\n        function=read_processed_chunk,\n        terminate_with_success=True,\n    )\n\n    pipeline = Pipeline(\n        steps=[\n            process_chunk_task_python,\n            process_chunk_task_notebook,\n            process_chunk_task_shell,\n            read_chunk,\n        ],\n    )\n\n    if execute:\n        pipeline.execute()\n\n    return pipeline\n\n\ndef main():\n    # Create a map state which iterates over a list of chunks.\n    # chunk is the value of the iterable.\n    # Upon completion of the map state, all the parameters of the tasks\n    # within the pipeline will be processed by the reducer.\n    # In this case, the reducer is the max of all the processed chunks.\n    map_state = Map(  # [concept:map-with-custom-reducer]\n        name=\"map state\",\n        iterate_on=\"chunks\",\n        iterate_as=\"chunk\",\n        reducer=\"lambda *x: max(x)\",  # [concept:custom-reducer]\n        branch=iterable_branch(execute=False),\n    )\n\n    collect = PythonTask(  # [concept:collect-step]\n        name=\"collect\",\n        function=assert_custom_reducer,\n        terminate_with_success=True,\n    )\n\n    pipeline = Pipeline(steps=[map_state, collect])  # [concept:pipeline]\n\n    pipeline.execute(parameters_file=\"examples/common/initial_parameters.yaml\")  # [concept:execution]\n\n    return pipeline\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Try it now: <pre><code>uv run examples/07-map/custom_reducer.py\n</code></pre></p>"},{"location":"advanced-patterns/map-patterns/#reducer_options","title":"Reducer Options","text":"<p>Lambda functions:</p> <ul> <li><code>\"lambda *x: max(x)\"</code> \u2192 Maximum value</li> <li><code>\"lambda *x: sum(x)\"</code> \u2192 Sum all values</li> <li><code>\"lambda *x: len(x)\"</code> \u2192 Count items</li> <li><code>\"lambda *x: x[0]\"</code> \u2192 Take first result only</li> </ul> <p>Python functions in dot notation:</p> <p>You can also reference Python functions using dot notation:</p> <pre><code># If you have a function in a module\n# my_module.py:\ndef custom_max_reducer(*results):\n    return max(results)\n\n# Use it in Map with dot notation\nmap_state = Map(\n    name=\"process_with_custom_reducer\",\n    iterate_on=\"chunks\",\n    iterate_as=\"chunk\",\n    reducer=\"my_module.custom_max_reducer\",  # Reference function by module path\n    branch=create_processing_workflow()\n)\n</code></pre> <p>Built-in function references:</p> <pre><code># Use built-in functions\nreducer=\"max\"           # Built-in max function\nreducer=\"sum\"           # Built-in sum function\nreducer=\"len\"           # Built-in len function\n\n# Use functions from standard library modules\nreducer=\"statistics.mean\"     # Average of results\nreducer=\"statistics.median\"   # Median of results\nreducer=\"operator.add\"        # Sum using operator module\n</code></pre>"},{"location":"advanced-patterns/map-patterns/#when_to_use_map","title":"When to use map","text":"<p>Perfect for:</p> <ul> <li>Processing file collections</li> <li>Batch processing data chunks</li> <li>Cross-validation in ML</li> <li>Parameter sweeps</li> <li>A/B testing multiple variants</li> </ul> <p>Example use cases: <pre><code># Process multiple datasets\niterate_on=\"datasets\", iterate_as=\"dataset\"\n\n# Test hyperparameters\niterate_on=\"learning_rates\", iterate_as=\"lr\"\n\n# Handle batch processing\niterate_on=\"file_paths\", iterate_as=\"file_path\"\n</code></pre></p> <p>Map vs Parallel</p> <ul> <li>Map: Same workflow, different data (for loop)</li> <li>Parallel: Different workflows, same time (independent tasks)</li> </ul> <p>Next: Learn about conditional workflows.</p>"},{"location":"advanced-patterns/mocking-testing/","title":"\ud83c\udfad Mocking and Testing","text":"<p>Test your pipeline structure and logic without running expensive operations.</p>"},{"location":"advanced-patterns/mocking-testing/#why_mock","title":"Why mock?","text":"<p>Test pipeline logic:</p> <ul> <li>Verify workflow structure</li> <li>Test parameter flow</li> <li>Check conditional branches</li> <li>Validate failure handling</li> </ul> <p>Without the cost:</p> <ul> <li>Skip slow ML training</li> <li>Avoid external API calls</li> <li>Test with fake data</li> <li>Debug workflow issues</li> </ul>"},{"location":"advanced-patterns/mocking-testing/#the_stub_pattern","title":"The stub pattern","text":"<p>Replace any task with a stub during development:</p> <pre><code>from runnable import Pipeline, Stub\n\ndef main():\n    # Replace expensive operations with stubs\n    data_extraction = Stub(name=\"extract_data\")         # Instead of slow API calls\n    model_training = Stub(name=\"train_model\")           # Instead of hours of training\n    report_generation = Stub(name=\"generate_report\")   # Instead of complex rendering\n\n    # Test your pipeline structure\n    pipeline = Pipeline(steps=[data_extraction, model_training, report_generation])\n    pipeline.execute()  # Runs instantly, tests workflow logic\n    return pipeline\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> See complete runnable code examples/01-tasks/stub.py<pre><code>\"\"\"\nThis is a simple pipeline that does 3 steps in sequence.\n\n    step 1 &gt;&gt; step 2 &gt;&gt; step 3 &gt;&gt; success\n\n    All the steps are stubbed and they will just pass through.\n    Use this pattern to define the skeleton of your pipeline\n    and flesh out the steps later.\n\n    Note that you can give any arbitrary keys to the steps\n    (like step 2).\n    This is handy to mock steps within mature pipelines.\n\n    You can run this pipeline by:\n       python examples/01-tasks/stub.py\n\nYou can execute this pipeline by:\n\n    python examples/01-tasks/stub.py\n\"\"\"\n\nfrom runnable import Pipeline, Stub\n\n\ndef main():\n    # this will always succeed\n    step1 = Stub(name=\"step1\")  # [concept:stub-task]\n\n    # It takes arbitrary arguments\n    # Useful for temporarily silencing steps within\n    # mature pipelines\n    step2 = Stub(name=\"step2\", what=\"is this thing\")  # [concept:stub-with-params]\n\n    step3 = Stub(name=\"step3\")  # [concept:stub-task]\n\n    pipeline = Pipeline(steps=[step1, step2, step3])  # [concept:pipeline]\n\n    pipeline.execute()  # [concept:execution]\n\n    # A function that creates pipeline should always return a\n    # Pipeline object\n    return pipeline\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Try it now: <pre><code>uv run examples/01-tasks/stub.py\n</code></pre></p> <p>Stubs act like real tasks but do nothing - perfect for testing structure.</p>"},{"location":"advanced-patterns/mocking-testing/#mock_entire_workflows","title":"Mock entire workflows","text":"<p>Replace expensive operations during testing:</p> <pre><code># Example task replacement (partial code)\n\n# Production version\nexpensive_training_task = PythonTask(\n    name=\"train_model\",\n    function=train_large_model,  # Takes hours\n    returns=[\"model\"]\n)\n\n# Test version\nmock_training_task = Stub(\n    name=\"train_model\",\n    returns=[\"model\"]  # Returns mock data\n)\n</code></pre>"},{"location":"advanced-patterns/mocking-testing/#configuration-based_mocking","title":"Configuration-based mocking","text":"<p>Use different configs for testing vs production:</p> <p>examples/08-mocking/mocked-config-simple.yaml: <pre><code>catalog:\n  type: file-system\n\nrun-log-store:\n  type: file-system\n\npipeline-executor:\n  type: mocked\n</code></pre></p> <p>No code changes needed - same pipeline, different behavior.</p>"},{"location":"advanced-patterns/mocking-testing/#patching_user_functions","title":"Patching user functions","text":"<p>Override specific functions for testing:</p> <p>examples/08-mocking/mocked-config-unittest.yaml: <pre><code>catalog:\n  type: file-system\n\nrun-log-store:\n  type: file-system\n\npipeline-executor:\n  type: mocked\n  config:\n    patches:\n      step 1:\n        command: exit 0\n</code></pre></p> <p>Advanced patching example: <pre><code>executor:\n  type: mocked\n  config:\n    patches:\n      hello python:\n        command: examples.common.functions.mocked_hello\n      hello shell:\n        command: echo \"hello from mocked\"\n      hello notebook:\n        command: examples/common/simple_notebook_mocked.ipynb\n</code></pre></p>"},{"location":"advanced-patterns/mocking-testing/#testing_patterns_with_mock_executor","title":"Testing patterns with mock executor","text":""},{"location":"advanced-patterns/mocking-testing/#test_workflow_structure","title":"Test workflow structure","text":"<pre><code>from runnable import Pipeline, PythonTask\n\ndef main():\n    # Your actual pipeline\n    pipeline = Pipeline(steps=[\n        PythonTask(function=extract_data, name=\"extract\"),\n        PythonTask(function=transform_data, name=\"transform\"),\n        PythonTask(function=load_data, name=\"load\")\n    ])\n\n    # Execute with mock configuration to test structure\n    pipeline.execute(configuration_file=\"examples/08-mocking/mocked-config-simple.yaml\")\n    return pipeline\n</code></pre>"},{"location":"advanced-patterns/mocking-testing/#test_with_patched_functions","title":"Test with patched functions","text":"<pre><code># test-config.yaml - Mock specific tasks\npipeline-executor:\n  type: mocked\n  config:\n    patches:\n      extract:\n        command: examples.test.functions.mock_extract_data\n      transform:\n        command: examples.test.functions.mock_transform_data\n</code></pre> <pre><code>def main():\n    pipeline = Pipeline(steps=[\n        PythonTask(function=extract_data, name=\"extract\"),\n        PythonTask(function=transform_data, name=\"transform\"),\n        PythonTask(function=load_data, name=\"load\")\n    ])\n\n    # Test with patched functions\n    pipeline.execute(configuration_file=\"test-config.yaml\")\n    return pipeline\n</code></pre>"},{"location":"advanced-patterns/mocking-testing/#test_failure_scenarios","title":"Test failure scenarios","text":"<pre><code># failure-test-config.yaml - Mock failure conditions\npipeline-executor:\n  type: mocked\n  config:\n    patches:\n      extract:\n        command: exit 1  # Simulate failure\n</code></pre> <pre><code>def main():\n    pipeline = Pipeline(steps=[\n        PythonTask(function=extract_data, name=\"extract\", on_failure=\"handle_failure\"),\n        PythonTask(function=handle_failure, name=\"handle_failure\"),\n        PythonTask(function=load_data, name=\"load\")\n    ])\n\n    # Test failure handling\n    pipeline.execute(configuration_file=\"failure-test-config.yaml\")\n    return pipeline\n</code></pre>"},{"location":"advanced-patterns/mocking-testing/#test_conditional_branches","title":"Test conditional branches","text":"<pre><code># branch-test-config.yaml - Mock decision outcomes\npipeline-executor:\n  type: mocked\n  config:\n    patches:\n      decision_task:\n        command: echo \"branch_a\"  # Force specific branch\n</code></pre> <pre><code>def main():\n    pipeline = Pipeline(steps=[\n        PythonTask(function=make_decision, name=\"decision_task\"),\n        # Conditional logic based on decision_task output\n    ])\n\n    # Test specific branch execution\n    pipeline.execute(configuration_file=\"branch-test-config.yaml\")\n    return pipeline\n</code></pre>"},{"location":"advanced-patterns/mocking-testing/#mocking_strategies","title":"Mocking strategies","text":"<p>Development:</p> <ul> <li>Start with stubs for all tasks</li> <li>Implement one task at a time</li> <li>Test each addition independently</li> </ul> <p>Testing:</p> <ul> <li>Mock external dependencies</li> <li>Use deterministic test data</li> <li>Test edge cases and failures</li> </ul> <p>Staging:</p> <ul> <li>Mix real and mocked components</li> <li>Test with production-like data</li> <li>Validate performance characteristics</li> </ul> <p>Mocking best practices</p> <ul> <li>Mock boundaries: External APIs, file systems, slow operations</li> <li>Keep interfaces: Mocks should match real task signatures</li> <li>Test both: Test with mocks AND real implementations</li> <li>Use configuration: Switch between mock/real via config files</li> </ul> <p>This completes our journey through Runnable's advanced patterns - from simple functions to sophisticated, resilient workflows.</p>"},{"location":"advanced-patterns/node-types/","title":"Custom Patterns","text":"<p>Create your own pipeline flow control patterns through runnable's extensible node type system.</p>"},{"location":"advanced-patterns/node-types/#the_core_insight","title":"The Core Insight","text":"<p>All node types follow the same pattern: They create pipeline flow control structures that wrap a <code>BaseNode</code> implementation, with runnable providing orchestration, graph traversal, and execution coordination.</p> <p>Runnable includes several built-in node types for common patterns:</p> <ul> <li>Task nodes: Execute individual functions, notebooks, or shell commands</li> <li>Parallel nodes: Execute multiple branches simultaneously (see Parallel Execution)</li> <li>Map nodes: Iterate over data with pipeline execution per item (see Map Patterns)</li> <li>Conditional nodes: Execute different branches based on parameters (see Conditional Workflows)</li> <li>Stub nodes: Pass-through nodes for testing and placeholders (see Mocking and Testing)</li> </ul>"},{"location":"advanced-patterns/node-types/#understanding_the_pattern_with_parallel_nodes","title":"Understanding the Pattern with Parallel Nodes","text":"<p>Let's examine how the <code>Parallel</code> node works to understand the extensibility pattern:</p> <pre><code>from runnable import Pipeline, PythonTask, Parallel\n\ndef main():\n    parallel_step = Parallel(\n        name=\"process_parallel\",\n        branches={\n            \"branch_a\": Pipeline(steps=[PythonTask(name=\"task_a\", function=task_a)]),\n            \"branch_b\": Pipeline(steps=[PythonTask(name=\"task_b\", function=task_b)])\n        }\n    )\n    pipeline = Pipeline(steps=[parallel_step])\n    pipeline.execute()\n    return pipeline\n</code></pre> <p>What happens internally:</p> <ol> <li><code>Parallel</code> (SDK class) provides the user API</li> <li><code>create_node()</code> method converts to <code>ParallelNode</code> (execution implementation)</li> <li><code>ParallelNode</code> handles the actual parallel branch coordination</li> <li>Entry point registration makes it discoverable</li> </ol>"},{"location":"advanced-patterns/node-types/#the_plugin_system","title":"The Plugin System","text":"<p>Node types are pluggable - runnable automatically discovers and loads custom node types via entry points.</p>"},{"location":"advanced-patterns/node-types/#how_pipeline_nodes_work_internally","title":"How Pipeline Nodes Work Internally","text":"<p>Every node type follows the same pattern:</p> <ol> <li>Node class: Provides the pipeline API (<code>Parallel</code>, <code>Map</code>, etc.)</li> <li>Node implementation: Handles the actual execution logic (<code>ParallelNode</code>, <code>MapNode</code>, etc.)</li> <li>Entry point registration: Makes it discoverable</li> </ol> <pre><code># Built-in node types are registered like this:\n[project.entry-points.'nodes']\n\"task\" = \"extensions.nodes.task:TaskNode\"\n\"parallel\" = \"extensions.nodes.parallel:ParallelNode\"\n\"map\" = \"extensions.nodes.map:MapNode\"\n\"conditional\" = \"extensions.nodes.conditional:ConditionalNode\"\n\"stub\" = \"extensions.nodes.stub:StubNode\"\n\"success\" = \"extensions.nodes.success:SuccessNode\"\n\"fail\" = \"extensions.nodes.fail:FailNode\"\n</code></pre>"},{"location":"advanced-patterns/node-types/#building_custom_node_types","title":"Building Custom Node Types","text":"<p>Create new node types for your specific pipeline flow control needs:</p>"},{"location":"advanced-patterns/node-types/#1_implement_the_node_implementation","title":"1. Implement the Node Implementation","text":"<pre><code># my_package/nodes.py\nfrom runnable.nodes import BaseNode\nfrom runnable.datastore import StepAttempt\n\nclass RetryNode(BaseNode):\n    \"\"\"Execute a pipeline with retry logic\"\"\"\n\n    def __init__(self, max_attempts: int = 3, **kwargs):\n        super().__init__(**kwargs)\n        self.max_attempts = max_attempts\n\n    def execute(self, **kwargs) -&gt; StepAttempt:\n        # Your retry execution logic\n        for attempt in range(self.max_attempts):\n            try:\n                # Execute the branch pipeline\n                result = self._execute_branch()\n                if result.status == \"SUCCESS\":\n                    return result\n            except Exception as e:\n                if attempt == self.max_attempts - 1:\n                    return StepAttempt(status=\"FAIL\", message=str(e))\n                # Log and continue to next attempt\n\n        return StepAttempt(status=\"FAIL\")\n</code></pre>"},{"location":"advanced-patterns/node-types/#2_create_the_pipeline_node_wrapper","title":"2. Create the Pipeline Node Wrapper","text":"<pre><code># my_package/nodes.py\nfrom runnable.sdk import BaseTraversal\n\nclass Retry(BaseTraversal):\n    \"\"\"Retry node for pipeline execution with failure recovery\"\"\"\n    branch: \"Pipeline\"\n    max_attempts: int = Field(default=3)\n\n    def create_node(self) -&gt; RetryNode:\n        return RetryNode(\n            name=self.name,\n            branch=self.branch._dag.model_copy(),\n            max_attempts=self.max_attempts,\n            **self.model_dump(exclude_defaults=True)\n        )\n</code></pre>"},{"location":"advanced-patterns/node-types/#3_register_the_node_type","title":"3. Register the Node Type","text":"<pre><code># pyproject.toml\n[project.entry-points.'nodes']\n\"retry\" = \"my_package.nodes:RetryNode\"\n</code></pre>"},{"location":"advanced-patterns/node-types/#4_use_your_custom_node_in_pipelines","title":"4. Use Your Custom Node in Pipelines","text":"<pre><code>from my_package.nodes import Retry\nfrom runnable import Pipeline, PythonTask\n\ndef main():\n    pipeline = Pipeline(steps=[\n        Retry(\n            name=\"resilient_process\",\n            max_attempts=5,\n            branch=Pipeline(steps=[\n                PythonTask(name=\"flaky_task\", function=potentially_failing_task)\n            ])\n        )\n    ])\n    pipeline.execute()\n    return pipeline\n</code></pre>"},{"location":"advanced-patterns/node-types/#integration_advantage","title":"Integration Advantage","text":"<p>\ud83d\udd11 Key Benefit: Custom node types live entirely in your codebase, enabling domain-specific pipeline flow control.</p>"},{"location":"advanced-patterns/node-types/#complete_control_customization","title":"Complete Control &amp; Customization","text":"<pre><code># In your private repository\n# company-workflows/nodes/business_nodes.py\n\nclass ApprovalGate(BaseTraversal):\n    \"\"\"Execute pipeline branch only after approval workflow\"\"\"\n    approval_channel: str = Field(...)\n    timeout_hours: int = Field(default=24)\n\n    def create_node(self) -&gt; ApprovalGateNode:\n        # Your proprietary approval system integration\n        pass\n</code></pre> <p>Integration benefits:</p> <ul> <li>\ud83d\udd12 Business Logic: Implement organization-specific workflow patterns and approvals</li> <li>\ud83c\udfe2 Domain-Specific: Create flow control for your specific business processes</li> <li>\ud83d\udcbc Governance: Built-in compliance gates, approval workflows, audit trails</li> <li>\ud83d\udd27 Standardization: Reusable flow control patterns across teams and projects</li> </ul>"},{"location":"advanced-patterns/node-types/#reusable_node_libraries","title":"Reusable Node Libraries","text":"<pre><code># Internal package: company-runnable-nodes\nfrom company_runnable_nodes import (\n    ApprovalGate,           # Business approval workflows\n    DataQualityGate,        # Quality control checkpoints\n    ComplianceCheck,        # Regulatory compliance gates\n    ResourceThrottle,       # Cost and resource management\n)\n\n# Teams build standardized workflows\npipeline = Pipeline(steps=[\n    PythonTask(name=\"prep\", function=prepare_data),\n    DataQualityGate(name=\"quality_check\", thresholds=quality_config),\n    ApprovalGate(name=\"manager_approval\", channel=\"#ml-approvals\"),\n    ComplianceCheck(name=\"sox_compliance\", requirements=[\"data_retention\", \"audit_trail\"]),\n    PythonTask(name=\"deploy\", function=deploy_model)\n])\n</code></pre> <p>This makes runnable a platform for building your organization's custom workflow patterns - standardized flow control, governance, and business logic.</p>"},{"location":"advanced-patterns/node-types/#need_help","title":"Need Help?","text":"<p>Custom node types involve understanding both pipeline flow control and your specific orchestration requirements.</p> <p>Get Support</p> <p>We're here to help you succeed! Building custom node types involves:</p> <ul> <li>Understanding runnable's graph execution engine and node lifecycle</li> <li>Implementing proper flow control and state management</li> <li>Integration with external systems for approvals, gates, or custom logic</li> <li>Plugin registration and pipeline composition</li> </ul> <p>Don't hesitate to reach out:</p> <ul> <li>\ud83d\udce7 Contact the team for architecture guidance and integration support</li> <li>\ud83e\udd1d Collaboration opportunities - we're interested in supporting workflow pattern innovations</li> <li>\ud83d\udcd6 Documentation feedback - help us improve these guides based on your implementation experience</li> </ul> <p>Your success with custom node types helps the entire runnable community!</p>"},{"location":"advanced-patterns/parallel-execution/","title":"\u26a1 Parallel Execution","text":"<p>Speed up workflows by running independent tasks simultaneously.</p>"},{"location":"advanced-patterns/parallel-execution/#the_pattern","title":"The pattern","text":"<p>When tasks don't depend on each other, run them in parallel:</p> <pre><code>flowchart TD\n    A[Start] --&gt; B[Parallel Step]\n    B --&gt; C[Branch 1]\n    B --&gt; D[Branch 2]\n    C --&gt; E[Task 1A]\n    C --&gt; F[Task 1B]\n    D --&gt; G[Task 2A]\n    D --&gt; H[Task 2B]\n    E --&gt; I[Continue]\n    F --&gt; I\n    G --&gt; I\n    H --&gt; I</code></pre> <pre><code>from runnable import Parallel, Pipeline, Stub\n\ndef main():\n    # Create the parallel step\n    parallel_step = Parallel(\n        name=\"parallel_step\",\n        branches={\n            \"branch1\": create_workflow(),  # Same workflow, independent execution\n            \"branch2\": create_workflow(),  # Same workflow, independent execution\n        }\n    )\n\n    # Continue after all branches complete\n    continue_step = Stub(name=\"continue_processing\")\n\n    pipeline = Pipeline(steps=[parallel_step, continue_step])\n    pipeline.execute()\n    return pipeline\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> See complete runnable code examples/06-parallel/parallel.py<pre><code>\"\"\"\nYou can execute this pipeline by:\n\n    python examples/06-parallel/parallel.py\n\"\"\"\n\nfrom examples.common.functions import hello\nfrom runnable import NotebookTask, Parallel, Pipeline, PythonTask, ShellTask, Stub\n\n\ndef traversal():\n    \"\"\"\n    Use the pattern of using \"execute\" to control the execution of the pipeline.\n\n    The same pipeline can be run independently from the command line.\n\n    WARNING: If the execution is not controlled by \"execute\", the pipeline will be executed\n    even during the definition of the branch in parallel steps.\n    \"\"\"\n    stub_task = Stub(name=\"hello stub\")\n\n    python_task = PythonTask(\n        name=\"hello python\",\n        function=hello,\n    )\n\n    shell_task = ShellTask(\n        name=\"hello shell\",\n        command=\"echo 'Hello World!'\",\n    )\n\n    notebook_task = NotebookTask(\n        name=\"hello notebook\",\n        notebook=\"examples/common/simple_notebook.ipynb\",\n        terminate_with_success=True,\n    )\n\n    # The pipeline has a mix of tasks.\n    # The order of execution follows the order of the tasks in the list.\n    pipeline = Pipeline(steps=[stub_task, python_task, shell_task, notebook_task])\n\n    return pipeline\n\n\ndef main():\n    parallel_step = Parallel(  # [concept:parallel]\n        name=\"parallel_step\",\n        branches={\n            \"branch1\": traversal(),  # [concept:branch-definition]\n            \"branch2\": traversal(),  # [concept:branch-definition]\n        },\n    )\n\n    continue_to = Stub(name=\"continue to\")  # [concept:continuation]\n\n    pipeline = Pipeline(steps=[parallel_step, continue_to])  # [concept:pipeline]\n\n    pipeline.execute()  # [concept:execution]\n    return pipeline\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Try it now: <pre><code>uv run examples/06-parallel/parallel.py\n</code></pre></p>"},{"location":"advanced-patterns/parallel-execution/#how_it_works","title":"How it works","text":"<ol> <li><code>Parallel()</code> \u2192 Creates parallel execution container</li> <li><code>branches={}</code> \u2192 Define independent workflows</li> <li>Same interface \u2192 Each branch is a regular pipeline</li> <li>Automatic synchronization \u2192 All branches complete before continuing</li> </ol>"},{"location":"advanced-patterns/parallel-execution/#real_workflow_example","title":"Real workflow example","text":"<p>Helper function (creates the workflow for each branch): <pre><code>def create_workflow():\n    from runnable import Pipeline, PythonTask, ShellTask, NotebookTask, Stub\n    from examples.common.functions import hello\n\n    # Each branch runs this same workflow independently\n    steps = [\n        Stub(name=\"hello_stub\"),\n        PythonTask(function=hello, name=\"hello_python\"),\n        ShellTask(command=\"echo 'Hello World!'\", name=\"hello_shell\"),\n        NotebookTask(notebook=\"examples/common/simple_notebook.ipynb\",\n                    name=\"hello_notebook\", terminate_with_success=True)\n    ]\n\n    return Pipeline(steps=steps)\n</code></pre></p> <p>Each branch runs the same workflow independently - perfect for processing different data sources or running multiple models.</p>"},{"location":"advanced-patterns/parallel-execution/#infinite_nesting","title":"\u267e\ufe0f Infinite nesting","text":"<p>Parallel steps can contain other parallel steps:</p> <pre><code>flowchart TD\n    A[Start] --&gt; B[Nested Parallel]\n    B --&gt; C[Branch 1]\n    B --&gt; D[Branch 2]\n    C --&gt; E[Sub-Parallel 1]\n    D --&gt; F[Sub-Parallel 2]\n    E --&gt; G[Task 1A]\n    E --&gt; H[Task 1B]\n    F --&gt; I[Task 2A]\n    F --&gt; J[Task 2B]\n    G --&gt; K[End]\n    H --&gt; K\n    I --&gt; K\n    J --&gt; K</code></pre> <pre><code>from runnable import Parallel, Pipeline\n\ndef main():\n    # Nested parallel: Each branch is itself a parallel pipeline\n    nested_parallel = Parallel(\n        name=\"nested_parallel\",\n        branches={\n            \"branch1\": parallel_pipeline(),  # This is a parallel pipeline\n            \"branch2\": parallel_pipeline()   # This is also a parallel pipeline\n        }\n    )\n\n    pipeline = Pipeline(steps=[nested_parallel])\n    pipeline.execute()\n    return pipeline\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> See complete runnable code examples/06-parallel/nesting.py<pre><code>\"\"\"\nExample to show case nesting of parallel steps.\n\nrunnable does not put a limit on the nesting of parallel steps.\nDeeply nested pipelines can be hard to read and not all\nexecutors support it.\n\nRun this pipeline as:\n    python examples/06-parallel/nesting.py\n\"\"\"\n\nfrom examples.common.functions import hello\nfrom runnable import NotebookTask, Parallel, Pipeline, PythonTask, ShellTask, Stub\n\n\ndef traversal(execute: bool = True):\n    \"\"\"\n    Use the pattern of using \"execute\" to control the execution of the pipeline.\n\n    The same pipeline can be run independently from the command line.\n\n    WARNING: If the execution is not controlled by \"execute\", the pipeline will be executed\n    even during the definition of the branch in parallel steps.\n    \"\"\"\n    stub_task = Stub(name=\"hello stub\")\n\n    python_task = PythonTask(\n        name=\"hello python\",\n        function=hello,\n    )\n\n    shell_task = ShellTask(\n        name=\"hello shell\",\n        command=\"echo 'Hello World!'\",\n    )\n\n    notebook_task = NotebookTask(\n        name=\"hello notebook\",\n        notebook=\"examples/common/simple_notebook.ipynb\",\n        terminate_with_success=True,\n    )\n\n    # The pipeline has a mix of tasks.\n    # The order of execution follows the order of the tasks in the list.\n    pipeline = Pipeline(steps=[stub_task, python_task, shell_task, notebook_task])\n\n    if execute:  # Do not execute the pipeline if we are using it as a branch\n        pipeline.execute()\n\n    return pipeline\n\n\ndef parallel_pipeline(execute: bool = True):\n    parallel_step = Parallel(\n        name=\"parallel step\",\n        terminate_with_success=True,\n        branches={\n            \"branch1\": traversal(execute=False),\n            \"branch2\": traversal(execute=False),\n        },\n    )\n\n    pipeline = Pipeline(steps=[parallel_step])\n\n    if execute:\n        pipeline.execute()\n    return pipeline\n\n\ndef main():\n    # Create a parallel step with parallel steps as branches.\n    parallel_step = Parallel(\n        name=\"nested_parallel\",\n        terminate_with_success=True,\n        branches={\n            \"branch1\": parallel_pipeline(execute=False),\n            \"branch2\": parallel_pipeline(execute=False),\n        },\n    )\n\n    pipeline = Pipeline(steps=[parallel_step])\n    pipeline.execute()\n    return pipeline\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Try it now: <pre><code>uv run examples/06-parallel/nesting.py\n</code></pre></p>"},{"location":"advanced-patterns/parallel-execution/#execution_environment","title":"Execution Environment","text":"<p>Parallel execution behavior depends on your pipeline executor:</p> Executor Parallel Support Configuration Required Local \u2705 Conditional <code>enable_parallel: true</code> + <code>chunked-fs</code> run log store Local Container \u2705 Conditional <code>enable_parallel: true</code> + <code>chunked-fs</code> run log store Argo Workflows \u2705 Always Built-in parallel orchestration"},{"location":"advanced-patterns/parallel-execution/#enable_local_parallel_execution","title":"Enable Local Parallel Execution","text":"<p>To run examples with parallel processing on your local machine:</p> ConfigurationExecute parallel_config.yaml<pre><code>pipeline-executor:\n  type: local  # or local-container\n  config:\n    enable_parallel: true\n\nrun-log-store:\n  type: chunked-fs  # Required for parallel writes\n\ncatalog:\n  type: file-system\n</code></pre> <pre><code># Run with parallel configuration\nRUNNABLE_CONFIGURATION_FILE=parallel_config.yaml uv run examples/06-parallel/parallel.py\n</code></pre> <p>Automatic Fallback</p> <p>If you don't configure parallel execution, local executors automatically run branches sequentially - examples still work perfectly!</p> <p>When to use parallel</p> <ul> <li>Independent data processing streams</li> <li>Running multiple ML models simultaneously</li> <li>Parallel feature engineering on different datasets</li> <li>Processing different file formats concurrently</li> </ul> <p>Next: Learn about map patterns for iterative processing.</p>"},{"location":"advanced-patterns/retry-recovery/","title":"\ud83d\udd04 Retry &amp; Recovery","text":"<p>Re-execute failed pipelines while preserving successful steps and enabling cross-environment debugging.</p>"},{"location":"advanced-patterns/retry-recovery/#the_problem","title":"The Problem","text":"<p>Your production pipeline fails at step 3 out of 5 after running expensive computation. Traditional approaches mean starting from scratch:</p> <pre><code>flowchart TD\n    A[Step 1: Load Data \u2705] --&gt; B[Step 2: Train Model \u2705]\n    B --&gt; C[\u274c Step 3: Deploy FAILS]\n    C --&gt; D[Step 4: Test \u274c]\n    D --&gt; E[Step 5: Notify \u274c]\n\n    F[Traditional Retry: Start Over] -.-&gt; A\n    G[\u2728 Runnable Retry: Resume from Step 3] -.-&gt; C\n\n    classDef highlight stroke:#ff6b35,stroke-width:3px,stroke-dasharray: 5 5\n    class G highlight</code></pre> <p>Problems with starting over:</p> <ul> <li>\u274c Lose expensive computation (data loading + model training)</li> <li>\u274c Debugging in production is slow and risky</li> <li>\u274c No way to test fixes in local environment first</li> <li>\u274c Waste compute resources and time</li> </ul>"},{"location":"advanced-patterns/retry-recovery/#the_runnable_solution_surgical_retry","title":"The Runnable Solution: Surgical Retry","text":"<p>Runnable's retry system preserves successful work and enables cross-environment debugging. Instead of restarting failed pipelines from scratch, only the failed steps and subsequent steps re-execute.</p> <pre><code># Same pipeline structure, different environments\nfrom runnable import Pipeline, PythonTask\n\ndef main():\n    pipeline = Pipeline(steps=[\n        PythonTask(function=load_data, name=\"load_data\"),\n        PythonTask(function=train_model, name=\"train_model\"),\n        PythonTask(function=deploy_model, name=\"deploy\"),  # This failed\n        PythonTask(function=run_tests, name=\"test\"),\n        PythonTask(function=send_notification, name=\"notify\"),\n    ])\n\n    pipeline.execute()  # Environment determined by configuration\n    return pipeline\n</code></pre>"},{"location":"advanced-patterns/retry-recovery/#production_failed_debug_locally","title":"Production Failed? Debug Locally","text":"<p>Your Argo pipeline just failed at the deployment step after 2 hours of expensive model training. What do you do?</p> <p>The Runnable Way: Cross-Environment Debugging</p> <p>Fix your code locally and retry using the exact same run data from production, while preserving all the expensive work:</p> <pre><code># 1. Your Argo pipeline failed (after expensive training succeeded)\nargo submit prod-pipeline.yaml -p run_id=prod-failure-001\n# Output: FAILED at step 3 (deploy), steps 1-2 (load data + train model) succeeded\n\n# 2. Switch to local environment, same run_id, MODIFIED code\nRUNNABLE_RETRY_RUN_ID=prod-failure-001 \\\nRUNNABLE_CONFIGURATION_FILE=local.yaml \\\nuv run my_pipeline.py\n\n# What happens:\n# Step 1 (load data): \u23e9 SKIPPED - already successful in production\n# Step 2 (train model): \u23e9 SKIPPED - already successful in production\n# Step 3 (deploy): \ud83d\udd27 EXECUTES locally with your FIXED code\n# Step 4 (test): \u2705 EXECUTES if step 3 succeeds\n# Step 5 (notify): \u2705 EXECUTES if step 4 succeeds\n</code></pre> <p>Fix the bug in your code:</p> <pre><code>def deploy_model(model_data):\n    # Add your debug fixes here - code changes are allowed!\n    print(f\"DEBUG: Model data shape: {model_data.shape}\")\n\n    # Fix the original bug that caused production failure\n    deploy_url = get_deployment_url()  # This was missing!\n\n    # Deploy with fixed logic\n    return deploy_model_to_endpoint(model_data, deploy_url)\n</code></pre> <p>Deploy the fix back to production:</p> <pre><code># 3. Once working locally, deploy fix to production\nRUNNABLE_RETRY_RUN_ID=prod-failure-001 \\\nargo submit prod-pipeline.yaml -p retry_run_id=prod-failure-001 -p retry_indicator=2\n# Only the failed steps execute in production with your fixes\n</code></pre>"},{"location":"advanced-patterns/retry-recovery/#why_this_changes_everything","title":"Why This Changes Everything","text":"<p>\ud83d\udd27 Fix Code, Keep Data Modify your functions to fix bugs while reusing the exact same data artifacts from the production run.</p> <p>\u26a1 Zero Waste Development The expensive model training (2 hours) runs once in production. Every debug iteration reuses that work.</p> <p>\ud83d\udee1\ufe0f Production Safety Debug and test code fixes on your laptop. Only deploy to production once you know it works.</p> <p>\ud83c\udf0d Cross-Environment Freedom Same pipeline structure, same parameters, same data - but different code and different execution environment.</p> <p>What Can Change vs What's Locked</p> <p>\u2705 Can Change (Debugging Freedom):</p> <ul> <li>Function implementations (fix bugs, add logging)</li> <li>Execution environment (local, container, Argo)</li> <li>Configuration files (resources, storage, etc.)</li> </ul> <p>\ud83d\udd12 Must Stay Same (Safety Constraints):</p> <ul> <li>Pipeline structure (same steps, same order)</li> <li>Step names and connections</li> <li>Parameters (uses original run's parameters)</li> </ul>"},{"location":"advanced-patterns/retry-recovery/#how_surgical_retry_works","title":"How Surgical Retry Works","text":"<p>Under the hood, Runnable's retry system uses four key mechanisms to enable safe cross-environment debugging:</p>"},{"location":"advanced-patterns/retry-recovery/#cli_retry_command","title":"CLI Retry Command","text":"<p>The simplest way to retry a failed run is using the <code>runnable retry</code> CLI command:</p> <pre><code># Retry a failed run\nrunnable retry &lt;run_id&gt;\n\n# Retry with a different configuration (e.g., local instead of Argo)\nrunnable retry &lt;run_id&gt; --config local.yaml\n\n# Retry with debug logging\nrunnable retry &lt;run_id&gt; --log-level DEBUG\n</code></pre> <p>The CLI automatically:</p> <ol> <li>Loads the original run log to find the pipeline definition</li> <li>Sets up the retry environment</li> <li>Re-executes the pipeline, skipping successful steps</li> </ol> <p>Run Log Store Must Match</p> <p>The retry command requires access to the original run's data. The configuration file you use must specify a run log store that can read the original run.</p> <p>For example, if the original run used <code>file-system</code> run log store in <code>.run_log_store/</code>, your retry config must also use <code>file-system</code> pointing to the same location.</p> <pre><code># retry-config.yaml - must match original run's storage\nrun-log-store:\n  type: file-system  # Same type as original run\n</code></pre>"},{"location":"advanced-patterns/retry-recovery/#environment_variable_method","title":"Environment Variable Method","text":"<p>Alternatively, retry mode activates when the <code>RUNNABLE_RETRY_RUN_ID</code> environment variable is set. This transforms normal execution into retry behavior:</p> <pre><code># Normal execution\nuv run my_pipeline.py\n\n# Retry execution - automatically detects and activates retry logic\nRUNNABLE_RETRY_RUN_ID=prod-failure-001 uv run my_pipeline.py\n</code></pre> <p>The system checks for this variable at startup. When present, it switches to retry mode and uses the specified run ID to locate the original execution data.</p>"},{"location":"advanced-patterns/retry-recovery/#step_skipping_logic","title":"Step Skipping Logic","text":"<p>During retry, each step is evaluated against the original run's execution history:</p> <ol> <li>Check Original Status - Query the run log store for the step's previous execution</li> <li>Success Check - If the last attempt was successful, skip the step entirely</li> <li>Failure Check - If the step failed or was never executed, run it with your new code</li> <li>Terminal Nodes - Always execute terminal success/failure nodes for proper cleanup</li> </ol> <p>This surgical approach means only failed and downstream steps re-execute, preserving all expensive successful work.</p>"},{"location":"advanced-patterns/retry-recovery/#safety_validations","title":"Safety Validations","text":"<p>Before retry begins, the system validates structural consistency:</p> <ul> <li>DAG Hash Verification - Ensures pipeline structure (steps, connections) hasn't changed</li> <li>Parameter Preservation - Uses original run's parameters, ignoring any new parameter files</li> <li>Run Log Availability - Confirms the original run data is accessible in the run log store</li> </ul> <p>If validation fails, retry is blocked to prevent data corruption or inconsistent results.</p>"},{"location":"advanced-patterns/retry-recovery/#attempt_tracking","title":"Attempt Tracking","text":"<p>The retry system maintains complete execution continuity through dual environment variables:</p> <ul> <li><code>RUNNABLE_RETRY_RUN_ID</code> - Links to the original failed run's data and execution context</li> <li><code>RUNNABLE_RETRY_INDICATOR</code> - Tracks which retry attempt this is (e.g., \"2\" for second attempt)</li> </ul> <p>This enables sophisticated retry chain tracking:</p> <ul> <li>Preserves History - Original attempts and their results remain intact</li> <li>New Attempts - Only re-executed steps get new attempt entries tagged with the retry indicator</li> <li>Cross-Environment Consistency - Same run ID works across local, container, and cloud environments</li> <li>Multi-Stage Retries - Can retry a retry, with each stage properly tracked via retry indicators</li> </ul> <p>This ensures debugging sessions maintain full traceability from the original production failure through multiple retry attempts to the successful fix.</p>"},{"location":"advanced-patterns/retry-recovery/#try_the_retry_examples","title":"Try the Retry Examples","text":"<p>Try the retry examples now: <pre><code># Simple retry example\nuv run examples/09-retry/simple_task.py\n\n# Linear pipeline retry\nuv run examples/09-retry/linear.py\n\n# Complex workflows with retry\nuv run examples/09-retry/parallel.py\nuv run examples/09-retry/conditional.py\nuv run examples/09-retry/map.py\n</code></pre></p> <p>Related: Failure Handling - Alternative execution paths vs retry</p>"},{"location":"architecture/context-isolation/","title":"Context Isolation in Runnable","text":""},{"location":"architecture/context-isolation/#problem_solved","title":"Problem Solved","text":"<p>Previously, <code>run_context</code> was a global variable that caused issues when multiple pipelines ran concurrently (e.g., in FastAPI endpoints). All pipelines would share the same context, leading to:</p> <ul> <li>Data leakage between pipelines</li> <li>Incorrect run IDs in logs</li> <li>Configuration mix-ups</li> <li>Resource conflicts</li> </ul>"},{"location":"architecture/context-isolation/#solution","title":"Solution","text":"<p>Replaced the global variable with Python's <code>contextvars</code> module, providing:</p> <ul> <li>Request isolation: Each execution context maintains its own run context</li> <li>Async safety: Contexts automatically propagate through async/await chains</li> <li>Thread safety: Works correctly with thread pools and concurrent execution</li> <li>Explicit error handling: Clear errors when no context is available</li> </ul>"},{"location":"architecture/context-isolation/#usage","title":"Usage","text":"<pre><code>from runnable.context import get_run_context, set_run_context\n\n# Get current context (returns None if no context)\ncurrent_context = get_run_context()\n\n# Set context (automatically isolated per request/task)\nset_run_context(my_context)\n\n# Context automatically propagates through async chains\nasync def my_async_function():\n    context = get_run_context()  # Same context as caller\n    await some_other_async_function()\n</code></pre>"},{"location":"architecture/context-isolation/#migration_notes","title":"Migration Notes","text":"<ul> <li>The global <code>context.run_context</code> variable has been removed</li> <li>New code must use <code>get_run_context()</code> to access the context</li> <li>Error handling is now explicit - functions raise <code>RuntimeError</code> if no context available</li> <li>No changes needed for FastAPI or async usage - isolation happens automatically</li> </ul>"},{"location":"architecture/context-isolation/#implementation_details","title":"Implementation Details","text":"<p>The implementation uses Python's <code>contextvars.ContextVar</code>:</p> <pre><code>_run_context_var: contextvars.ContextVar[Optional[RunnableContextType]] = contextvars.ContextVar(\n    'run_context',\n    default=None\n)\n\ndef get_run_context() -&gt; Optional[RunnableContextType]:\n    \"\"\"Get the current run context for this execution context.\"\"\"\n    return _run_context_var.get()\n\ndef set_run_context(context: RunnableContextType) -&gt; None:\n    \"\"\"Set the run context for this execution context.\"\"\"\n    _run_context_var.set(context)\n</code></pre> <p>This ensures each async task, thread, or request maintains its own isolated context.</p>"},{"location":"compare/kedro/","title":"\ud83c\udd9a Runnable vs Kedro: Simplicity Wins","text":"<p>Both Runnable and Kedro solve pipeline orchestration, but with radically different philosophies. Here's a side-by-side comparison using a real ML workflow.</p>"},{"location":"compare/kedro/#the_example_existing_ml_functions","title":"The Example: Existing ML Functions","text":"<p>Let's start with typical Python functions you might already have:</p> <pre><code>import pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nimport joblib\n\ndef load_and_clean_data():\n    \"\"\"Your existing data loading function.\"\"\"\n    customers = pd.read_csv(\"s3://bucket/raw-data/customers.csv\")\n    transactions = pd.read_csv(\"s3://bucket/raw-data/transactions.csv\")\n\n    data = customers.merge(transactions, on=\"customer_id\").dropna()\n    X = data.drop(['target'], axis=1)\n    y = data['target']\n\n    X.to_csv(\"features.csv\", index=False)\n    y.to_csv(\"target.csv\", index=False)\n    return {\"n_samples\": len(X), \"n_features\": X.shape[1]}\n\ndef train_random_forest(n_samples, n_features, max_depth=10):\n    \"\"\"Your existing RF training function.\"\"\"\n    X = pd.read_csv(\"features.csv\")\n    y = pd.read_csv(\"target.csv\").values.ravel()\n\n    model = RandomForestClassifier(max_depth=max_depth, random_state=42)\n    model.fit(X, y)\n    joblib.dump(model, \"rf_model.pkl\")\n\n    return {\"model_type\": \"RandomForest\", \"accuracy\": model.score(X, y)}\n\ndef train_xgboost(n_samples, n_features, max_depth=10):\n    \"\"\"Your existing XGBoost training function.\"\"\"\n    X = pd.read_csv(\"features.csv\")\n    y = pd.read_csv(\"target.csv\").values.ravel()\n\n    model = xgb.XGBClassifier(max_depth=max_depth, random_state=42)\n    model.fit(X, y)\n    joblib.dump(model, \"xgb_model.pkl\")\n\n    return {\"model_type\": \"XGBoost\", \"accuracy\": model.score(X, y)}\n\ndef select_best_model(model_results):\n    \"\"\"Your existing model selection function.\"\"\"\n    best_model = max(model_results, key=lambda x: x['accuracy'])\n    # Copy best model logic...\n    return best_model\n</code></pre> <p>Goal: Create a pipeline that runs these functions with parallel model training.</p>"},{"location":"compare/kedro/#making_it_work_with_runnable","title":"Making It Work with Runnable","text":"<p>Work required: Add pipeline wrapper (functions stay unchanged)</p> <pre><code>from runnable import Pipeline, PythonTask, Parallel, Catalog\n\n# Import your existing functions (no changes needed)\nfrom your_ml_code import load_and_clean_data, train_random_forest, train_xgboost, select_best_model\n\ndef main():\n    pipeline = Pipeline(steps=[\n        PythonTask(function=load_and_clean_data, returns=[\"n_samples\", \"n_features\"]),\n        Parallel(branches={\n            \"rf\": PythonTask(function=train_random_forest, returns=[\"rf_results\"]).as_pipeline(),\n            \"xgb\": PythonTask(function=train_xgboost, returns=[\"xgb_results\"]).as_pipeline()\n        }),\n        PythonTask(function=select_best_model, returns=[\"best_model\"])\n    ])\n    pipeline.execute()\n    return pipeline  # Required for Runnable\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>That's it. Functions unchanged, single wrapper file.</p>"},{"location":"compare/kedro/#making_it_work_with_kedro","title":"Making It Work with Kedro","text":"<p>Work required: Project restructuring + configuration files</p>"},{"location":"compare/kedro/#required_project_structure","title":"Required Project Structure","text":"<pre><code>ml-kedro-project/\n\u251c\u2500\u2500 conf/base/\n\u2502   \u251c\u2500\u2500 catalog.yml          # Data source/destination definitions\n\u2502   \u251c\u2500\u2500 parameters.yml       # Pipeline parameters\n\u2502   \u2514\u2500\u2500 logging.yml          # Logging configuration\n\u251c\u2500\u2500 src/ml_kedro_project/\n\u2502   \u251c\u2500\u2500 pipelines/\n\u2502   \u2502   \u251c\u2500\u2500 data_engineering/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 nodes.py     # Data processing functions\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 pipeline.py  # Pipeline definition\n\u2502   \u2502   \u2514\u2500\u2500 data_science/\n\u2502   \u2502       \u251c\u2500\u2500 nodes.py     # ML model functions\n\u2502   \u2502       \u2514\u2500\u2500 pipeline.py  # ML pipeline definition\n\u2502   \u2514\u2500\u2500 pipeline_registry.py # Register all pipelines\n\u2514\u2500\u2500 pyproject.toml\n</code></pre>"},{"location":"compare/kedro/#configuration_files_required","title":"Configuration Files Required","text":"<p>Data Catalog (<code>conf/base/catalog.yml</code>) <pre><code># Must define every data input/output with type and location\ncustomers_raw:\n  type: pandas.CSVDataSet\n  filepath: data/01_raw/customers.csv\n\nfeatures:\n  type: pandas.CSVDataSet\n  filepath: data/03_primary/features.csv\n\nrf_model:\n  type: pickle.PickleDataSet\n  filepath: data/06_models/rf_model.pkl\n# ... repeat for all data assets\n</code></pre></p> <p>Parameters (<code>conf/base/parameters.yml</code>) <pre><code>model_options:\n  max_depth: 15\n  random_state: 42\n</code></pre></p>"},{"location":"compare/kedro/#functions_must_be_restructured","title":"Functions Must Be Restructured","text":"<p>Original function: <pre><code>def train_random_forest(n_samples, n_features, max_depth=10):\n    # Your existing logic\n</code></pre></p> <p>Kedro requires changing to: <pre><code>def train_random_forest(features: pd.DataFrame, target: pd.Series,\n                       parameters: Dict[str, Any]) -&gt; Dict[str, Any]:\n    # Must accept data from catalog, parameters from config\n    model = RandomForestClassifier(max_depth=parameters[\"model_options\"][\"max_depth\"])\n    # Restructured logic to fit Kedro patterns\n    return {\"model\": model, \"accuracy\": accuracy}\n</code></pre></p> <p>Pipeline Registration Required: <pre><code># src/ml_kedro_project/pipeline_registry.py\ndef register_pipelines() -&gt; Dict[str, Pipeline]:\n    return {\n        \"__default__\": data_engineering.create_pipeline() + data_science.create_pipeline()\n    }\n</code></pre></p> <p>Running the Pipeline: <pre><code>kedro new --starter=pandas-iris ml-kedro-project\n# Implement node functions, pipeline definitions, configurations\nkedro run\n</code></pre></p>"},{"location":"compare/kedro/#core_capabilities_comparison","title":"Core Capabilities Comparison","text":""},{"location":"compare/kedro/#workflow_features","title":"Workflow Features","text":"Feature Runnable Approach Kedro Approach Pipeline Definition Single Python file with minimal setup Structured project layout with enforced conventions Task Types Python, Notebooks, Shell, Stubs Python nodes Parallel Execution <code>Parallel()</code> with explicit branching Automatic dependency resolution Conditional Logic Native <code>Conditional()</code> support Manual implementation in node logic Map/Reduce Native <code>Map()</code> with custom reducers Manual implementation required Iterative Loops Native <code>Loop()</code> with break conditions and safety limits Manual implementation with while loops required"},{"location":"compare/kedro/#data_handling","title":"Data Handling","text":"Feature Runnable Approach Kedro Approach File Management Simple <code>Catalog(put/get)</code> with minimal config Rich catalog.yml definitions with fine control Data Versioning Content-based hashing for change detection Timestamp-based versioning Storage Backends File, S3, Minio via plugins 20+ built-in dataset types with validation Data Lineage Automatic via run logs kedro-viz visualization"},{"location":"compare/kedro/#production_deployment","title":"Production Deployment","text":"Feature Runnable Approach Kedro Approach Environment Portability Same code runs local/container/K8s/Argo Requires deployment-specific configurations Container Execution Same containerized code runs across environments May require deployment-specific configurations Extensibility Entry points auto-discovery - custom executors, catalogs, secrets in your codebase Plugin system - public kedro-* packages or custom internal plugins Monitoring Basic run logs Rich hooks ecosystem MLOps Integration Tool-agnostic - choose your own MLOps stack Plugin ecosystem (MLflow, Airflow via kedro-* packages)"},{"location":"compare/kedro/#when_to_choose_each_tool","title":"When to Choose Each Tool","text":""},{"location":"compare/kedro/#choose_runnable_when","title":"Choose Runnable When:","text":"<ul> <li>Working with existing Python functions without refactoring</li> <li>Need multi-environment portability (local \u2192 container \u2192 K8s \u2192 Argo)</li> <li>Require advanced workflow patterns (parallel, conditional, map-reduce)</li> <li>Want immediate productivity with minimal setup</li> <li>Working with mixed task types (Python + notebooks + shell)</li> </ul>"},{"location":"compare/kedro/#choose_kedro_when","title":"Choose Kedro When:","text":"<ul> <li>Need standardized project structure across large teams</li> <li>Require rich data catalog features and validation</li> <li>Heavy ETL pipelines with extensive data governance needs</li> <li>Want established MLOps ecosystem integrations (MLflow, Airflow)</li> <li>Already invested in Kedro infrastructure and expertise</li> </ul>"},{"location":"compare/kedro/#implementation_structure_comparison","title":"Implementation Structure Comparison","text":"<p>Runnable Approach:</p> <ul> <li>Minimal disruption: Wrap existing functions directly without changes</li> <li>Single file: Complete pipeline in one Python file</li> <li>No restructuring: Keep your current code organization and patterns</li> <li>Optional configuration: Add YAML configs only when needed for specific environments</li> </ul> <p>Kedro Approach:</p> <ul> <li>Project restructuring: Requires adopting Kedro's directory structure and conventions</li> <li>Multi-file organization: Separate files for nodes, pipelines, catalogs, and configurations</li> <li>Function refactoring: Convert existing functions to fit Kedro node patterns</li> <li>Required configuration: YAML files for catalog, parameters, and logging are essential</li> </ul>"},{"location":"compare/kedro/#try_both_yourself","title":"\ud83d\ude80 Try Both Yourself","text":"<p>Test Runnable (2 minutes): <pre><code>pip install runnable\n# Copy the Runnable example above\npython ml_pipeline.py\n</code></pre></p> <p>Test Kedro (2+ hours): <pre><code>pip install kedro\nkedro new --starter=pandas-iris my-project\n# Implement all the files shown above\nkedro run\n</code></pre></p> <p>The productivity difference speaks for itself.</p> <p>Next: See how Runnable compares to Metaflow and other orchestration tools.</p>"},{"location":"compare/metaflow/","title":"\ud83c\udd9a Runnable vs Metaflow: Capability Comparison","text":"<p>Both Runnable and Metaflow solve ML pipeline orchestration with different approaches. Here's a side-by-side comparison using a real ML workflow.</p>"},{"location":"compare/metaflow/#the_example_existing_ml_functions","title":"The Example: Existing ML Functions","text":"<p>Let's start with typical Python functions you might already have:</p> <pre><code>import pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nimport joblib\n\ndef load_and_clean_data():\n    \"\"\"Your existing data loading function.\"\"\"\n    customers = pd.read_csv(\"s3://bucket/raw-data/customers.csv\")\n    transactions = pd.read_csv(\"s3://bucket/raw-data/transactions.csv\")\n\n    data = customers.merge(transactions, on=\"customer_id\").dropna()\n    X = data.drop(['target'], axis=1)\n    y = data['target']\n\n    X.to_csv(\"features.csv\", index=False)\n    y.to_csv(\"target.csv\", index=False)\n    return {\"n_samples\": len(X), \"n_features\": X.shape[1]}\n\ndef train_random_forest(n_samples, n_features, max_depth=10):\n    \"\"\"Your existing RF training function.\"\"\"\n    X = pd.read_csv(\"features.csv\")\n    y = pd.read_csv(\"target.csv\").values.ravel()\n\n    model = RandomForestClassifier(max_depth=max_depth, random_state=42)\n    model.fit(X, y)\n    joblib.dump(model, \"rf_model.pkl\")\n\n    return {\"model_type\": \"RandomForest\", \"accuracy\": model.score(X, y)}\n\ndef train_xgboost(n_samples, n_features, max_depth=10):\n    \"\"\"Your existing XGBoost training function.\"\"\"\n    X = pd.read_csv(\"features.csv\")\n    y = pd.read_csv(\"target.csv\").values.ravel()\n\n    model = xgb.XGBClassifier(max_depth=max_depth, random_state=42)\n    model.fit(X, y)\n    joblib.dump(model, \"xgb_model.pkl\")\n\n    return {\"model_type\": \"XGBoost\", \"accuracy\": model.score(X, y)}\n\ndef select_best_model(model_results):\n    \"\"\"Your existing model selection function.\"\"\"\n    best_model = max(model_results, key=lambda x: x['accuracy'])\n    # Copy best model logic...\n    return best_model\n</code></pre> <p>Goal: Create a pipeline that runs these functions with parallel model training.</p>"},{"location":"compare/metaflow/#making_it_work_with_runnable","title":"Making It Work with Runnable","text":"<p>Work required: Add pipeline wrapper (functions stay unchanged)</p> <pre><code>from runnable import Pipeline, PythonTask, Parallel\n\n# Import your existing functions (no changes needed)\nfrom your_ml_code import load_and_clean_data, train_random_forest, train_xgboost, select_best_model\n\ndef main():\n    pipeline = Pipeline(steps=[\n        PythonTask(function=load_and_clean_data, returns=[\"n_samples\", \"n_features\"]),\n        Parallel(branches={\n            \"rf\": PythonTask(function=train_random_forest, returns=[\"rf_results\"]).as_pipeline(),\n            \"xgb\": PythonTask(function=train_xgboost, returns=[\"xgb_results\"]).as_pipeline()\n        }),\n        PythonTask(function=select_best_model, returns=[\"best_model\"])\n    ])\n    pipeline.execute()\n    return pipeline  # Required for Runnable\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>That's it. Functions unchanged, single wrapper file.</p>"},{"location":"compare/metaflow/#making_it_work_with_metaflow","title":"Making It Work with Metaflow","text":"<p>Work required: Convert functions to FlowSpec class structure</p> <p>Functions Can Stay External:</p> <pre><code># your_ml_code.py (functions unchanged)\ndef load_and_clean_data():\n    # Your existing logic stays the same\n    return {\"n_samples\": 1000, \"n_features\": 20}\n\ndef train_random_forest(n_samples, n_features, max_depth=10):\n    # Your existing logic stays the same\n    return {\"model_type\": \"RandomForest\", \"accuracy\": 0.95}\n</code></pre> <p>Metaflow requires FlowSpec wrapper: <pre><code>from metaflow import FlowSpec, step, Parameter\n# Import your existing functions (no changes needed)\nfrom your_ml_code import load_and_clean_data, train_random_forest, train_xgboost, select_best_model\n\nclass MLTrainingFlow(FlowSpec):\n    max_depth = Parameter('max_depth', default=15)\n\n    @step\n    def start(self):\n        # Call your existing function directly\n        data_stats = load_and_clean_data()\n        self.n_samples = data_stats['n_samples']\n        self.n_features = data_stats['n_features']\n        self.next(self.train_models, foreach=['RandomForest', 'XGBoost'])\n\n    @step\n    def train_models(self):\n        # Call your existing functions directly\n        if self.input == 'RandomForest':\n            results = train_random_forest(self.n_samples, self.n_features, self.max_depth)\n        else:\n            results = train_xgboost(self.n_samples, self.n_features, self.max_depth)\n\n        self.model_results = results\n        self.next(self.select_best)\n\n    @step\n    def select_best(self, inputs):\n        model_results = [input.model_results for input in inputs]\n        self.best = select_best_model(model_results)  # Call your existing function\n        self.next(self.end)\n\n    @step\n    def end(self):\n        pass\n</code></pre></p> <p>Running the Pipeline: <pre><code>python ml_metaflow.py run --max_depth 15\n</code></pre></p>"},{"location":"compare/metaflow/#core_capabilities_comparison","title":"Core Capabilities Comparison","text":""},{"location":"compare/metaflow/#workflow_features","title":"Workflow Features","text":"Feature Runnable Approach Metaflow Approach Pipeline Definition Single Python file with minimal setup FlowSpec class with decorators Task Types Python, Notebooks, Shell, Stubs Python steps with flow state Parameter Configuration YAML/JSON config files via <code>parameters_file</code> Config files and command-line parameters Parallel Execution <code>Parallel()</code> with explicit branching <code>foreach</code> parameter for fan-out execution Conditional Logic Native <code>Conditional()</code> support Manual implementation in step logic Map/Reduce Native <code>Map()</code> with custom reducers <code>foreach</code> with join steps for result aggregation Iterative Loops Native <code>Loop()</code> with break conditions and safety limits Loop decorator support with manual condition management"},{"location":"compare/metaflow/#data_handling","title":"Data Handling","text":"Feature Runnable Approach Metaflow Approach File Management Automatic file sync via <code>Catalog(put/get)</code> Manual file I/O - no catalog system Data Versioning Content-based hashing for change detection Automatic versioning via Metaflow datastore (Python objects only) Storage Backends File, S3, Minio via plugins Local, S3, Azure, GCP datastores Data Lineage Automatic via run logs Rich lineage through Metaflow UI"},{"location":"compare/metaflow/#production_deployment","title":"Production Deployment","text":"Feature Runnable Approach Metaflow Approach Environment Portability Same code runs local/container/K8s/Argo Same FlowSpec runs local/AWS/K8s with --with flags AWS Integration Manual configuration required Native AWS Batch, Step Functions integration Monitoring Basic run logs and timeline visualization Rich Metaflow UI with execution graphs Extensibility Entry points auto-discovery for custom task types, executors, catalogs Limited plugin system - primarily configuration-based extensions"},{"location":"compare/metaflow/#when_to_choose_each_tool","title":"When to Choose Each Tool","text":""},{"location":"compare/metaflow/#choose_runnable_when","title":"Choose Runnable When:","text":"<ul> <li>Working with existing Python functions without refactoring</li> <li>Need multi-environment portability (local \u2192 container \u2192 K8s \u2192 Argo)</li> <li>Require advanced workflow patterns (parallel, conditional, map-reduce)</li> <li>Want immediate productivity with minimal setup</li> <li>Working with mixed task types (Python + notebooks + shell)</li> </ul>"},{"location":"compare/metaflow/#choose_metaflow_when","title":"Choose Metaflow When:","text":"<ul> <li>Need rich execution visualization and monitoring</li> <li>Heavy investment in AWS services and infrastructure</li> <li>Managing hundreds/thousands of concurrent workflows</li> <li>Want automatic Python object serialization between steps</li> <li>Already familiar with decorator-based patterns</li> <li>Need built-in experiment tracking and comparison</li> </ul>"},{"location":"compare/metaflow/#implementation_structure_comparison","title":"Implementation Structure Comparison","text":"<p>Runnable Approach:</p> <ul> <li>Minimal disruption: Wrap existing functions directly without changes</li> <li>Single file: Complete pipeline in one Python file</li> <li>No restructuring: Keep your current code organization and patterns</li> <li>Optional infrastructure: Add AWS/K8s configs only when needed for specific environments</li> </ul> <p>Metaflow Approach:</p> <ul> <li>Function restructuring: Convert existing functions to fit FlowSpec class patterns</li> <li>Decorator-based: Use <code>@step</code> and <code>@parallel</code> decorators for flow control</li> <li>Flow state management: Store data in <code>self</code> attributes between steps</li> <li>Infrastructure integration: Built-in AWS Batch, Step Functions, S3 datastore</li> </ul> <p>Next: See how Runnable compares to Kedro and other orchestration tools.</p>"},{"location":"jobs/","title":"Jobs: Run Functions Once \ud83c\udfaf","text":"<p>Jobs are Runnable's solution for one-time function execution. Perfect for standalone tasks, testing, analysis, and automation scripts.</p>"},{"location":"jobs/#what_are_jobs","title":"What Are Jobs?","text":"<p>Jobs wrap your Python functions to provide:</p> <ul> <li>Execution tracking - Know what ran, when, and what happened</li> <li>Parameter management - Pass configuration from files or environment</li> <li>Output capture - Automatically store results and logs</li> <li>Error handling - Graceful failure management</li> <li>Reproducibility - Consistent execution across environments</li> </ul>"},{"location":"jobs/#quick_start","title":"Quick Start","text":"<pre><code>from runnable import PythonJob\n\ndef analyze_data():\n    \"\"\"A simple analysis function.\"\"\"\n    data = [1, 2, 3, 4, 5]\n    result = sum(data) / len(data)\n    print(f\"Average: {result}\")\n    return result\n\ndef main():\n    job = PythonJob(function=analyze_data)\n    job.execute()\n    return job\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> See complete runnable code examples/11-jobs/python_tasks.py<pre><code>\"\"\"\nYou can execute this pipeline by:\n\n    python examples/01-tasks/python_tasks.py\n\nThe stdout of \"Hello World!\" would be captured as execution\nlog and stored in the catalog.\n\nAn example of the catalog structure:\n\n.catalog\n\u2514\u2500\u2500 baked-heyrovsky-0602\n    \u2514\u2500\u2500 hello.execution.log\n\n2 directories, 1 file\n\n\nThe hello.execution.log has the captured stdout of \"Hello World!\".\n\"\"\"\n\nfrom examples.common.functions import hello\nfrom runnable import PythonJob\n\n\ndef main():\n    job = PythonJob(function=hello)\n\n    job.execute()\n\n    return job\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Try it now: <pre><code>uv run examples/11-jobs/python_tasks.py\n</code></pre></p>"},{"location":"jobs/#common_use_cases","title":"Common Use Cases","text":""},{"location":"jobs/#data_analysis","title":"\ud83d\udcca Data Analysis","text":"<p>Perfect for one-off analysis tasks:</p> <pre><code>def monthly_sales_analysis():\n    # run a notebook\n</code></pre>"},{"location":"jobs/#function_testing","title":"\ud83e\uddea Function Testing","text":"<p>Test your functions in isolation:</p> <pre><code>def test_my_algorithm():\n    # Test edge cases, validate outputs\n    assert my_algorithm([1, 2, 3]) == expected_result\n</code></pre>"},{"location":"jobs/#report_generation","title":"\ud83d\udcdd Report Generation","text":"<p>Generate standalone reports:</p> <pre><code>def generate_weekly_report():\n    # Collect data, create visualizations, export PDF\n    return \"report_2024_week_47.pdf\"\n</code></pre>"},{"location":"jobs/#job_types","title":"Job Types","text":"<p>Runnable supports multiple job types for different scenarios:</p> Job Type Purpose Example PythonJob Execute Python functions Data analysis, calculations NotebookJob Run Jupyter notebooks Interactive analysis, reports ShellJob Execute shell commands System operations, deployments"},{"location":"jobs/#key_features","title":"Key Features","text":""},{"location":"jobs/#simple_execution","title":"\u2705 Simple Execution","text":"<pre><code>job = PythonJob(function=my_function)\njob.execute()\n</code></pre>"},{"location":"jobs/#parameter_support","title":"\u2699\ufe0f Parameter Support","text":"<pre><code>job.execute(parameters_file=\"config.yaml\")\n</code></pre>"},{"location":"jobs/#automatic_output_storage","title":"\ud83d\udcc1 Automatic Output Storage","text":"<ul> <li>Execution logs captured automatically</li> <li>Results stored in catalog</li> <li>Reproducible execution history</li> </ul>"},{"location":"jobs/#when_to_use_jobs_vs_pipelines","title":"When to Use Jobs vs Pipelines","text":"Scenario Use Jobs Use Pipelines Single function to run \u2705 \u274c Testing a function \u2705 \u274c One-off analysis \u2705 \u274c Multi-step workflow \u274c \u2705 Data dependencies between steps \u274c \u2705 Reproducible processes \u274c \u2705 <p>Start Simple, Grow Complex</p> <p>Start with a Job to test your function, then evolve it into a Pipeline when you need multiple steps or complex workflows.</p>"},{"location":"jobs/#whats_next","title":"What's Next?","text":"<ul> <li>Jobs vs Pipelines - Detailed comparison and decision guide</li> </ul>"},{"location":"jobs/file-storage/","title":"File Storage \ud83d\udcc1","text":"<p>Automatically store files created during Job execution using the Catalog system.</p>"},{"location":"jobs/file-storage/#basic_file_storage","title":"Basic File Storage","text":"<p>Jobs can capture and store files your function creates:</p> <pre><code>from examples.common.functions import write_files\nfrom runnable import Catalog, PythonJob\n\ndef main():\n    write_catalog = Catalog(put=[\"df.csv\", \"data_folder/data.txt\"])\n    job = PythonJob(\n        function=write_files,\n        catalog=write_catalog,\n    )\n\n    job.execute()\n    return job\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> See complete runnable code examples/11-jobs/catalog.py<pre><code>from examples.common.functions import write_files\nfrom runnable import Catalog, PythonJob\n\n\ndef main():\n    write_catalog = Catalog(put=[\"df.csv\", \"data_folder/data.txt\"])\n    job = PythonJob(\n        function=write_files,\n        catalog=write_catalog,\n    )\n\n    job.execute()\n\n    return job\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Try it now: <pre><code>uv run examples/11-jobs/catalog.py\n</code></pre></p>"},{"location":"jobs/file-storage/#what_happens","title":"What Happens","text":"<p>Function Creates Files: - <code>df.csv</code> in working directory - <code>data_folder/data.txt</code> in subdirectory</p> <p>Catalog Stores Copies: <pre><code>.catalog/unsolvable-ramanujan-0634/\n\u251c\u2500\u2500 df.csv                    # Copied CSV file\n\u251c\u2500\u2500 data_folder/\n\u2502   \u2514\u2500\u2500 data.txt             # Copied text file\n\u2514\u2500\u2500 jobBGR.execution.log     # Execution log\n</code></pre></p> <p>Summary Shows: <pre><code>{\n    \"Output catalog content\": [\"df.csv\", \"data_folder/data.txt\"],\n    \"status\": \"SUCCESS\"\n}\n</code></pre></p>"},{"location":"jobs/file-storage/#copy_vs_no-copy_modes","title":"Copy vs No-Copy Modes","text":""},{"location":"jobs/file-storage/#copy_mode_default","title":"Copy Mode (Default)","text":"<pre><code># Files are copied to catalog\nCatalog(put=[\"results.csv\", \"model.pkl\"])\n# Same as: Catalog(put=[\"results.csv\", \"model.pkl\"], store_copy=True)\n</code></pre> <ul> <li>\u2705 Files copied to <code>.catalog/{run-id}/</code></li> <li>\u2705 Original files remain in working directory</li> <li>\u2705 Full file versioning and backup</li> </ul>"},{"location":"jobs/file-storage/#no-copy_mode_hash_only","title":"No-Copy Mode (Hash Only)","text":"<pre><code># Files are tracked but not copied\nCatalog(put=[\"large_dataset.csv\", \"model.pkl\"], store_copy=False)\n</code></pre> See complete runnable code examples/11-jobs/catalog_no_copy.py<pre><code>from examples.common.functions import write_files\nfrom runnable import Catalog, PythonJob\n\n\ndef main():\n    write_catalog = Catalog(put=[\"df.csv\", \"data_folder/data.txt\"], store_copy=False)\n    job = PythonJob(\n        function=write_files,\n        catalog=write_catalog,\n    )\n\n    job.execute()\n\n    return job\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <ul> <li>\u2705 MD5 hash captured for integrity verification</li> <li>\u2705 Files remain in working directory only</li> <li>\u2705 Prevents copying large or frequently unchanged data</li> </ul> <p>When to use <code>store_copy=False</code>:</p> <ul> <li>Large files (datasets, models) where copying is expensive</li> <li>Unchanging reference data that doesn't need versioning</li> <li>Network storage where files are already backed up</li> <li>Performance optimization for frequently accessed files</li> </ul>"},{"location":"jobs/file-storage/#file_pattern_support","title":"File Pattern Support","text":""},{"location":"jobs/file-storage/#exact_file_names","title":"Exact File Names","text":"<pre><code>Catalog(put=[\"results.csv\", \"model.pkl\", \"config.json\"])\n</code></pre>"},{"location":"jobs/file-storage/#directory_support","title":"Directory Support","text":"<pre><code>Catalog(put=[\"output_folder/\", \"logs/debug.log\", \"data/processed.csv\"])\n</code></pre>"},{"location":"jobs/file-storage/#glob_pattern_support","title":"Glob Pattern Support","text":"<pre><code># Glob patterns are supported\nCatalog(put=[\"plots/*.png\", \"reports/*.pdf\", \"logs/*.log\"])\n\n# Multiple patterns\nCatalog(put=[\"output/**/*.csv\", \"results/*.json\", \"charts/*.png\"])\n\n# Complex patterns\nCatalog(put=[\"data/**/processed_*.parquet\", \"models/best_model_*.pkl\"])\n</code></pre>"},{"location":"jobs/file-storage/#common_use_cases","title":"Common Use Cases","text":""},{"location":"jobs/file-storage/#data_analysis_job","title":"Data Analysis Job","text":"<pre><code>def analyze_sales_data():\n    # Analysis creates multiple outputs\n    df.to_csv(\"sales_summary.csv\")\n\n    # Create multiple plots\n    for region in [\"north\", \"south\", \"east\", \"west\"]:\n        plot_regional_data(region)\n        plt.savefig(f\"plots/sales_trend_{region}.png\")\n\n    with open(\"insights.txt\", \"w\") as f:\n        f.write(\"Key findings...\")\n\n    return {\"total_sales\": 50000}\n\n# Store all outputs using glob patterns\ncatalog = Catalog(put=[\"sales_summary.csv\", \"plots/*.png\", \"insights.txt\"])\njob = PythonJob(function=analyze_sales_data, catalog=catalog)\n</code></pre>"},{"location":"jobs/file-storage/#model_training_job","title":"Model Training Job","text":"<pre><code>def train_model():\n    # Training creates artifacts\n    model.save(\"trained_model.pkl\")\n    history.to_csv(\"training_history.csv\")\n\n    with open(\"model_metrics.json\", \"w\") as f:\n        json.dump({\"accuracy\": 0.95}, f)\n\n    return model\n\n# Store model artifacts\ncatalog = Catalog(put=[\"trained_model.pkl\", \"training_history.csv\", \"model_metrics.json\"])\njob = PythonJob(function=train_model, catalog=catalog)\n</code></pre>"},{"location":"jobs/file-storage/#report_generation_job","title":"Report Generation Job","text":"<pre><code>def generate_monthly_report():\n    # Report generation creates files\n    create_pdf_report(\"monthly_report.pdf\")\n\n    # Generate multiple chart types\n    save_charts_to(\"charts/\")  # Creates charts/sales.png, charts/growth.png, etc.\n\n    # Export data in multiple formats\n    export_data_to(\"data/summary.csv\")\n    export_data_to(\"data/details.json\")\n\n    return \"Report completed\"\n\n# Store report outputs using glob patterns\ncatalog = Catalog(\n    put=[\"monthly_report.pdf\", \"charts/*.png\", \"data/*.csv\", \"data/*.json\"],\n    store_copy=True  # Reports should be archived\n)\njob = PythonJob(function=generate_monthly_report, catalog=catalog)\n</code></pre>"},{"location":"jobs/file-storage/#large_data_processing_job","title":"Large Data Processing Job","text":"<pre><code>def process_large_dataset():\n    # Processing creates large intermediate files\n    processed_data.to_parquet(\"processed_data.parquet\")  # Large file\n    summary_stats.to_csv(\"summary.csv\")                   # Small file\n\n    return {\"rows_processed\": 1000000}\n\n# Mixed storage strategy\ncatalog = Catalog(put=[\"processed_data.parquet\", \"summary.csv\"], store_copy=False)\n# Hash-only for the large file, but still tracks both\njob = PythonJob(function=process_large_dataset, catalog=catalog)\n</code></pre>"},{"location":"jobs/file-storage/#catalog_structure","title":"Catalog Structure","text":"<p>Jobs organize files by run ID:</p> <pre><code>.catalog/\n\u251c\u2500\u2500 run-id-001/\n\u2502   \u251c\u2500\u2500 function_name.execution.log\n\u2502   \u251c\u2500\u2500 output_file1.csv\n\u2502   \u2514\u2500\u2500 data_folder/\n\u2502       \u2514\u2500\u2500 nested_file.txt\n\u251c\u2500\u2500 run-id-002/\n\u2502   \u251c\u2500\u2500 function_name.execution.log\n\u2502   \u2514\u2500\u2500 different_output.json\n\u2514\u2500\u2500 run-id-003/\n    \u251c\u2500\u2500 function_name.execution.log\n    \u2514\u2500\u2500 large_file.parquet  # Only if store_copy=True\n</code></pre>"},{"location":"jobs/file-storage/#best_practices","title":"Best Practices","text":""},{"location":"jobs/file-storage/#choose_appropriate_storage_mode","title":"\u2705 Choose Appropriate Storage Mode","text":"<pre><code># Small, important files - copy them\nCatalog(put=[\"config.json\", \"results.csv\"], store_copy=True)\n\n# Large, reference files - hash only\nCatalog(put=[\"dataset.parquet\", \"model.pkl\"], store_copy=False)\n</code></pre>"},{"location":"jobs/file-storage/#organize_output_files","title":"\u2705 Organize Output Files","text":"<pre><code>def my_analysis():\n    # Create organized output structure\n    os.makedirs(\"outputs\", exist_ok=True)\n    results.to_csv(\"outputs/results.csv\")\n    plots.savefig(\"outputs/visualization.png\")\n\ncatalog = Catalog(put=[\"outputs/\"])  # Store entire directory\n</code></pre>"},{"location":"jobs/file-storage/#document_file_purposes","title":"\u2705 Document File Purposes","text":"<pre><code># Clear naming for catalog files\ncatalog = Catalog(put=[\n    \"final_results.csv\",      # Main output\n    \"diagnostic_plots.png\",   # Quality checks\n    \"processing_log.txt\",     # Execution details\n])\n</code></pre>"},{"location":"jobs/file-storage/#whats_next","title":"What's Next?","text":"<p>You can now store Job outputs automatically! Next topics:</p> <ul> <li>Job Types - Shell and Notebook Jobs</li> </ul> <p>Ready to explore different Job types? Continue to Job Types!</p>"},{"location":"jobs/first-job/","title":"Your First Job","text":"<p>Let's start with the simplest possible Job: running a function once.</p>"},{"location":"jobs/first-job/#the_basics","title":"The Basics","text":"<p>A Job wraps your function to provide execution tracking, logging, and output storage.</p> <pre><code>from examples.common.functions import hello\nfrom runnable import PythonJob\n\ndef main():\n    job = PythonJob(function=hello)\n    job.execute()\n    return job  # REQUIRED: Always return the job object\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> See complete runnable code examples/11-jobs/python_tasks.py<pre><code>\"\"\"\nYou can execute this pipeline by:\n\n    python examples/01-tasks/python_tasks.py\n\nThe stdout of \"Hello World!\" would be captured as execution\nlog and stored in the catalog.\n\nAn example of the catalog structure:\n\n.catalog\n\u2514\u2500\u2500 baked-heyrovsky-0602\n    \u2514\u2500\u2500 hello.execution.log\n\n2 directories, 1 file\n\n\nThe hello.execution.log has the captured stdout of \"Hello World!\".\n\"\"\"\n\nfrom examples.common.functions import hello\nfrom runnable import PythonJob\n\n\ndef main():\n    job = PythonJob(function=hello)\n\n    job.execute()\n\n    return job\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Try it now: <pre><code>uv run examples/11-jobs/python_tasks.py\n</code></pre></p> <p>Always Return the Job Object</p> <p>Your <code>main()</code> function must return the job object. This is required for:</p> <ul> <li>Execution tracking - Runnable needs the job object to track execution status</li> <li>Metadata access - The returned object contains run IDs, execution logs, and results</li> <li>Integration compatibility - External tools expect the job object for further processing</li> <li>Debugging support - Access to execution context and error details</li> </ul> <p>\u274c Without return: <pre><code>def main():\n    job = PythonJob(function=hello)\n    job.execute()\n    # Missing return - breaks Runnable's execution model!\n</code></pre></p> <p>\u2705 Correct pattern: <pre><code>def main():\n    job = PythonJob(function=hello)\n    job.execute()\n    return job  # Required for proper execution tracking\n</code></pre></p>"},{"location":"jobs/first-job/#what_happens_when_you_run_it","title":"What Happens When You Run It","text":"<p>1. Rich Context Display <pre><code>\ud83c\udfc3 Lets go!!\nWorking with context:\nJobContext(\n    run_id='null-panini-0628',\n    catalog=FileSystemCatalog(catalog_location='.catalog'),\n    job_executor=LocalJobExecutor(),\n    ...\n)\n</code></pre></p> <p>2. Function Execution <pre><code>Hello World!\nWARNING:This is a warning message.\n</code></pre></p> <p>3. Automatic Storage</p> <ul> <li>Execution logs captured in <code>catalog</code></li> <li>Unique run ID generated for tracking</li> <li>All output preserved automatically</li> </ul> <p>4. Summary Report <pre><code>{\n    \"Available parameters\": [],\n    \"Output parameters\": [],\n    \"status\": \"SUCCESS\"\n}\n</code></pre></p>"},{"location":"jobs/first-job/#generated_run_ids","title":"Generated Run IDs","text":"<p>Each execution gets a unique, memorable run ID: - <code>null-panini-0628</code> - <code>minty-brattain-0628</code> - <code>feasible-booth-0628</code></p> <p>These organize your catalog and make runs easy to find.</p>"},{"location":"jobs/first-job/#custom_run_ids","title":"Custom Run IDs","text":"<p>Control execution tracking with custom identifiers:</p> <pre><code># Set custom run ID for tracking and debugging\nexport RUNNABLE_RUN_ID=\"experiment-learning-rate-comparison-v1\"\nuv run training_job.py\n</code></pre> <p>Benefits:</p> <ul> <li>Easy identification in logs and run history</li> <li>Consistent naming across related executions</li> <li>Better debugging when tracking specific experiments</li> <li>Integration with external systems using predictable IDs</li> </ul> <pre><code># Example: A/B testing with clear run IDs\nexport RUNNABLE_RUN_ID=\"model-comparison-baseline-v1\"\nuv run baseline_job.py\n\nexport RUNNABLE_RUN_ID=\"model-comparison-experimental-v1\"\nuv run experimental_job.py\n</code></pre> <p>Default vs Custom Run IDs</p> <p>Without RUNNABLE_RUN_ID: Auto-generated names like <code>null-panini-0628</code></p> <p>With RUNNABLE_RUN_ID: Your custom identifier <code>experiment-learning-rate-comparison-v1</code></p>"},{"location":"jobs/first-job/#job_types_at_a_glance","title":"Job Types at a Glance","text":"Job Type Purpose Example Use PythonJob Execute Python functions Data analysis, calculations ShellJob Run shell commands File processing, system ops NotebookJob Execute Jupyter notebooks Interactive analysis, reports"},{"location":"jobs/first-job/#whats_next","title":"What's Next?","text":"<p>Now that you can run basic Jobs:</p> <ul> <li>Working with Data - Store and return function outputs</li> <li>Parameters &amp; Environment - Configure Jobs without code changes</li> <li>File Storage - Automatically archive Job outputs</li> </ul> <p>Ready to return data from your Jobs? Continue to Working with Data!</p>"},{"location":"jobs/job-types/","title":"Job Types","text":"<p>Execute different types of tasks through runnable's extensible job type system.</p>"},{"location":"jobs/job-types/#the_core_insight","title":"The Core Insight","text":"<p>All job types follow the same pattern: They wrap a <code>TaskType</code> that handles the actual execution, with runnable providing the context, logging, and infrastructure.</p>"},{"location":"jobs/job-types/#built-in_job_types","title":"Built-in Job Types","text":""},{"location":"jobs/job-types/#python_jobs","title":"Python Jobs \ud83d\udc0d","text":"<p>Execute Python functions with full context tracking:</p> <pre><code>from runnable import PythonJob\n\ndef my_analysis():\n    return {\"result\": \"success\"}\n\ndef main():\n    job = PythonJob(function=my_analysis, returns=[\"result\"])\n    job.execute()\n    return job\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"jobs/job-types/#shell_jobs","title":"Shell Jobs \ud83d\udd27","text":"<p>Execute shell commands with full context tracking:</p> <pre><code>from runnable import ShellJob\n\ndef main():\n    job = ShellJob(command=\"echo 'Hello World!'\")\n    job.execute()\n    return job\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"jobs/job-types/#notebook_jobs","title":"Notebook Jobs \ud83d\udcd3","text":"<p>Execute Jupyter notebooks with complete output preservation:</p> <p>Installation Required</p> <p>Notebook execution requires the optional notebook dependency: <pre><code>pip install runnable[notebook]\n</code></pre></p> <pre><code>from runnable import NotebookJob\n\ndef main():\n    job = NotebookJob(notebook=\"examples/common/simple_notebook.ipynb\")\n    job.execute()\n    return job\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"jobs/job-types/#job_execution_context","title":"Job Execution Context","text":"<p>All Job types share the same rich execution context and features:</p>"},{"location":"jobs/job-types/#common_features_across_all_job_types","title":"Common Features Across All Job Types","text":"<ul> <li>Parameters: All jobs support the same parameter system (files, environment variables)</li> <li>Catalog: File storage and retrieval using glob patterns</li> <li>Return values: Specify what data to capture and store</li> <li>Secrets: Access to environment variables and secret management</li> <li>Run logs: Complete execution tracking and metadata</li> </ul>"},{"location":"jobs/job-types/#execution_context_example","title":"Execution Context Example","text":"<pre><code>JobContext(\n    execution_mode='python',           # or 'shell', 'notebook'\n    run_id='feasible-booth-0628',      # Unique execution ID\n    catalog=FileSystemCatalog(         # File storage\n        catalog_location='.catalog'\n    ),\n    job_executor=LocalJobExecutor(),   # Execution environment\n    secrets=EnvSecretsManager(),       # Secret management\n    run_log_store=FileSystemRunLogstore()  # Metadata storage\n)\n</code></pre>"},{"location":"jobs/job-types/#choosing_the_right_job_type","title":"Choosing the Right Job Type","text":"Use Case Best Job Type Why Custom data analysis PythonJob Full control, return values, type safety System operations ShellJob Leverage existing scripts and tools Interactive analysis NotebookJob Visual outputs, step-by-step exploration File processing ShellJob Use command-line tools directly Model training PythonJob or NotebookJob Python for code, Notebook for exploration Report generation NotebookJob Rich outputs with plots and formatting"},{"location":"jobs/job-types/#the_plugin_system","title":"The Plugin System","text":"<p>Job types are pluggable - runnable automatically discovers and loads custom job types via entry points.</p>"},{"location":"jobs/job-types/#how_jobs_work_internally","title":"How Jobs Work Internally","text":"<p>Every job type follows the same pattern:</p> <ol> <li>Job wrapper: Provides the user API (<code>PythonJob</code>, <code>ShellJob</code>, etc.)</li> <li>Task type: Handles the actual execution (<code>PythonTaskType</code>, <code>ShellTaskType</code>, etc.)</li> <li>Entry point registration: Makes it discoverable</li> </ol> <pre><code>[project.entry-points.'tasks']\n\"python\" = \"runnable.tasks:PythonTaskType\"\n\"shell\" = \"runnable.tasks:ShellTaskType\"\n\"notebook\" = \"runnable.tasks:NotebookTaskType\"\n</code></pre>"},{"location":"jobs/job-types/#building_custom_job_types","title":"Building Custom Job Types","text":"<p>Create new job types for your specific execution needs:</p>"},{"location":"jobs/job-types/#1_implement_the_task_type","title":"1. Implement the Task Type","text":"<pre><code># my_package/tasks.py\nfrom runnable.tasks import BaseTaskType\n\nclass RTaskType(BaseTaskType):\n    \"\"\"Execute R scripts with full runnable integration\"\"\"\n    task_type: str = \"r\"\n    script_path: str = Field(...)\n\n    def execute_command(\n        self,\n        map_variable: MapVariableType = None,\n    ) -&gt; StepAttempt:\n        # Your R execution logic\n        command = f\"Rscript {self.script_path}\"\n        # Run command and return StepAttempt\n        pass\n</code></pre>"},{"location":"jobs/job-types/#2_create_the_job_wrapper","title":"2. Create the Job Wrapper","text":"<pre><code># my_package/jobs.py\nfrom runnable.sdk import BaseJob\n\nclass RJob(BaseJob):\n    # The name of the plugin of Task\n    command_type: str = Field(default=\"r\")\n\n    # The fields should be the same as the corresponding task definition\n    script_path: str = Field(...)\n</code></pre>"},{"location":"jobs/job-types/#3_register_task_entry_point","title":"3. Register Task Entry Point","text":"<pre><code># pyproject.toml\n[project.entry-points.'tasks']\n\"r\" = \"my_package.tasks:RTaskType\"\n</code></pre>"},{"location":"jobs/job-types/#4_use_your_custom_job","title":"4. Use Your Custom Job","text":"<pre><code>from my_package.jobs import RJob\n\ndef main():\n    job = RJob(script_path=\"analysis.R\")\n    job.execute()\n    return job\n</code></pre>"},{"location":"jobs/job-types/#integration_advantage","title":"Integration Advantage","text":"<p>\ud83d\udd11 Key Benefit: Custom job types live entirely in your codebase, enabling domain-specific execution models.</p>"},{"location":"jobs/job-types/#complete_control_customization","title":"Complete Control &amp; Customization","text":"<pre><code># In your private repository\n# company-analytics/jobs/proprietary_jobs.py\n\nclass CompanyAnalyticsJob(BaseJob):\n    \"\"\"Execute proprietary analytics with company-specific integrations\"\"\"\n    dataset_id: str = Field(...)\n    compliance_level: str = Field(default=\"confidential\")\n\n    def get_task(self) -&gt; CompanyAnalyticsTaskType:\n        # Your proprietary task implementation\n        pass\n</code></pre> <p>Integration benefits:</p> <ul> <li>\ud83d\udd12 Proprietary Tools: Integrate with internal analytics platforms, databases, or custom tools</li> <li>\ud83c\udfe2 Domain-Specific: Create job types for your specific business logic (financial modeling, scientific computing, etc.)</li> <li>\ud83d\udcbc Compliance: Implement organization-specific security, audit, and governance requirements</li> <li>\ud83d\udd27 Standardization: Create reusable job types across teams and projects</li> </ul>"},{"location":"jobs/job-types/#reusable_job_libraries","title":"Reusable Job Libraries","text":"<pre><code># Internal package: company-runnable-jobs\nfrom company_runnable_jobs import (\n    FinancialModelingJob,     # Company financial calculations\n    ComplianceReportJob,      # SOX/regulatory reporting\n    DataScienceJob,          # Your ML platform integration\n    CustomerAnalyticsJob,    # CRM and analytics integration\n)\n\n# Teams use your standardized job types\njob = FinancialModelingJob(\n    model_type=\"monte_carlo\",\n    compliance_required=True\n)\njob.execute()\n</code></pre> <p>This makes runnable a platform for creating your company's custom execution ecosystem - from simple scripts to complex domain-specific workflows.</p>"},{"location":"jobs/job-types/#need_help","title":"Need Help?","text":"<p>Custom job types involve understanding both the task execution model and your target tool's integration requirements.</p> <p>Get Support</p> <p>We're here to help you succeed! Building custom job types involves:</p> <ul> <li>Understanding runnable's task execution lifecycle</li> <li>Integrating with external tools and platforms</li> <li>Proper error handling and result management</li> <li>Plugin registration and discovery</li> </ul> <p>Don't hesitate to reach out:</p> <ul> <li>\ud83d\udce7 Contact the team for architecture guidance and integration support</li> <li>\ud83e\udd1d Collaboration opportunities - we're interested in supporting domain-specific integrations</li> <li>\ud83d\udcd6 Documentation feedback - help us improve these guides based on your implementation experience</li> </ul> <p>Your success with custom job types helps the entire runnable community!</p>"},{"location":"jobs/parameters/","title":"Parameters &amp; Environment \u2699\ufe0f","text":"<p>Configure Jobs without changing code using parameters and environment variables.</p>"},{"location":"jobs/parameters/#parameter_files","title":"Parameter Files","text":"<p>Pass configuration to Jobs using YAML files:</p> <pre><code>from examples.torch.single_cpu import run_single_cpu_training\nfrom runnable import PythonJob\n\ndef main():\n    training_job = PythonJob(function=run_single_cpu_training)\n\n    # Execute with parameters from YAML file\n    training_job.execute(parameters_file=\"examples/torch/single_cpu_params.yaml\")\n\n    return training_job\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> See complete runnable code examples/torch/single_cpu_job.py<pre><code>\"\"\"\nRunnable job version of single CPU PyTorch training.\n\nThis demonstrates how to wrap the run_single_cpu_training function\nin a PythonJob and execute it using runnable with parameters from a YAML file.\n\nYou can execute this job by:\n    python examples/torch/single_cpu_job.py\n\"\"\"\n\nfrom examples.torch.single_cpu import run_single_cpu_training\nfrom runnable import PythonJob\n\n\ndef main():\n    \"\"\"\n    Create and execute a job with the single CPU training function using parameters from YAML file.\n    \"\"\"\n    # Create a PythonJob that wraps our training function\n    training_job = PythonJob(\n        function=run_single_cpu_training,\n    )\n\n    # Execute with parameters from the YAML file\n    training_job.execute(parameters_file=\"examples/torch/single_cpu_params.yaml\")\n\n    return training_job\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Parameter File (single_cpu_params.yaml): <pre><code>learning_rate: 0.01\nnum_epochs: 50\nbatch_size: 32\n</code></pre></p> <p>Parameter Display During Execution: <pre><code>Parameters available for the execution:\n{\n    'learning_rate': JsonParameter(value=0.01),\n    'num_epochs': JsonParameter(value=50),\n    'batch_size': JsonParameter(value=32)\n}\n</code></pre></p>"},{"location":"jobs/parameters/#environment_variable_overrides","title":"Environment Variable Overrides \ud83c\udfc6","text":"<p>Environment variables always win over YAML values:</p> <pre><code># Override individual parameters\nexport RUNNABLE_PRM_learning_rate=0.05\nexport RUNNABLE_PRM_num_epochs=100\n\n# Run the same Job - now uses overridden values!\nuv run examples/torch/single_cpu_job.py\n</code></pre> <p>Result: Job uses <code>learning_rate=0.05</code> and <code>num_epochs=100</code> instead of YAML values.</p>"},{"location":"jobs/parameters/#dynamic_parameter_files","title":"Dynamic Parameter Files","text":"<p>Switch parameter files without changing code:</p> <pre><code># Development environment\nexport RUNNABLE_PARAMETERS_FILE=\"configs/dev.yaml\"\nuv run my_job.py\n\n# Production environment\nexport RUNNABLE_PARAMETERS_FILE=\"configs/prod.yaml\"\nuv run my_job.py  # Same code, different config!\n</code></pre> <pre><code>def main():\n    job = PythonJob(function=my_function)\n\n    # No parameters_file specified - uses RUNNABLE_PARAMETERS_FILE\n    job.execute()\n    return job\n</code></pre>"},{"location":"jobs/parameters/#three-layer_parameter_precedence","title":"Three-Layer Parameter Precedence","text":"<p>Parameters are resolved in this order (highest priority wins):</p> <ol> <li>Individual overrides: <code>RUNNABLE_PRM_key=\"value\"</code></li> <li>Environment file: <code>RUNNABLE_PARAMETERS_FILE=\"config.yaml\"</code></li> <li>Code-specified: <code>job.execute(parameters_file=\"config.yaml\")</code></li> </ol> <p>Same flexibility as Pipelines</p> <p>Jobs inherit the exact same parameter system as Pipelines. Perfect for dev/staging/prod environments without code changes!</p>"},{"location":"jobs/parameters/#common_patterns","title":"Common Patterns","text":""},{"location":"jobs/parameters/#environment-specific_configurations","title":"Environment-Specific Configurations","text":"<p>Development: <pre><code>export RUNNABLE_PARAMETERS_FILE=\"configs/dev.yaml\"\nexport RUNNABLE_PRM_debug=true\nexport RUNNABLE_PRM_sample_size=1000\n</code></pre></p> <p>Production: <pre><code>export RUNNABLE_PARAMETERS_FILE=\"configs/prod.yaml\"\nexport RUNNABLE_PRM_debug=false\nexport RUNNABLE_PRM_sample_size=1000000\n</code></pre></p>"},{"location":"jobs/parameters/#complex_parameter_types","title":"Complex Parameter Types","text":"<pre><code># JSON objects\nexport RUNNABLE_PRM_model_config='{\"learning_rate\": 0.01, \"epochs\": 100}'\n\n# Lists\nexport RUNNABLE_PRM_features='[\"age\", \"income\", \"location\"]'\n\n# Nested configuration\nexport RUNNABLE_PRM_database='{\"host\": \"prod.db.com\", \"port\": 5432}'\n</code></pre>"},{"location":"jobs/parameters/#testing_different_values","title":"Testing Different Values","text":"<pre><code># Test different thresholds\nexport RUNNABLE_PRM_confidence_threshold=0.8\nuv run analysis_job.py\n\nexport RUNNABLE_PRM_confidence_threshold=0.9\nuv run analysis_job.py\n\n# Test different data sources\nexport RUNNABLE_PRM_data_source=\"s3://bucket/test-data.csv\"\nuv run processing_job.py\n\nexport RUNNABLE_PRM_data_source=\"s3://bucket/prod-data.csv\"\nuv run processing_job.py\n</code></pre>"},{"location":"jobs/parameters/#parameter_file_examples","title":"Parameter File Examples","text":""},{"location":"jobs/parameters/#basic_configuration","title":"Basic Configuration","text":"config.yaml<pre><code># Data settings\ninput_file: \"data/sales.csv\"\noutput_dir: \"results/\"\n\n# Processing settings\nbatch_size: 1000\nparallel_jobs: 4\n\n# Model settings\nmodel_type: \"random_forest\"\nmax_depth: 10\n</code></pre>"},{"location":"jobs/parameters/#parameter_validation","title":"Parameter Validation","text":"<p>Functions receive parameters with full type checking:</p> <pre><code>def process_data(\n    input_file: str,\n    batch_size: int = 100,\n    debug: bool = False,\n    model_config: dict = None\n):\n    # Parameters are validated and converted automatically\n    print(f\"Processing {input_file} with batch_size={batch_size}\")\n\n    if debug:\n        print(\"Debug mode enabled\")\n\n    return {\"processed\": True}\n</code></pre>"},{"location":"jobs/parameters/#converting_from_argparse_scripts","title":"Converting from Argparse Scripts","text":"<p>Zero-code migration: Existing argparse functions work directly with PythonJobs! Runnable automatically converts YAML parameters to <code>argparse.Namespace</code> objects.</p>"},{"location":"jobs/parameters/#your_existing_argparse_script","title":"Your Existing Argparse Script","text":"single_cpu_args.py<pre><code>import argparse\nimport torch\n# ... other imports\n\ndef run_single_cpu_training(args: argparse.Namespace):\n    \"\"\"Training function that expects parsed arguments.\"\"\"\n    print(f\"Learning Rate: {args.learning_rate}, Epochs: {args.num_epochs}\")\n    print(f\"Batch Size: {args.batch_size}\")\n\n    # Use args.learning_rate, args.num_epochs, args.batch_size\n    # ... training logic\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Single-CPU PyTorch Training\")\n    parser.add_argument(\"--learning_rate\", type=float, default=0.01)\n    parser.add_argument(\"--num_epochs\", type=int, default=50)\n    parser.add_argument(\"--batch_size\", type=int, default=32)\n\n    args = parser.parse_args()\n    run_single_cpu_training(args)\n</code></pre> <p>Current usage: <code>python single_cpu_args.py --learning_rate 0.05 --num_epochs 100</code></p>"},{"location":"jobs/parameters/#direct_pythonjob_integration","title":"Direct PythonJob Integration","text":"<p>No code changes needed - just wrap your existing function:</p> argparse_job.py<pre><code>from my_module import run_single_cpu_training  # Your existing function!\nfrom runnable import PythonJob\n\ndef main():\n    # Use your argparse function directly - no modifications needed\n    training_job = PythonJob(function=run_single_cpu_training)\n\n    # Runnable automatically converts YAML to argparse.Namespace\n    training_job.execute(parameters_file=\"training_params.yaml\")\n\n    return training_job\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Create parameter file (matches your argparse arguments): training_params.yaml<pre><code>learning_rate: 0.01\nnum_epochs: 50\nbatch_size: 32\n</code></pre></p> <p>New usage: <code>uv run argparse_job.py</code> - same function, zero code changes!</p> See complete working example examples/torch/single_cpu_args.py<pre><code># single_cpu_train_with_args.py\n\nimport argparse  # New: for command-line arguments\nimport time\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\n\ndef run_single_cpu_training(args: argparse.Namespace):\n    \"\"\"\n    Runs a simple training loop on a single CPU core.\n    Accepts parsed arguments for hyperparameters.\n    \"\"\"\n    print(\n        f\"Parameters: learning_rate={args.learning_rate}, num_epochs={args.num_epochs}, batch_size={args.batch_size}\"\n    )\n    print(\"--- Starting Single-CPU Training ---\")\n    print(f\"Learning Rate: {args.learning_rate}, Epochs: {args.num_epochs}\")\n    print(f\"Batch Size: {args. batch_size}\")\n\n    # 1. Define a simple model\n    class SimpleModel(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.linear = nn.Linear(10, 1)  # Input 10 features, output 1\n\n        def forward(self, x):\n            return self.linear(x)\n\n    model = SimpleModel()\n    device = torch.device(\"cpu\")  # Explicitly set device to CPU\n    model.to(device)\n\n    # 2. Create a dummy dataset\n    num_samples = 1000  # Larger dataset to see the difference in speed later\n    num_features = 10\n    X = torch.randn(num_samples, num_features)\n    y = (\n        torch.sum(X * torch.arange(1, num_features + 1).float(), dim=1, keepdim=True)\n        + torch.randn(num_samples, 1) * 0.1\n    )\n\n    dataset = TensorDataset(X, y)\n    dataloader = DataLoader(\n        dataset, batch_size=args.batch_size, shuffle=True\n    )  # Use batch_size parameter\n\n    # 3. Define optimizer and loss function\n    optimizer = optim.SGD(\n        model.parameters(), lr=args.learning_rate\n    )  # Use learning_rate parameter\n    criterion = nn.MSELoss()\n\n    start_time = time.time()\n\n    # 4. Training loop\n    for epoch in range(args.num_epochs):  # Use num_epochs parameter\n        model.train()\n        total_loss = 0\n        for batch_idx, (inputs, targets) in enumerate(dataloader):\n            inputs, targets = inputs.to(device), targets.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n\n        avg_loss = total_loss / len(dataloader)\n        print(f\"Epoch {epoch+1}/{args.num_epochs}, Loss: {avg_loss:.4f}\")\n\n    end_time = time.time()\n    print(f\"\\nSingle-CPU Training complete in {end_time - start_time:.2f} seconds!\")\n\n    # Save the model\n    model_save_path = (\n        f\"single_cpu_model_lr{args.learning_rate}_epochs{args.num_epochs}.pth\"\n    )\n    torch.save(model.state_dict(), model_save_path)\n    print(f\"Model saved to {model_save_path}\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Single-CPU PyTorch Training Example.\")\n    parser.add_argument(\n        \"--learning_rate\",\n        type=float,\n        default=0.01,\n        help=\"Learning rate for the optimizer (default: 0.01)\",\n    )\n    parser.add_argument(\n        \"--num_epochs\",\n        type=int,\n        default=50,\n        help=\"Number of training epochs (default: 50)\",\n    )\n    parser.add_argument(\n        \"--batch_size\",\n        type=int,\n        default=32,\n        help=\"Batch size for training (default: 32)\",\n    )\n\n    args = parser.parse_args()\n    run_single_cpu_training(args)\n</code></pre> <p>Magic conversion</p> <p>Runnable automatically creates an <code>argparse.Namespace</code> object from your YAML parameters. Your function receives exactly what it expects - no code changes required!</p>"},{"location":"jobs/parameters/#migration_benefits","title":"Migration Benefits","text":"<p>\ud83d\udd04 Replace command-line complexity: <pre><code># Before: Long command lines\npython script.py --learning_rate 0.05 --num_epochs 100 --batch_size 64\n\n# After: Clean execution\nuv run training_job.py\n</code></pre></p> <p>Keep both versions</p> <p>Your original argparse script continues working unchanged. The PythonJob version gives you additional capabilities without any migration risk!</p>"},{"location":"jobs/parameters/#best_practices","title":"Best Practices","text":""},{"location":"jobs/parameters/#use_the_three-layer_system","title":"\u2705 Use the Three-Layer System","text":"<p>Combine all parameter methods for maximum flexibility:</p> <pre><code># 1. Base config in code\njob.execute(parameters_file=\"base_config.yaml\")\n\n# 2. Environment-specific file\nexport RUNNABLE_PARAMETERS_FILE=\"prod_overrides.yaml\"\n\n# 3. Individual runtime tweaks\nexport RUNNABLE_PRM_debug=true\n</code></pre>"},{"location":"jobs/parameters/#environment_variables_for_deployment_values","title":"\u2705 Environment Variables for Deployment Values","text":"<p>Use env vars for values that differ between environments:</p> <pre><code># Production deployment\nexport RUNNABLE_PRM_database_url=\"postgresql://prod:5432/app\"\nexport RUNNABLE_PRM_api_key=\"prod-key-123\"\nexport RUNNABLE_PRM_debug=false\n</code></pre>"},{"location":"jobs/parameters/#yaml_for_complex_configuration","title":"\u2705 YAML for Complex Configuration","text":"<p>Keep structured config in parameter files:</p> <pre><code># Complex nested configuration\nmodel_settings:\n  learning_rate: 0.01\n  layers: [128, 64, 32]\n  dropout: 0.2\n\ndata_pipeline:\n  source: \"s3://bucket/data/\"\n  transformations: [\"normalize\", \"encode_categoricals\"]\n  validation_split: 0.2\n</code></pre>"},{"location":"jobs/parameters/#whats_next","title":"What's Next?","text":"<p>Your Jobs are now fully configurable! Next topics:</p> <ul> <li>File Storage - Store files created during execution</li> <li>Job Types - Shell and Notebook Jobs</li> </ul> <p>Ready to store files from your Jobs? Continue to File Storage!</p>"},{"location":"jobs/working-with-data/","title":"Working with Data \ud83d\udce6","text":"<p>Learn how to store and return data from your Jobs.</p>"},{"location":"jobs/working-with-data/#returning_data_from_functions","title":"Returning Data from Functions","text":"<p>When your function returns data, specify what should be stored using the <code>returns</code> parameter:</p> <pre><code>from examples.common.functions import write_parameter\nfrom runnable import PythonJob, metric, pickled\n\ndef main():\n    job = PythonJob(\n        function=write_parameter,\n        returns=[\n            pickled(\"df\"),        # pandas DataFrame (complex object)\n            \"integer\",            # JSON-serializable integer\n            \"floater\",            # JSON-serializable float\n            \"stringer\",           # JSON-serializable string\n            \"pydantic_param\",     # Pydantic model (auto-handled)\n            metric(\"score\"),      # Metric for tracking\n        ],\n    )\n\n    job.execute()\n    return job\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> See complete runnable code examples/11-jobs/passing_parameters_python.py<pre><code>\"\"\"\nThe below example shows how to set/get parameters in python\ntasks of the pipeline.\n\nThe function, set_parameter, returns\n    - JSON serializable types\n    - pydantic models\n    - pandas dataframe, any \"object\" type\n\npydantic models are implicitly handled by runnable\nbut \"object\" types should be marked as \"pickled\".\n\nUse pickled even for python data types is advised for\nreasonably large collections.\n\nRun the below example as:\n    python examples/03-parameters/passing_parameters_python.py\n\n\"\"\"\n\nfrom examples.common.functions import write_parameter\nfrom runnable import PythonJob, metric, pickled\n\n\ndef main():\n    job = PythonJob(\n        function=write_parameter,\n        returns=[\n            pickled(\"df\"),\n            \"integer\",\n            \"floater\",\n            \"stringer\",\n            \"pydantic_param\",\n            metric(\"score\"),\n        ],\n    )\n\n    job.execute()\n\n    return job\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Try it now: <pre><code>uv run examples/11-jobs/passing_parameters_python.py\n</code></pre></p>"},{"location":"jobs/working-with-data/#what_gets_stored","title":"What Gets Stored","text":"<pre><code>{\n    \"Output parameters\": [\n        (\"df\", \"Pickled object stored in catalog as: df\"),\n        (\"integer\", 1),\n        (\"floater\", 3.14),\n        (\"stringer\", \"hello\"),\n        (\"pydantic_param\", {\"x\": 10, \"foo\": \"bar\"}),\n        (\"score\", 0.9)\n    ],\n    \"Metrics\": [(\"score\", 0.9)],\n    \"status\": \"SUCCESS\"\n}\n</code></pre>"},{"location":"jobs/working-with-data/#return_type_guide","title":"Return Type Guide","text":"Type Usage Storage Location Example <code>pickled(\"name\")</code> Complex objects (DataFrames, models) <code>.catalog/{run-id}/name.dill</code> <code>pickled(\"model\")</code> <code>\"name\"</code> JSON-serializable (int, float, str, dict) Job summary <code>\"count\"</code> <code>metric(\"name\")</code> Trackable metrics Metrics section + summary <code>metric(\"accuracy\")</code> Pydantic models Auto-handled objects Job summary as JSON <code>\"user_profile\"</code>"},{"location":"jobs/working-with-data/#practical_examples","title":"Practical Examples","text":""},{"location":"jobs/working-with-data/#data_analysis_job","title":"Data Analysis Job","text":"<pre><code>def analyze_sales():\n    # Your analysis logic here\n    summary = {\"total_sales\": 50000, \"growth\": 0.15}\n    return summary\n\njob = PythonJob(\n    function=analyze_sales,\n    returns=[\"summary\"]\n)\n</code></pre>"},{"location":"jobs/working-with-data/#model_training_job","title":"Model Training Job","text":"<pre><code>def train_model():\n    # Training logic here\n    model = create_model()\n    accuracy = 0.95\n    return model, accuracy\n\njob = PythonJob(\n    function=train_model,\n    returns=[pickled(\"model\"), metric(\"accuracy\")]\n)\n</code></pre>"},{"location":"jobs/working-with-data/#report_generation_job","title":"Report Generation Job","text":"<pre><code>def generate_report():\n    # Report logic here\n    report_path = \"monthly_report.pdf\"\n    metrics = {\"pages\": 12, \"charts\": 5}\n    return report_path, metrics\n\njob = PythonJob(\n    function=generate_report,\n    returns=[\"report_path\", \"metrics\"]\n)\n</code></pre>"},{"location":"jobs/working-with-data/#viewing_stored_data","title":"Viewing Stored Data","text":"<p>After execution, check what was stored:</p> <pre><code># List catalog contents\nls .catalog/{run-id}/\n\n# View pickled objects (requires Python)\n# Complex objects are in .dill files\n\n# Simple values appear in job summary\n# Check terminal output for JSON summary\n</code></pre>"},{"location":"jobs/working-with-data/#best_practices","title":"Best Practices","text":""},{"location":"jobs/working-with-data/#always_specify_returns","title":"\u2705 Always Specify Returns","text":"<pre><code># Good - explicit about what to keep\njob = PythonJob(\n    function=my_function,\n    returns=[\"result\", metric(\"score\")]\n)\n</code></pre>"},{"location":"jobs/working-with-data/#dont_forget_returns","title":"\u274c Don't Forget Returns","text":"<pre><code># Bad - function output will be lost\njob = PythonJob(function=my_function)  # No returns specified!\n</code></pre>"},{"location":"jobs/working-with-data/#use_appropriate_types","title":"\u2705 Use Appropriate Types","text":"<pre><code>returns=[\n    pickled(\"large_dataframe\"),  # For complex objects\n    \"simple_count\",              # For basic values\n    metric(\"accuracy\"),          # For trackable metrics\n]\n</code></pre>"},{"location":"jobs/working-with-data/#whats_next","title":"What's Next?","text":"<p>You can now store Job outputs! Next steps:</p> <ul> <li>Parameters &amp; Environment - Configure Jobs dynamically</li> <li>File Storage - Store files created during execution</li> <li>Job Types - Shell and Notebook Jobs</li> </ul> <p>Ready to make your Jobs configurable? Continue to Parameters &amp; Environment!</p>"},{"location":"pipelines/connecting-functions/","title":"\ud83d\udd17 Connecting Functions","text":"<p>The magic happens when you chain functions together. Runnable makes this effortless.</p>"},{"location":"pipelines/connecting-functions/#what_you_already_know","title":"What you already know","text":"<p>You probably chain functions like this:</p> <pre><code>def write_parameter():\n    df = pd.DataFrame({\"x\": [1, 2, 3]})\n    return df, 10, 3.14, \"hello\", SamplePydanticModel(x=10, foo=\"bar\"), 0.95\n\ndef read_parameter(df, integer, floater, stringer, pydantic_param, score):\n    print(f\"Received: df={len(df)} rows, integer={integer}, score={score}\")\n    return df.mean()\n\n# Manual chaining\ndf, integer, floater, stringer, pydantic_param, score = write_parameter()\nresult = read_parameter(df, integer, floater, stringer, pydantic_param, score)\n</code></pre>"},{"location":"pipelines/connecting-functions/#runnable_does_the_chaining_for_you","title":"Runnable does the chaining for you","text":"<p>Same functions, automatic parameter passing:</p> <pre><code>from runnable import Pipeline, PythonTask, pickled, metric\n\ndef main():\n    # Step 1: Create data with named outputs\n    step1 = PythonTask(\n        function=write_parameter,\n        returns=[pickled(\"df\"), \"integer\", \"floater\", \"stringer\", \"pydantic_param\", metric(\"score\")]\n    )\n\n    # Step 2: Process data - parameters matched automatically!\n    step2 = PythonTask(function=read_parameter)\n\n    pipeline = Pipeline(steps=[step1, step2])\n    pipeline.execute()\n    return pipeline\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>\u2728 Magic: The <code>df</code> returned by <code>write_parameter</code> automatically becomes the <code>df</code> parameter for <code>read_parameter</code>.</p> See complete runnable code examples/03-parameters/passing_parameters_python.py<pre><code>\"\"\"\nThe below example shows how to set/get parameters in python\ntasks of the pipeline.\n\nThe function, set_parameter, returns\n    - JSON serializable types\n    - pydantic models\n    - pandas dataframe, any \"object\" type\n\npydantic models are implicitly handled by runnable\nbut \"object\" types should be marked as \"pickled\".\n\nUse pickled even for python data types is advised for\nreasonably large collections.\n\nRun the below example as:\n    python examples/03-parameters/passing_parameters_python.py\n\n\"\"\"\n\nfrom examples.common.functions import read_parameter, write_parameter\nfrom runnable import Pipeline, PythonTask, metric, pickled\n\n\ndef main():\n    write_parameters = PythonTask(\n        function=write_parameter,\n        returns=[\n            pickled(\"df\"),\n            \"integer\",\n            \"floater\",\n            \"stringer\",\n            \"pydantic_param\",\n            metric(\"score\"),\n        ],\n        name=\"set_parameter\",\n    )\n\n    read_parameters = PythonTask(\n        function=read_parameter,\n        terminate_with_success=True,\n        name=\"get_parameters\",\n    )\n\n    pipeline = Pipeline(\n        steps=[write_parameters, read_parameters],\n    )\n\n    _ = pipeline.execute()\n\n    return pipeline\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Try it now: <pre><code>uv run examples/03-parameters/passing_parameters_python.py\n</code></pre></p>"},{"location":"pipelines/connecting-functions/#how_it_works","title":"How it works","text":"<ol> <li>Step 1 returns values with names: <code>returns=[\"df\", \"score\"]</code></li> <li>Step 2 function signature: <code>def analyze(df, score):</code></li> <li>Runnable matches return names to parameter names automatically</li> </ol>"},{"location":"pipelines/connecting-functions/#mix_different_task_types","title":"Mix different task types","text":"<p>Python functions, notebooks, and shell scripts all work together:</p> <pre><code>from runnable import Pipeline, PythonTask, NotebookTask, ShellTask\n\ndef main():\n    pipeline = Pipeline(steps=[\n        PythonTask(function=create_data, returns=[pickled(\"df\")]),\n        NotebookTask(notebook_path=\"process.ipynb\", returns=[\"processed_df\"]),\n        ShellTask(command=\"./analyze.sh\", returns=[\"report_path\"]),\n        PythonTask(function=send_email)  # Gets report_path automatically\n    ])\n    pipeline.execute()\n    return pipeline\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> See complete runnable code examples/02-sequential/traversal.py<pre><code>\"\"\"\nYou can execute this pipeline by:\n\n    python examples/02-sequential/traversal.py\n\nA pipeline can have any \"tasks\" as part of it. In the\nbelow example, we have a mix of stub, python, shell and notebook tasks.\n\nAs with simpler tasks, the stdout and stderr of each task are captured\nand stored in the catalog.\n\"\"\"\n\nfrom examples.common.functions import hello\nfrom runnable import NotebookTask, Pipeline, PythonTask, ShellTask, Stub\n\n\ndef main():\n    stub_task = Stub(name=\"hello stub\")  # [concept:stub-task]\n\n    python_task = PythonTask(  # [concept:python-task]\n        name=\"hello python\", function=hello, overrides={\"argo\": \"smaller\"}\n    )\n\n    shell_task = ShellTask(  # [concept:shell-task]\n        name=\"hello shell\",\n        command=\"echo 'Hello World!'\",\n    )\n\n    notebook_task = NotebookTask(  # [concept:notebook-task]\n        name=\"hello notebook\",\n        notebook=\"examples/common/simple_notebook.ipynb\",\n    )\n\n    # The pipeline has a mix of tasks.\n    # The order of execution follows the order of the tasks in the list.\n    pipeline = Pipeline(  # [concept:pipeline]\n        steps=[  # (2)\n            stub_task,  # (1)\n            python_task,\n            shell_task,\n            notebook_task,\n        ]\n    )\n\n    pipeline.execute()  # [concept:execution]\n\n    return pipeline\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Try it now: <pre><code>uv run examples/02-sequential/traversal.py\n</code></pre></p> <p>Parameter matching</p> <p>Return names must match parameter names. <code>returns=[\"data\"]</code> \u2192 <code>def process(data):</code></p> <p>Parameter Type Compatibility</p> <p>Parameter passing works between task types, but with important constraints based on data types and how each task type receives parameters:</p> <p>How Parameters Are Passed:</p> Task Type How Parameters Are Received Input Parameters Output Parameters Python Function arguments All types (primitive, pickled, pydantic, metric) All types (primitive, pickled, pydantic, metric) Notebook Tagged parameter cells (variables replaced) Python primitives only (int, str, float, list, dict) All types (primitive, pickled, pydantic, metric) Shell Environment variables Python primitives only (int, str, float, list, dict) Python primitives only (int, str, float, list, dict) <p>Notebook Parameter Mechanism:</p> <p>Notebooks receive parameters through tagged cells where variable values are replaced:</p> <pre><code># In your notebook's first cell (tagged as \"parameters\"):\ncount = None      # This will be replaced with actual value\nstatus = None     # This will be replaced with actual value\n</code></pre> <p>\u2705 This works: <pre><code>def main():\n    Pipeline(steps=[\n        PythonTask(function=extract_data, returns=[\"count\", \"status\"]),    # primitives \u2192\n        NotebookTask(notebook=\"clean.ipynb\", returns=[\"df\"]),              # \u2192 notebook receives via parameter cells\n        PythonTask(function=analyze, returns=[\"report\"])                   # \u2192 python can receive pickled df\n    ]).execute()\n\nif __name__ == \"__main__\":\n    main()\n</code></pre></p> <p>\u274c This won't work: <pre><code>def main():\n    Pipeline(steps=[\n        PythonTask(function=create_model, returns=[pickled(\"model\")]),     # pickled object \u2192\n        NotebookTask(notebook=\"use_model.ipynb\")                           # \u2192 notebook can't receive pickled objects!\n    ]).execute()\n\nif __name__ == \"__main__\":\n    main()\n</code></pre></p> <p>Next: Understand when to use jobs vs pipelines.</p>"},{"location":"pipelines/file-management/","title":"\ud83d\udcc1 File Management Made Easy","text":"<p>Tired of managing temporary files between tasks? Runnable's catalog system handles it automatically and gives you complete execution traceability.</p>"},{"location":"pipelines/file-management/#the_old_way_manual_file_management","title":"The old way (manual file management)","text":"<pre><code>def create_report():\n    df = analyze_data()\n    df.to_csv(\"temp_results.csv\")  # Hope this exists later...\n\ndef send_report():\n    df = pd.read_csv(\"temp_results.csv\")  # Hope this file is there...\n    # What if the path changed? What if step 1 failed?\n</code></pre>"},{"location":"pipelines/file-management/#the_runnable_way_automatic","title":"The Runnable way (automatic)","text":"<p>Step 1: Create and store files</p> <pre><code>from runnable import Catalog, PythonTask\n\ndef write_files():\n    # Create your files\n    df.to_csv(\"df.csv\")\n    with open(\"data_folder/data.txt\", \"w\") as f:\n        f.write(\"Important data\")\n\n# Store files automatically\ntask1 = PythonTask(\n    function=write_files,\n    catalog=Catalog(put=[\"df.csv\", \"data_folder/data.txt\"])\n)\n</code></pre> <p>Step 2: Retrieve and use files</p> <pre><code>def read_files():\n    # Files are automatically available here!\n    df = pd.read_csv(\"df.csv\")  # \u2705 File is there\n    with open(\"data_folder/data.txt\") as f:\n        data = f.read()  # \u2705 File is there\n\n# Get files automatically\ntask2 = PythonTask(\n    function=read_files,\n    catalog=Catalog(get=[\"df.csv\", \"data_folder/data.txt\"])\n)\n</code></pre>"},{"location":"pipelines/file-management/#how_it_works","title":"How it works","text":"<ol> <li><code>put=[\"file.csv\"]</code> \u2192 Runnable stores the file safely</li> <li><code>get=[\"file.csv\"]</code> \u2192 Runnable makes the file available in the next task</li> <li>No path management \u2192 Files appear where your code expects them</li> </ol>"},{"location":"pipelines/file-management/#full_workflow_example","title":"Full workflow example","text":"<pre><code>from runnable import Pipeline, PythonTask, Catalog\n\ndef main():\n    # Complete workflow with automatic file management\n    pipeline = Pipeline(steps=[\n        PythonTask(\n            function=write_files,\n            catalog=Catalog(put=[\"df.csv\", \"data_folder/data.txt\"]),\n            name=\"create_files\"\n        ),\n        PythonTask(\n            function=read_files,\n            catalog=Catalog(get=[\"df.csv\", \"data_folder/data.txt\"]),\n            name=\"process_files\"\n        )\n    ])\n    pipeline.execute()\n    return pipeline\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> See complete runnable code examples/04-catalog/catalog.py<pre><code>\"\"\"\nDemonstrates moving files within tasks.\n\n- generate_data: creates df.csv and data_folder/data.txt\n\n- delete_local_after_generate: deletes df.csv and data_folder/data.txt\n    This step ensures that the local files are deleted after the step\n\n- read_data_py: reads df.csv and data_folder/data.txt\n\n- delete_local_after_python_get: deletes df.csv and data_folder/data.txt\n    This step ensures that the local files are deleted after the step\n\n- read_data_shell: reads df.csv and data_folder/data.txt\n\n- delete_local_after_shell_get: deletes df.csv and data_folder/data.txt\n    This step ensures that the local files are deleted after the step\n\n- read_data_notebook: reads df.csv and data_folder/data.txt\n\n- delete_local_after_notebook_get: deletes df.csv and data_folder/data.txt\n\nUse this pattern to move files that are not dill friendly.\n\nAll the files are stored in catalog.\n\nRun this pipeline as:\n    python examples/04-catalog/catalog.py\n\nYou can execute this pipeline by:\n\n    python examples/04-catalog/catalog.py\n\"\"\"\n\nfrom examples.common.functions import read_files, write_files\nfrom runnable import Catalog, NotebookTask, Pipeline, PythonTask, ShellTask\n\n\ndef main():\n    write_catalog = Catalog(put=[\"df.csv\", \"data_folder/data.txt\"])\n    generate_data = PythonTask(\n        name=\"generate_data\",\n        function=write_files,\n        catalog=write_catalog,\n    )\n\n    delete_files_command = \"\"\"\n        rm df.csv || true &amp;&amp; \\\n        rm data_folder/data.txt || true\n    \"\"\"\n    # delete from local files after generate\n    # since its local catalog, we delete to show \"get from catalog\"\n    delete_local_after_generate = ShellTask(\n        name=\"delete_after_generate\",\n        command=delete_files_command,\n    )\n\n    read_catalog = Catalog(get=[\"df.csv\", \"data_folder/data.txt\"])\n    read_data_python = PythonTask(\n        name=\"read_data_py\",\n        function=read_files,\n        catalog=read_catalog,\n    )\n\n    delete_local_after_python_get = ShellTask(\n        name=\"delete_after_generate_python\",\n        command=delete_files_command,\n    )\n\n    read_data_shell_command = \"\"\"\n    (ls df.csv &gt;&gt; /dev/null 2&gt;&amp;1 &amp;&amp; echo yes) || exit 1 &amp;&amp; \\\n    (ls data_folder/data.txt &gt;&gt; /dev/null 2&gt;&amp;1 &amp;&amp; echo yes) || exit 1\n    \"\"\"\n    read_data_shell = ShellTask(\n        name=\"read_data_shell\",\n        command=read_data_shell_command,\n        catalog=read_catalog,\n    )\n\n    delete_local_after_shell_get = ShellTask(\n        name=\"delete_after_generate_shell\",\n        command=delete_files_command,\n    )\n\n    read_data_notebook = NotebookTask(\n        notebook=\"examples/common/read_files.ipynb\",\n        name=\"read_data_notebook\",\n        catalog=read_catalog,\n    )\n\n    delete_local_after_notebook_get = ShellTask(\n        name=\"delete_after_generate_notebook\",\n        command=delete_files_command,\n        terminate_with_success=True,\n    )\n\n    pipeline = Pipeline(\n        steps=[\n            generate_data,\n            delete_local_after_generate,\n            read_data_python,\n            delete_local_after_python_get,\n            read_data_shell,\n            delete_local_after_shell_get,\n            read_data_notebook,\n            delete_local_after_notebook_get,\n        ]\n    )\n    _ = pipeline.execute()\n\n    return pipeline\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Try it now: <pre><code>uv run examples/04-catalog/catalog.py\n</code></pre></p>"},{"location":"pipelines/file-management/#multiple_files_and_folders","title":"Multiple files and folders","text":"<pre><code># Store multiple files explicitly\ncatalog=Catalog(put=[\"results.csv\", \"plots/\", \"model.pkl\"])\n\n# Retrieve what you need\ncatalog=Catalog(get=[\"results.csv\", \"model.pkl\"])\n</code></pre>"},{"location":"pipelines/file-management/#no-copy_mode_for_large_files","title":"No-Copy Mode for Large Files \ud83d\ude80","text":"<p>For large files or datasets, copying can be expensive and unnecessary. Use <code>store_copy=False</code> to track files without copying them:</p> <pre><code># Large dataset processing - track but don't copy\ntask1 = PythonTask(\n    function=process_large_dataset,\n    catalog=Catalog(put=[\"large_dataset.parquet\", \"model.pkl\"], store_copy=False)\n)\n\n# Next task can still access the files\ntask2 = PythonTask(\n    function=analyze_results,\n    catalog=Catalog(get=[\"large_dataset.parquet\"])\n)\n</code></pre> <p>What happens with <code>store_copy=False</code>:</p> <ul> <li>\u2705 MD5 hash captured for integrity verification</li> <li>\u2705 Files remain in original location</li> <li>\u2705 No disk space duplication for large files</li> <li>\u2705 Faster execution - no time spent copying</li> <li>\u2705 Still tracked in pipeline execution history</li> </ul> <p>When to use no-copy mode:</p> <ul> <li>Large datasets (GB+ files) where copying is slow and expensive</li> <li>Reference data that doesn't change and is already stored safely</li> <li>Network storage where files are already backed up</li> <li>Performance-critical pipelines where copy time matters</li> </ul> <p>Example with mixed copy strategies:</p> <pre><code>pipeline = Pipeline(steps=[\n    PythonTask(\n        function=prepare_data,\n        catalog=Catalog(put=[\n            \"config.json\",           # Small file - copy it\n            \"large_input.parquet\"    # Large file - hash only\n        ], store_copy=False),        # Applies to all files\n        name=\"prepare\"\n    ),\n    PythonTask(\n        function=process_data,\n        catalog=Catalog(get=[\"config.json\", \"large_input.parquet\"]),\n        name=\"process\"\n    )\n])\n</code></pre>"},{"location":"pipelines/file-management/#glob-style_wildcards","title":"Glob-style wildcards","text":"<p>Use wildcards to match multiple files automatically:</p> <pre><code># Store all CSV files\ncatalog=Catalog(put=[\"*.csv\"])\n\n# Store all files in data folder\ncatalog=Catalog(put=[\"data/*\"])\n\n# Store all Python files recursively\ncatalog=Catalog(put=[\"**/*.py\"])\n\n# Store all files with specific pattern\ncatalog=Catalog(put=[\"results_*.json\", \"plots/*.png\"])\n</code></pre> <p>Common wildcard patterns:</p> Pattern Matches <code>*.csv</code> All CSV files in current directory <code>data/*</code> All files in the data folder <code>**/*.py</code> All Python files in current and subdirectories <code>results_*.json</code> Files like <code>results_train.json</code>, <code>results_test.json</code> <code>plots/*.png</code> All PNG files in the plots folder <p>Example with wildcards:</p> <pre><code>def main():\n    pipeline = Pipeline(steps=[\n        PythonTask(\n            function=create_multiple_outputs,\n            catalog=Catalog(put=[\"*.csv\", \"plots/*.png\"]),  # Store all CSVs and plot PNGs\n            name=\"generate_data\"\n        ),\n        PythonTask(\n            function=process_outputs,\n            catalog=Catalog(get=[\"data_*.csv\", \"plots/summary.png\"]),  # Get specific files\n            name=\"process_data\"\n        )\n    ])\n    pipeline.execute()\n    return pipeline\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"pipelines/file-management/#why_this_matters","title":"Why this matters","text":"<p>Without catalog:</p> <ul> <li>\u274c Manual path management</li> <li>\u274c Files get lost between environments</li> <li>\u274c Hard to reproduce workflows</li> <li>\u274c Cleanup is manual</li> </ul> <p>With catalog:</p> <ul> <li>\u2705 Automatic file management</li> <li>\u2705 Works across different environments</li> <li>\u2705 Perfect reproducibility</li> <li>\u2705 Automatic cleanup</li> </ul>"},{"location":"pipelines/file-management/#automatic_execution_traceability","title":"Automatic execution traceability","text":"<p>Runnable automatically captures all execution outputs in the <code>catalog</code> for complete traceability:</p>"},{"location":"pipelines/file-management/#what_gets_captured","title":"What gets captured","text":"<p>For every task execution, Runnable stores:</p> <ol> <li>Execution logs - Complete stdout/stderr output from your tasks</li> <li>Output notebooks - Executed notebooks with all outputs and results (for NotebookTask)</li> <li>Environment information - Environment variables and execution context</li> <li>Timestamps - Precise execution timing information</li> </ol>"},{"location":"pipelines/file-management/#where_to_find_execution_outputs","title":"Where to find execution outputs","text":"<pre><code>.catalog/\n\u2514\u2500\u2500 {run-id}/                          # Unique run identifier\n    \u251c\u2500\u2500 taskname123.execution.log       # Task stdout/stderr output\n    \u251c\u2500\u2500 output_notebook.ipynb          # Executed notebook (if NotebookTask)\n    \u2514\u2500\u2500 data_files/                    # Your catalog files\n</code></pre> <p>Example after running a pipeline:</p> <pre><code>.catalog/\n\u2514\u2500\u2500 pleasant-nobel-2303/\n    \u251c\u2500\u2500 hello_task.execution.log        # \"Hello World!\" output captured\n    \u251c\u2500\u2500 data_processing.execution.log   # All Python print statements\n    \u251c\u2500\u2500 analysis_notebook_out.ipynb     # Executed notebook with results\n    \u2514\u2500\u2500 results.csv                     # Your data files\n</code></pre>"},{"location":"pipelines/file-management/#complete_execution_visibility","title":"Complete execution visibility","text":"<p>Python tasks: Capture all <code>print()</code>, <code>logging</code>, warnings, and errors: <pre><code>$ cat .catalog/run-id/python_task.execution.log\n[23:02:46] Parameters available for the execution:\n           {'input_file': 'data.csv'}\nProcessing 1000 rows...\nModel accuracy: 94.2%\nWARNING: Low confidence predictions detected\n</code></pre></p> <p>Notebook tasks: Store executed notebooks with all outputs:</p> <ul> <li>Input notebook: <code>analysis.ipynb</code></li> <li>Output notebook: <code>.catalog/run-id/analysis_out.ipynb</code> (with all cell outputs)</li> <li>Execution log: <code>.catalog/run-id/notebook_task.execution.log</code></li> </ul> <p>Shell tasks: Capture all command output and environment: <pre><code>$ cat .catalog/run-id/shell_task.execution.log\nInstalling dependencies...\nRunning analysis script...\n=== COLLECT ===\nRUNNABLE_RUN_ID=run-id\nPWD=/path/to/project\nResults saved to output.txt\n</code></pre></p>"},{"location":"pipelines/file-management/#why_this_matters_for_debugging","title":"Why this matters for debugging","text":"<p>No more digging through logs! Everything is organized by run ID:</p> <pre><code>def main():\n    pipeline = Pipeline(steps=[\n        PythonTask(function=data_processing, name=\"process\"),\n        NotebookTask(notebook=\"analysis.ipynb\", name=\"analyze\"),\n        ShellTask(command=\"./deploy.sh\", name=\"deploy\")\n    ])\n    result = pipeline.execute()\n\n    # Check .catalog/{run_id}/ for complete execution trace:\n    # - process123.execution.log (Python output)\n    # - analyze456_out.ipynb (executed notebook)\n    # - deploy789.execution.log (shell output)\n\n    return pipeline\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Best practices</p> <ul> <li>Use catalog for files that flow between tasks. Keep truly temporary files local.</li> <li>Use wildcards (<code>*.csv</code>, <code>data/*</code>) to automatically capture multiple files without manual listing.</li> <li>Be specific with wildcards to avoid capturing unwanted files (<code>results_*.csv</code> vs <code>*.csv</code>).</li> <li>Use <code>store_copy=False</code> for large files to save disk space and improve performance.</li> <li>Check <code>.catalog/{run-id}/</code> for complete execution traceability - no need to dig through environment-specific logs!</li> </ul> <p>Next: See how the same code can run anywhere with different configurations.</p>"},{"location":"pipelines/handling-data/","title":"\ud83d\udcca Adding Your Data","text":"<p>Your functions probably create and return data. Runnable handles this automatically.</p>"},{"location":"pipelines/handling-data/#start_with_your_return_values","title":"Start with your return values","text":"<p>Here's a function that creates some data:</p> <pre><code>def write_parameter():\n    \"\"\"A function that returns multiple parameters\"\"\"\n    df = pd.DataFrame({\"x\": [1, 2, 3]})\n    return df, 10, 3.14, \"hello\", SamplePydanticModel(x=10, foo=\"bar\"), 0.95\n</code></pre>"},{"location":"pipelines/handling-data/#job_mode_-_returns_work_automatically","title":"Job mode - Returns work automatically","text":"<p>In job mode, you can name your return values for better tracking and storage:</p> <pre><code>from runnable import PythonJob, pickled, metric\n\ndef main():\n    job = PythonJob(\n        function=write_parameter,\n        returns=[\n            pickled(\"df\"),        # For pandas DataFrames\n            \"integer\",            # Simple types work as-is\n            \"floater\",\n            \"stringer\",\n            \"pydantic_param\",     # Pydantic models handled automatically\n            metric(\"score\")       # Mark metrics for monitoring\n        ]\n    )\n    job.execute()\n    return job\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> See complete runnable code examples/11-jobs/passing_parameters_python.py<pre><code>\"\"\"\nThe below example shows how to set/get parameters in python\ntasks of the pipeline.\n\nThe function, set_parameter, returns\n    - JSON serializable types\n    - pydantic models\n    - pandas dataframe, any \"object\" type\n\npydantic models are implicitly handled by runnable\nbut \"object\" types should be marked as \"pickled\".\n\nUse pickled even for python data types is advised for\nreasonably large collections.\n\nRun the below example as:\n    python examples/03-parameters/passing_parameters_python.py\n\n\"\"\"\n\nfrom examples.common.functions import write_parameter\nfrom runnable import PythonJob, metric, pickled\n\n\ndef main():\n    job = PythonJob(\n        function=write_parameter,\n        returns=[\n            pickled(\"df\"),\n            \"integer\",\n            \"floater\",\n            \"stringer\",\n            \"pydantic_param\",\n            metric(\"score\"),\n        ],\n    )\n\n    job.execute()\n\n    return job\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Try it now: <pre><code>uv run examples/11-jobs/passing_parameters_python.py\n</code></pre></p>"},{"location":"pipelines/handling-data/#pipeline_mode_-_name_your_outputs","title":"Pipeline mode - Name your outputs","text":"<p>When your function is part of a workflow, name the outputs so other tasks can use them:</p> <pre><code>from runnable import PythonTask, pickled, metric\n\ntask = PythonTask(\n    function=write_parameter,\n    returns=[\n        pickled(\"df\"),        # For pandas DataFrames\n        \"integer\",            # Simple types work as-is\n        \"floater\",\n        \"stringer\",\n        \"pydantic_param\",     # Pydantic models handled automatically\n        metric(\"score\")       # Mark metrics for monitoring\n    ]\n)\n</code></pre> <p>Now downstream tasks can access <code>df</code>, <code>integer</code>, <code>floater</code>, etc.</p> See complete runnable code examples/03-parameters/passing_parameters_python.py<pre><code>\"\"\"\nThe below example shows how to set/get parameters in python\ntasks of the pipeline.\n\nThe function, set_parameter, returns\n    - JSON serializable types\n    - pydantic models\n    - pandas dataframe, any \"object\" type\n\npydantic models are implicitly handled by runnable\nbut \"object\" types should be marked as \"pickled\".\n\nUse pickled even for python data types is advised for\nreasonably large collections.\n\nRun the below example as:\n    python examples/03-parameters/passing_parameters_python.py\n\n\"\"\"\n\nfrom examples.common.functions import read_parameter, write_parameter\nfrom runnable import Pipeline, PythonTask, metric, pickled\n\n\ndef main():\n    write_parameters = PythonTask(\n        function=write_parameter,\n        returns=[\n            pickled(\"df\"),\n            \"integer\",\n            \"floater\",\n            \"stringer\",\n            \"pydantic_param\",\n            metric(\"score\"),\n        ],\n        name=\"set_parameter\",\n    )\n\n    read_parameters = PythonTask(\n        function=read_parameter,\n        terminate_with_success=True,\n        name=\"get_parameters\",\n    )\n\n    pipeline = Pipeline(\n        steps=[write_parameters, read_parameters],\n    )\n\n    _ = pipeline.execute()\n\n    return pipeline\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Try it now: <pre><code>uv run examples/03-parameters/passing_parameters_python.py\n</code></pre></p>"},{"location":"pipelines/handling-data/#handle_different_data_types","title":"Handle different data types","text":""},{"location":"pipelines/handling-data/#large_or_complex_objects","title":"\ud83d\udce6 Large or complex objects","text":"<p>Use <code>pickled()</code> for pandas DataFrames, models, or large collections:</p> <pre><code>returns=[pickled(\"df\"), \"score\"]\n</code></pre>"},{"location":"pipelines/handling-data/#track_metrics","title":"\ud83d\udcc8 Track metrics","text":"<p>Use <code>metric()</code> for numbers you want to monitor:</p> <pre><code>returns=[metric(\"accuracy\"), metric(\"loss\")]\n</code></pre>"},{"location":"pipelines/handling-data/#everything_else","title":"\ud83d\udccb Everything else","text":"<p>Simple types (strings, numbers, lists) work as-is:</p> <pre><code>returns=[\"count\", \"status\", \"results\"]\n</code></pre> <p>Pro tip</p> <p>Name your returns clearly. <code>[\"df\", \"score\"]</code> is better than <code>[\"output1\", \"output2\"]</code>.</p> <p>Parameter types across task types</p> <p>The <code>returns=[]</code> syntax works across all task types, but parameter type support varies:</p> <ul> <li>Python functions (<code>PythonTask</code>, <code>PythonJob</code>): Full support for all parameter types (primitive, pickled, pydantic, metric)</li> <li>Jupyter notebooks (<code>NotebookTask</code>, <code>NotebookJob</code>): Can receive primitives only, can return all types</li> <li>Shell scripts (<code>ShellTask</code>, <code>ShellJob</code>): Limited to primitive types for both input and output</li> </ul> <p>Choose your parameter types based on the task types in your workflow!</p> <p>Next: Learn how to connect functions in workflows.</p>"},{"location":"pipelines/jobs-vs-pipelines/","title":"\ud83c\udfaf Jobs vs Pipelines: When to Use Which?","text":"<p>Both jobs and pipelines run your functions. The difference is intent.</p>"},{"location":"pipelines/jobs-vs-pipelines/#jobs_run_this_once","title":"\ud83c\udfaf Jobs: \"Run this once\"","text":"<p>Perfect for standalone tasks:</p> <pre><code>from runnable import PythonJob\n\ndef analyze_sales_data():\n    # Load data, run analysis, generate report\n    return \"Analysis complete!\"\n\ndef main():\n    # Job: Just run it\n    job = PythonJob(function=analyze_sales_data)\n    job.execute()\n    return job  # REQUIRED: Always return the job object\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> See complete runnable code examples/11-jobs/python_tasks.py<pre><code>\"\"\"\nYou can execute this pipeline by:\n\n    python examples/01-tasks/python_tasks.py\n\nThe stdout of \"Hello World!\" would be captured as execution\nlog and stored in the catalog.\n\nAn example of the catalog structure:\n\n.catalog\n\u2514\u2500\u2500 baked-heyrovsky-0602\n    \u2514\u2500\u2500 hello.execution.log\n\n2 directories, 1 file\n\n\nThe hello.execution.log has the captured stdout of \"Hello World!\".\n\"\"\"\n\nfrom examples.common.functions import hello\nfrom runnable import PythonJob\n\n\ndef main():\n    job = PythonJob(function=hello)\n\n    job.execute()\n\n    return job\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Try it now: <pre><code>uv run examples/11-jobs/python_tasks.py\n</code></pre></p>"},{"location":"pipelines/jobs-vs-pipelines/#when_to_use_jobs","title":"When to use jobs:","text":"<ul> <li>One-off analysis: \"Analyze this dataset\"</li> <li>Testing functions: \"Does my code work?\"</li> <li>Standalone reports: \"Generate monthly summary\"</li> <li>Data exploration: \"What's in this file?\"</li> </ul>"},{"location":"pipelines/jobs-vs-pipelines/#pipelines_this_is_step_x_of_many","title":"\ud83d\udd17 Pipelines: \"This is step X of many\"","text":"<p>Perfect for multi-step workflows:</p> <pre><code>from runnable import Pipeline, PythonTask\n\ndef load_data():\n    return {\"users\": 1000, \"sales\": 50000}\n\ndef clean_data(raw_data):\n    return {\"clean_users\": raw_data[\"users\"], \"clean_sales\": raw_data[\"sales\"]}\n\ndef train_model(cleaned_data):\n    return f\"Model trained on {cleaned_data['clean_users']} users\"\n\ndef main():\n    # Pipeline: Chain them together\n    pipeline = Pipeline(steps=[\n        PythonTask(function=load_data, returns=[\"raw_data\"]),\n        PythonTask(function=clean_data, returns=[\"cleaned_data\"]),\n        PythonTask(function=train_model, returns=[\"model\"])\n    ])\n    pipeline.execute()\n    return pipeline  # REQUIRED: Always return the pipeline object\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> See complete runnable code examples/03-parameters/passing_parameters_python.py<pre><code>\"\"\"\nThe below example shows how to set/get parameters in python\ntasks of the pipeline.\n\nThe function, set_parameter, returns\n    - JSON serializable types\n    - pydantic models\n    - pandas dataframe, any \"object\" type\n\npydantic models are implicitly handled by runnable\nbut \"object\" types should be marked as \"pickled\".\n\nUse pickled even for python data types is advised for\nreasonably large collections.\n\nRun the below example as:\n    python examples/03-parameters/passing_parameters_python.py\n\n\"\"\"\n\nfrom examples.common.functions import read_parameter, write_parameter\nfrom runnable import Pipeline, PythonTask, metric, pickled\n\n\ndef main():\n    write_parameters = PythonTask(\n        function=write_parameter,\n        returns=[\n            pickled(\"df\"),\n            \"integer\",\n            \"floater\",\n            \"stringer\",\n            \"pydantic_param\",\n            metric(\"score\"),\n        ],\n        name=\"set_parameter\",\n    )\n\n    read_parameters = PythonTask(\n        function=read_parameter,\n        terminate_with_success=True,\n        name=\"get_parameters\",\n    )\n\n    pipeline = Pipeline(\n        steps=[write_parameters, read_parameters],\n    )\n\n    _ = pipeline.execute()\n\n    return pipeline\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Try it now: <pre><code>uv run examples/03-parameters/passing_parameters_python.py\n</code></pre></p>"},{"location":"pipelines/jobs-vs-pipelines/#when_to_use_pipelines","title":"When to use pipelines:","text":"<ul> <li>Multi-step workflows: \"Load \u2192 Clean \u2192 Train \u2192 Deploy\"</li> <li>Data pipelines: \"Extract \u2192 Transform \u2192 Load\"</li> <li>Reproducible processes: \"Run the same steps every time\"</li> <li>Complex dependencies: \"Step 3 needs outputs from steps 1 and 2\"</li> </ul>"},{"location":"pipelines/jobs-vs-pipelines/#same_function_different_contexts","title":"\ud83d\udd04 Same function, different contexts","text":"<p>Here's the same function used both ways:</p> <pre><code>def hello():\n    \"The most basic function\"\n    print(\"Hello World!\")\n</code></pre> <p>As a job: <pre><code>from runnable import PythonJob\n\ndef main():\n    job = PythonJob(function=hello)\n    job.execute()\n    return job  # REQUIRED: Always return the job object\n\nif __name__ == \"__main__\":\n    main()\n</code></pre></p> <p>As a pipeline task: <pre><code>from runnable import Pipeline, PythonTask\n\ndef main():\n    task = PythonTask(function=hello, name=\"say_hello\")\n    pipeline = Pipeline(steps=[task])\n    pipeline.execute()\n    return pipeline  # REQUIRED: Always return the pipeline object\n\nif __name__ == \"__main__\":\n    main()\n</code></pre></p>"},{"location":"pipelines/jobs-vs-pipelines/#quick_decision_guide","title":"Quick decision guide","text":"I want to... Use Test my function Job Run analysis once Job Generate a report Job Process data in multiple steps Pipeline Chain different functions Pipeline Run the same workflow repeatedly Pipeline <p>You can always switch</p> <p>Start with a job to test your function, then move it into a pipeline when you're ready to build a workflow.</p> <p>Essential Pattern: Always Return Objects</p> <p>Both jobs and pipelines must be returned from your <code>main()</code> function. This pattern is critical for:</p> <p>\ud83d\udd0d Execution Tracking: Runnable tracks run status, timing, and metadata through the returned object</p> <p>\ud83d\udcca Result Access: The returned object contains execution results, logs, and run IDs</p> <p>\ud83d\udd17 Integration: External tools and monitoring systems need the object for further processing</p> <p>\ud83d\udc1b Debugging: Error details and execution context are accessible via the returned object</p> <p>\u274c Missing returns break functionality: <pre><code>def main():\n    job = PythonJob(function=my_function)\n    job.execute()\n    # Missing return - loses execution tracking!\n\ndef main():\n    pipeline = Pipeline(steps=[...])\n    pipeline.execute()\n    # Missing return - no access to results!\n</code></pre></p> <p>\u2705 Always use this pattern: <pre><code>def main():\n    job = PythonJob(function=my_function)\n    job.execute()\n    return job  # Essential for Runnable's execution model\n\ndef main():\n    pipeline = Pipeline(steps=[...])\n    pipeline.execute()\n    return pipeline  # Required for result access and tracking\n</code></pre></p> <p>Custom Execution Models</p> <p>Need to run jobs beyond Python, Shell, and Notebooks? Create custom task types and executors for any infrastructure or execution model using Runnable's extensible plugin architecture.</p> <p>\u2192 Custom Job Executors \u2192 Custom Pipeline Executors</p>"},{"location":"pipelines/jobs-vs-pipelines/#whats_next","title":"What's Next?","text":"<ul> <li>Pipeline Parameters - Configure pipelines with parameters and custom run IDs</li> <li>Task Types - Different ways to define pipeline steps (Python, notebooks, shell scripts)</li> <li>Visualization - Visualize pipeline execution with interactive timelines</li> </ul>"},{"location":"pipelines/parameters-from-outside/","title":"\u26a1 Parameters From Outside","text":"<p>The coolest part: Change your function's behavior without changing your code.</p>"},{"location":"pipelines/parameters-from-outside/#your_function_accepts_parameters","title":"Your function accepts parameters","text":"<pre><code>def read_initial_params_as_pydantic(\n    integer: int,\n    floater: float,\n    stringer: str,\n    pydantic_param: ComplexParams,\n    envvar: str,\n):\n    # Your function uses these parameters\n    print(f\"Processing {integer} items with {stringer} config\")\n    return f\"Processed {len(pydantic_param)} records\"\n</code></pre>"},{"location":"pipelines/parameters-from-outside/#method_1_yaml_files","title":"Method 1: YAML files","text":"<p>Create a parameters file:</p> parameters.yaml<pre><code>integer: 1\nfloater: 3.14\nstringer: \"hello\"\npydantic_param:\n  x: 10\n  foo: \"bar\"\nenvvar: \"not set\"  # Will be overridden by environment variable\n</code></pre> <p>Run with parameters:</p> <pre><code>from runnable import Pipeline, PythonTask\n\ndef main():\n    task = PythonTask(function=read_initial_params_as_pydantic)\n    pipeline = Pipeline(steps=[task])\n    pipeline.execute(parameters_file=\"parameters.yaml\")\n    return pipeline\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> See complete runnable code examples/03-parameters/static_parameters_python.py<pre><code>\"\"\"\nThe below example showcases setting up known initial parameters for a pipeline\nof only python tasks\n\nThe initial parameters as defined in the yaml file are:\n    simple: 1\n    complex_param:\n        x: 10\n        y: \"hello world!!\"\n\nrunnable allows using pydantic models for deeply nested parameters and\ncasts appropriately based on annotation. eg: read_initial_params_as_pydantic\n\nIf no annotation is provided, the parameter is assumed to be a dictionary.\neg: read_initial_params_as_json\n\nYou can set the initial parameters from environment variables as well.\neg: Any environment variable prefixed by \"RUNNABLE_PRM_\" will be picked up by runnable\n\nRun this pipeline as:\n    python examples/03-parameters/static_parameters_python.py\n\n\"\"\"\n\nimport os\n\nfrom examples.common.functions import (\n    function_using_argparse,\n    function_using_kwargs,\n    read_initial_params_as_json,\n    read_initial_params_as_pydantic,\n)\nfrom runnable import Pipeline, PythonTask\n\n\ndef main():\n    \"\"\"\n    Signature of read_initial_params_as_pydantic\n    def read_initial_params_as_pydantic(\n        integer: int,\n        floater: float,\n        stringer: str,\n        pydantic_param: ComplexParams,\n        envvar: str,\n    ):\n    \"\"\"\n    read_params_as_pydantic = PythonTask(\n        function=read_initial_params_as_pydantic,\n        name=\"read_params_as_pydantic\",\n    )\n\n    read_params_as_json = PythonTask(\n        function=read_initial_params_as_json,\n        name=\"read_params_as_json\",\n    )\n\n    using_argparse = PythonTask(\n        function=function_using_argparse,\n        name=\"function_using_argparse\",\n    )\n\n    using_kwargs = PythonTask(\n        function=function_using_kwargs,\n        name=\"function_using_kwargs\",\n    )\n\n    pipeline = Pipeline(\n        steps=[\n            read_params_as_pydantic,\n            read_params_as_json,\n            using_argparse,\n            using_kwargs,\n        ],\n    )\n\n    _ = pipeline.execute(parameters_file=\"examples/common/initial_parameters.yaml\")\n\n    return pipeline\n\n\nif __name__ == \"__main__\":\n    # Any parameter prefixed by \"RUNNABLE_PRM_\" will be picked up by runnable\n    os.environ[\"RUNNABLE_PRM_envvar\"] = \"from env\"\n    main()\n</code></pre> <p>Try it now: <pre><code>uv run examples/03-parameters/static_parameters_python.py\n</code></pre></p>"},{"location":"pipelines/parameters-from-outside/#method_2_environment_variables","title":"Method 2: Environment variables","text":"<p>Set variables with <code>RUNNABLE_PRM_</code> prefix:</p> <pre><code>export RUNNABLE_PRM_integer=42\nexport RUNNABLE_PRM_stringer=\"production data\"\nexport RUNNABLE_PRM_envvar=\"from env\"  # This overrides YAML!\n</code></pre> <pre><code>import os\nfrom runnable import Pipeline, PythonTask\n\ndef main():\n    task = PythonTask(function=read_initial_params_as_pydantic)\n    pipeline = Pipeline(steps=[task])\n\n    # Same pipeline execution as before\n    pipeline.execute(parameters_file=\"parameters.yaml\")\n    return pipeline\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"pipelines/parameters-from-outside/#environment_variables_win","title":"\ud83c\udfc6 Environment variables win","text":"<p>If you have both YAML and environment variables, environment variables take priority:</p> <ul> <li>YAML file says: <code>envvar: \"not set\"</code></li> <li>Environment variable: <code>RUNNABLE_PRM_envvar=\"from env\"</code></li> <li>Result: Your function gets <code>\"from env\"</code> \u2705</li> </ul>"},{"location":"pipelines/parameters-from-outside/#why_this_is_powerful","title":"Why this is powerful","text":"<p>Same code, different behavior:</p> <pre><code># Option 1: Individual parameter overrides\nexport RUNNABLE_PRM_dataset=\"small_test_data.csv\"\nuv run my_pipeline.py\n\nexport RUNNABLE_PRM_dataset=\"full_production_data.csv\"\nuv run my_pipeline.py\n\n# Option 2: Complete parameter file switching\nexport RUNNABLE_PARAMETERS_FILE=\"configs/dev.yaml\"\nuv run my_pipeline.py\n\nexport RUNNABLE_PARAMETERS_FILE=\"configs/prod.yaml\"\nuv run my_pipeline.py\n</code></pre> <p>No code changes needed!</p>"},{"location":"pipelines/parameters-from-outside/#method_3_dynamic_parameter_files","title":"Method 3: Dynamic parameter files","text":"<p>Switch parameter files without changing code using <code>RUNNABLE_PARAMETERS_FILE</code>:</p> <pre><code># Use different parameter files for different environments\nexport RUNNABLE_PARAMETERS_FILE=\"dev-parameters.yaml\"\nexport RUNNABLE_PARAMETERS_FILE=\"prod-parameters.yaml\"\nexport RUNNABLE_PARAMETERS_FILE=\"staging-parameters.yaml\"\n</code></pre> <pre><code>from runnable import Pipeline, PythonTask\n\ndef main():\n    task = PythonTask(function=read_initial_params_as_pydantic)\n    pipeline = Pipeline(steps=[task])\n\n    # No need to specify parameters_file - uses RUNNABLE_PARAMETERS_FILE\n    pipeline.execute()\n    return pipeline\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Powerful deployment pattern: <pre><code># Development\nexport RUNNABLE_PARAMETERS_FILE=\"configs/dev.yaml\"\nuv run my_pipeline.py\n\n# Production\nexport RUNNABLE_PARAMETERS_FILE=\"configs/prod.yaml\"\nuv run my_pipeline.py  # Same code, different parameters!\n</code></pre></p>"},{"location":"pipelines/parameters-from-outside/#complex_parameters_work_too","title":"Complex parameters work too","text":"<pre><code># Nested objects\nexport RUNNABLE_PRM_model_config='{\"learning_rate\": 0.01, \"epochs\": 100}'\n\n# Lists\nexport RUNNABLE_PRM_features='[\"age\", \"income\", \"location\"]'\n</code></pre> <p>Pro tip</p> <p>Three-layer flexibility:</p> <ol> <li>Code-specified: <code>pipeline.execute(parameters_file=\"base.yaml\")</code></li> <li>Environment override: <code>RUNNABLE_PARAMETERS_FILE=\"prod.yaml\"</code> (overrides code)</li> <li>Individual parameters: <code>RUNNABLE_PRM_key=\"value\"</code> (overrides both)</li> </ol> <p>Perfect for different environments (dev/staging/prod) without code changes!</p> <p>Next: Learn about automatic file management between tasks.</p>"},{"location":"pipelines/pipeline-parameters/","title":"\ud83d\udd27 Pipeline Parameters &amp; Environment","text":"<p>Configure Pipeline execution without changing code using parameters and environment variables.</p>"},{"location":"pipelines/pipeline-parameters/#parameter_system","title":"Parameter System","text":"<p>Pipelines share the same flexible parameter system as Jobs, with three layers of configuration precedence:</p> <ol> <li>Individual overrides: <code>RUNNABLE_PRM_key=\"value\"</code> (highest priority)</li> <li>Environment file: <code>RUNNABLE_PARAMETERS_FILE=\"config.yaml\"</code></li> <li>Code-specified: <code>pipeline.execute(parameters_file=\"config.yaml\")</code> (lowest priority)</li> </ol> <pre><code>from runnable import Pipeline, PythonTask\n\ndef process_data(batch_size=100, debug=False):\n    print(f\"Processing with batch_size={batch_size}, debug={debug}\")\n    return {\"processed\": True}\n\ndef main():\n    pipeline = Pipeline(steps=[\n        PythonTask(function=process_data, name=\"process\")\n    ])\n\n    # Execute with parameter file\n    pipeline.execute(parameters_file=\"config.yaml\")\n    return pipeline\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Parameter File (config.yaml): <pre><code>batch_size: 1000\ndebug: true\n</code></pre></p>"},{"location":"pipelines/pipeline-parameters/#environment_variable_overrides","title":"Environment Variable Overrides","text":"<p>Override any parameter at runtime:</p> <pre><code># Override specific parameters\nexport RUNNABLE_PRM_batch_size=500\nexport RUNNABLE_PRM_debug=false\n\n# Run pipeline - uses overridden values\nuv run data_pipeline.py\n</code></pre>"},{"location":"pipelines/pipeline-parameters/#custom_run_ids","title":"Custom Run IDs","text":"<p>Control pipeline execution tracking with custom identifiers:</p> <pre><code># Set custom run ID for tracking\nexport RUNNABLE_RUN_ID=\"data-pipeline-daily-run-2024-11-20\"\nuv run data_processing_pipeline.py\n</code></pre> <p>Benefits:</p> <ul> <li>Easy identification in logs and run history</li> <li>Consistent naming across pipeline executions</li> <li>Better debugging when tracking specific pipeline runs</li> <li>Integration with external systems using predictable IDs</li> </ul>"},{"location":"pipelines/pipeline-parameters/#pipeline_run_id_examples","title":"Pipeline Run ID Examples","text":"<pre><code># Daily data processing\nexport RUNNABLE_RUN_ID=\"etl-daily-$(date +%Y-%m-%d)\"\nuv run daily_etl_pipeline.py\n\n# Experiment tracking\nexport RUNNABLE_RUN_ID=\"experiment-feature-engineering-v2\"\nuv run ml_experiment_pipeline.py\n\n# Environment-specific runs\nexport RUNNABLE_RUN_ID=\"staging-deployment-$(git rev-parse --short HEAD)\"\nuv run deployment_pipeline.py\n</code></pre> <p>Default vs Custom Run IDs</p> <p>Without RUNNABLE_RUN_ID: Auto-generated names like <code>courtly-easley-1719</code></p> <p>With RUNNABLE_RUN_ID: Your custom identifier <code>data-pipeline-daily-run-2024-11-20</code></p>"},{"location":"pipelines/pipeline-parameters/#dynamic_parameter_files","title":"Dynamic Parameter Files","text":"<p>Switch configurations without code changes:</p> <pre><code># Development environment\nexport RUNNABLE_PARAMETERS_FILE=\"configs/dev.yaml\"\nuv run ml_pipeline.py\n\n# Production environment\nexport RUNNABLE_PARAMETERS_FILE=\"configs/prod.yaml\"\nuv run ml_pipeline.py  # Same code, different config!\n</code></pre>"},{"location":"pipelines/pipeline-parameters/#common_pipeline_patterns","title":"Common Pipeline Patterns","text":""},{"location":"pipelines/pipeline-parameters/#environment-specific_configurations","title":"Environment-Specific Configurations","text":"<p>Development (dev.yaml): <pre><code>data_source: \"s3://dev-bucket/sample-data/\"\nbatch_size: 100\ndebug: true\nparallel_workers: 1\n</code></pre></p> <p>Production (prod.yaml): <pre><code>data_source: \"s3://prod-bucket/full-data/\"\nbatch_size: 10000\ndebug: false\nparallel_workers: 8\n</code></pre></p>"},{"location":"pipelines/pipeline-parameters/#multi-stage_pipeline_configuration","title":"Multi-Stage Pipeline Configuration","text":"<pre><code># Configure entire pipeline execution\nexport RUNNABLE_PARAMETERS_FILE=\"configs/full-pipeline.yaml\"\nexport RUNNABLE_RUN_ID=\"daily-ml-pipeline-$(date +%Y%m%d)\"\n\n# Override specific stages\nexport RUNNABLE_PRM_training_epochs=100\nexport RUNNABLE_PRM_validation_split=0.2\n\nuv run ml_training_pipeline.py\n</code></pre>"},{"location":"pipelines/pipeline-parameters/#best_practices","title":"Best Practices","text":""},{"location":"pipelines/pipeline-parameters/#use_run_ids_for_pipeline_tracking","title":"\u2705 Use Run IDs for Pipeline Tracking","text":"<pre><code># Predictable naming for scheduled runs\nexport RUNNABLE_RUN_ID=\"weekly-report-$(date +%Y-week-%U)\"\n\n# Git-based versioning for deployments\nexport RUNNABLE_RUN_ID=\"deploy-$(git rev-parse --short HEAD)\"\n\n# Feature branch testing\nexport RUNNABLE_RUN_ID=\"test-$(git branch --show-current)-$(date +%s)\"\n</code></pre>"},{"location":"pipelines/pipeline-parameters/#environment_variables_for_deployment","title":"\u2705 Environment Variables for Deployment","text":"<pre><code># Production deployment values\nexport RUNNABLE_PRM_database_url=\"postgresql://prod:5432/warehouse\"\nexport RUNNABLE_PRM_s3_bucket=\"company-prod-data\"\nexport RUNNABLE_PRM_notification_webhook=\"https://alerts.company.com/pipeline\"\n</code></pre>"},{"location":"pipelines/pipeline-parameters/#layered_configuration_strategy","title":"\u2705 Layered Configuration Strategy","text":"<pre><code>def main():\n    pipeline = Pipeline(steps=[...])\n\n    # 1. Base configuration in code\n    pipeline.execute(parameters_file=\"base_config.yaml\")\n\n    # 2. Environment-specific overrides via RUNNABLE_PARAMETERS_FILE\n    # 3. Runtime tweaks via RUNNABLE_PRM_* variables\n    # 4. Tracking via RUNNABLE_RUN_ID\n\n    return pipeline\n</code></pre> <p>Shared Parameter System</p> <p>Pipelines use the exact same parameter system as Jobs. Once you learn parameters for Jobs, you already know how to configure Pipelines!</p>"},{"location":"pipelines/pipeline-parameters/#whats_next","title":"What's Next?","text":"<ul> <li>Jobs vs Pipelines - When to use which approach</li> <li>Task Types - Different ways to define pipeline steps</li> </ul>"},{"location":"pipelines/reproducibility/","title":"\ud83d\udd04 Perfect Reproducibility Every Time","text":"<p>Tired of \"it worked on my machine\" problems? Runnable automatically captures everything needed to reproduce your workflows.</p>"},{"location":"pipelines/reproducibility/#the_old_way_hope_and_pray","title":"The old way (hope and pray)","text":"<pre><code>def analyze_data():\n    # Which version of pandas was this?\n    # What were the input files?\n    # Which git commit was this?\n    df = pd.read_csv(\"data.csv\")  # Hope it's the same data...\n    return df.groupby('category').mean()  # Hope same pandas behavior...\n</code></pre>"},{"location":"pipelines/reproducibility/#the_runnable_way_automatic_tracking","title":"The Runnable way (automatic tracking)","text":"<p>Every run captures: <pre><code>from runnable import Pipeline, PythonTask\n\npipeline = Pipeline(steps=[\n    PythonTask(function=analyze_data, name=\"analysis\")\n])\nresult = pipeline.execute()  # Everything is automatically tracked!\n</code></pre></p> <p>After running, you get:</p> <ul> <li>\ud83c\udd94 Unique run ID: <code>clever-einstein-1234</code></li> <li>\ud83d\udcdd Complete execution log: <code>.run_log_store/clever-einstein-1234.json</code></li> <li>\ud83d\udcbe All data artifacts: <code>.catalog/clever-einstein-1234/</code></li> <li>\ud83d\udd0d Full metadata: Parameters, timings, code versions</li> </ul>"},{"location":"pipelines/reproducibility/#what_gets_tracked_automatically","title":"What gets tracked automatically","text":"<p>Code &amp; Environment: <pre><code>{\n  \"code_identities\": [{\n    \"code_identifier\": \"7079b8df5c4bf826d3baf6e3f5839ba6193d88dd\",\n    \"code_identifier_type\": \"git\",\n    \"code_identifier_url\": \"https://github.com/your-org/project.git\"\n  }]\n}\n</code></pre></p> <p>Parameters &amp; Data Flow: <pre><code>{\n  \"input_parameters\": {\"threshold\": 0.95},\n  \"output_parameters\": {\"accuracy\": 0.87},\n  \"data_catalog\": [{\n    \"name\": \"model.pkl\",\n    \"data_hash\": \"8650858600ce25b35e978ecb162414d9\"\n  }]\n}\n</code></pre></p> <p>Execution Context: <pre><code>{\n  \"start_time\": \"2025-11-04 22:48:42.128195\",\n  \"status\": \"SUCCESS\",\n  \"pipeline_executor\": {\"service_name\": \"local\"},\n  \"dag_hash\": \"d26e1287acb814e58c72a1c67914033eb0fb2e26\"\n}\n</code></pre></p>"},{"location":"pipelines/reproducibility/#complete_workflow_example","title":"Complete workflow example","text":"<pre><code>from runnable import Pipeline, PythonTask, Catalog, pickled\n\ndef train_model(learning_rate: float = 0.01):\n    model = train_ml_model(learning_rate)\n    return {\"model\": model, \"accuracy\": 0.87}\n\ndef evaluate_model(model, test_data_path: str):\n    accuracy = evaluate(model, test_data_path)\n    return {\"final_accuracy\": accuracy}\n\npipeline = Pipeline(steps=[\n    PythonTask(\n        function=train_model,\n        returns=[pickled(\"model\"), (\"accuracy\", \"json\")],\n        catalog=Catalog(get=[\"train.csv\"], put=[\"model.pkl\"])\n    ),\n    PythonTask(\n        function=evaluate_model,\n        catalog=Catalog(get=[\"test.csv\"])\n    )\n])\n\n# Everything gets tracked automatically\nresult = pipeline.execute()\nprint(f\"Run ID: {result.run_id}\")  # clever-einstein-1234\n</code></pre> See complete runnable code examples/03-parameters/passing_parameters_python.py<pre><code>\"\"\"\nThe below example shows how to set/get parameters in python\ntasks of the pipeline.\n\nThe function, set_parameter, returns\n    - JSON serializable types\n    - pydantic models\n    - pandas dataframe, any \"object\" type\n\npydantic models are implicitly handled by runnable\nbut \"object\" types should be marked as \"pickled\".\n\nUse pickled even for python data types is advised for\nreasonably large collections.\n\nRun the below example as:\n    python examples/03-parameters/passing_parameters_python.py\n\n\"\"\"\n\nfrom examples.common.functions import read_parameter, write_parameter\nfrom runnable import Pipeline, PythonTask, metric, pickled\n\n\ndef main():\n    write_parameters = PythonTask(\n        function=write_parameter,\n        returns=[\n            pickled(\"df\"),\n            \"integer\",\n            \"floater\",\n            \"stringer\",\n            \"pydantic_param\",\n            metric(\"score\"),\n        ],\n        name=\"set_parameter\",\n    )\n\n    read_parameters = PythonTask(\n        function=read_parameter,\n        terminate_with_success=True,\n        name=\"get_parameters\",\n    )\n\n    pipeline = Pipeline(\n        steps=[write_parameters, read_parameters],\n    )\n\n    _ = pipeline.execute()\n\n    return pipeline\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Try it now: <pre><code>uv run examples/03-parameters/passing_parameters_python.py\n</code></pre></p>"},{"location":"pipelines/reproducibility/#exploring_your_run_history","title":"Exploring your run history","text":"<p>Find your run: <pre><code>ls .run_log_store/\n# clever-einstein-1234.json\n# nervous-tesla-5678.json\n</code></pre></p> <p>Examine what happened: <pre><code>import json\nwith open('.run_log_store/clever-einstein-1234.json') as f:\n    run_log = json.load(f)\n\nprint(f\"Status: {run_log['status']}\")\nprint(f\"Final accuracy: {run_log['parameters']['final_accuracy']}\")\n</code></pre></p> <p>Access the data: <pre><code>ls .catalog/clever-einstein-1234/\n# model.pkl\n# train.csv\n# test.csv\n# step1_execution.log\n</code></pre></p>"},{"location":"pipelines/reproducibility/#real_example_catalog_tracking","title":"Real example: Catalog tracking","text":"<p>Let's see how file management gets tracked:</p> <pre><code>from runnable import Pipeline, PythonTask, Catalog\n\ndef generate_data():\n    # Create files that will be tracked\n    df.to_csv(\"df.csv\")\n    with open(\"data_folder/data.txt\", \"w\") as f:\n        f.write(\"Important data\")\n\ndef process_data():\n    # Files are automatically available here\n    df = pd.read_csv(\"df.csv\")\n    with open(\"data_folder/data.txt\") as f:\n        content = f.read()\n\npipeline = Pipeline(steps=[\n    PythonTask(\n        function=generate_data,\n        catalog=Catalog(put=[\"df.csv\", \"data_folder/data.txt\"]),\n        name=\"generate\"\n    ),\n    PythonTask(\n        function=process_data,\n        catalog=Catalog(get=[\"df.csv\", \"data_folder/data.txt\"]),\n        name=\"process\"\n    )\n])\npipeline.execute()\n</code></pre> See complete runnable code examples/04-catalog/catalog_python.py<pre><code>\"\"\"\nYou can execute this pipeline by:\n\n    python examples/04-catalog/catalog_python.py\n\"\"\"\n\nfrom examples.common.functions import read_files, write_files\nfrom runnable import Catalog, Pipeline, PythonTask, ShellTask\n\n\ndef main():\n    write_catalog = Catalog(put=[\"df.csv\", \"data_folder/data.txt\"])\n    generate_data = PythonTask(\n        name=\"generate_data_python\",\n        function=write_files,\n        catalog=write_catalog,\n    )\n\n    delete_files_command = \"\"\"\n        rm df.csv || true &amp;&amp; \\\n        rm data_folder/data.txt || true\n    \"\"\"\n    # delete from local files after generate\n    # since its local catalog, we delete to show \"get from catalog\"\n    delete_local_after_generate = ShellTask(\n        name=\"delete_after_generate\",\n        command=delete_files_command,\n    )\n\n    read_catalog = Catalog(get=[\"df.csv\", \"data_folder/data.txt\"])\n    read_data_python = PythonTask(\n        name=\"read_data_python\",\n        function=read_files,\n        catalog=read_catalog,\n        terminate_with_success=True,\n    )\n\n    pipeline = Pipeline(\n        steps=[\n            generate_data,\n            delete_local_after_generate,\n            read_data_python,\n        ]\n    )\n    _ = pipeline.execute()\n\n    return pipeline\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Try it now: <pre><code>uv run examples/04-catalog/catalog_python.py\n</code></pre></p>"},{"location":"pipelines/reproducibility/#what_gets_logged_for_each_step","title":"What gets logged for each step","text":"<p>Step Summary: <pre><code>{\n  \"Name\": \"generate_data\",\n  \"Input catalog content\": [],\n  \"Available parameters\": [],\n  \"Output catalog content\": [\"df.csv\", \"data_folder/data.txt\"],\n  \"Output parameters\": [],\n  \"Metrics\": [],\n  \"Code identities\": [\"git:7079b8df5c4bf826d3baf6e3f5839ba6193d88dd\"],\n  \"status\": \"SUCCESS\"\n}\n</code></pre></p> <p>File Tracking: <pre><code>{\n  \"data_catalog\": [\n    {\n      \"name\": \"df.csv\",\n      \"data_hash\": \"8650858600ce25b35e978ecb162414d9\",\n      \"catalog_relative_path\": \"run-id-123/df.csv\",\n      \"stage\": \"put\"\n    }\n  ]\n}\n</code></pre></p>"},{"location":"pipelines/reproducibility/#why_this_matters","title":"Why this matters","text":"<p>Without automatic tracking:</p> <ul> <li>\u274c \"It worked last week\" debugging sessions</li> <li>\u274c Lost parameter combinations that worked</li> <li>\u274c No way to reproduce important results</li> <li>\u274c Manual documentation that gets stale</li> </ul> <p>With Runnable's tracking:</p> <ul> <li>\u2705 Every run is completely reproducible</li> <li>\u2705 Compare results across different runs</li> <li>\u2705 Debug with full execution context</li> <li>\u2705 Zero-effort audit trails for compliance</li> </ul>"},{"location":"pipelines/reproducibility/#advanced_custom_run_tracking","title":"Advanced: Custom run tracking","text":"<pre><code># Tag important runs\npipeline.execute(tag=\"production-candidate\")\n\n# Environment-specific tracking\npipeline.execute(config=\"configs/production.yaml\")\n</code></pre>"},{"location":"pipelines/reproducibility/#custom_run_ids_via_environment","title":"Custom Run IDs via Environment","text":"<p>Control pipeline execution tracking with custom identifiers:</p> <pre><code># Set custom run ID for tracking and debugging\nexport RUNNABLE_RUN_ID=\"experiment-learning-rate-001\"\nuv run ml_pipeline.py\n\n# Daily ETL runs with dates\nexport RUNNABLE_RUN_ID=\"etl-daily-$(date +%Y-%m-%d)\"\nuv run data_pipeline.py\n\n# Experiment tracking with git context\nexport RUNNABLE_RUN_ID=\"experiment-$(git branch --show-current)-v2\"\nuv run research_pipeline.py\n</code></pre> <p>Benefits for reproducibility:</p> <ul> <li>Predictable naming for experiment tracking</li> <li>Easy identification in run history and logs</li> <li>Integration with external systems and CI/CD</li> <li>Consistent tracking across related pipeline executions</li> </ul>"},{"location":"pipelines/reproducibility/#run_id_patterns","title":"Run ID patterns","text":"<p>Runnable generates memorable run IDs automatically:</p> <ul> <li><code>obnoxious-williams-2248</code> - From our catalog example</li> <li><code>nervous-sinoussi-2248</code> - From our parameters example</li> <li><code>clever-einstein-1234</code> - Hypothetical example</li> </ul> <p>Each ID is unique and helps you easily reference specific runs in conversations and debugging.</p> <p>Best practices</p> <ul> <li>Let Runnable generate run IDs for exploration</li> <li>Use tags for important experimental runs</li> <li>Keep your git repo clean for reliable code tracking</li> <li>Use the catalog for all data that flows between steps</li> </ul> <p>Next: Learn how to deploy anywhere while keeping the same reproducibility guarantees.</p>"},{"location":"pipelines/task-types/","title":"Task Types","text":"<p>Execute different types of tasks in pipelines through runnable's extensible task type system.</p>"},{"location":"pipelines/task-types/#the_core_insight","title":"The Core Insight","text":"<p>All task types follow the same pattern: They create pipeline steps that wrap a <code>TaskType</code> for actual execution, with runnable providing orchestration, parameter passing, and data flow.</p>"},{"location":"pipelines/task-types/#built-in_task_types","title":"Built-in Task Types","text":""},{"location":"pipelines/task-types/#python_tasks","title":"Python Tasks \ud83d\udc0d","text":"<p>Execute Python functions as pipeline steps:</p> <pre><code>from runnable import Pipeline, PythonTask\nfrom examples.common.functions import hello\n\ndef main():\n    task = PythonTask(function=hello, name=\"say_hello\")\n    pipeline = Pipeline(steps=[task])\n    pipeline.execute()\n    return pipeline  # REQUIRED: Always return the pipeline object\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Perfect for: Data processing, ML models, business logic</p> <p>IDE Debugging Just Works</p> <p>Python tasks are plain functions - set breakpoints and debug with any IDE. No special configuration required.</p> <pre><code>def process_data(input_file: str) -&gt; dict:\n    data = load_file(input_file)  # Set breakpoint here\n    result = transform(data)       # Step through code\n    return {\"output\": result}      # Inspect variables\n</code></pre> <p>VSCode, PyCharm, or any Python debugger works out of the box. Runnable calls your functions directly during local execution - no subprocess isolation or remote calls to complicate debugging.</p>"},{"location":"pipelines/task-types/#notebook_tasks","title":"Notebook Tasks \ud83d\udcd3","text":"<p>Execute Jupyter notebooks as pipeline steps:</p> <pre><code>from runnable import Pipeline, NotebookTask\n\ndef main():\n    task = NotebookTask(\n        name=\"analyze\",\n        notebook=\"examples/common/simple_notebook.ipynb\"\n    )\n    pipeline = Pipeline(steps=[task])\n    pipeline.execute()\n    return pipeline  # REQUIRED: Always return the pipeline object\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Perfect for: Exploration, visualization, reporting</p>"},{"location":"pipelines/task-types/#shell_tasks","title":"Shell Tasks \ud83d\udd27","text":"<p>Execute shell commands as pipeline steps:</p> <pre><code>from runnable import Pipeline, ShellTask\n\ndef main():\n    task = ShellTask(\n        name=\"greet\",\n        command=\"echo 'Hello World!'\"\n    )\n    pipeline = Pipeline(steps=[task])\n    pipeline.execute()\n    return pipeline  # REQUIRED: Always return the pipeline object\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Perfect for: System commands, external tools, legacy scripts</p>"},{"location":"pipelines/task-types/#stub_tasks","title":"Stub Tasks \ud83c\udfad","text":"<p>Placeholder tasks for testing and workflow structure:</p> <pre><code>from runnable import Pipeline, Stub\n\ndef main():\n    pipeline = Pipeline(steps=[\n        Stub(name=\"extract_data\"),\n        Stub(name=\"process_data\"),\n        Stub(name=\"save_results\")\n    ])\n    pipeline.execute()\n    return pipeline  # REQUIRED: Always return the pipeline object\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Perfect for: Testing pipeline structure, placeholder steps</p>"},{"location":"pipelines/task-types/#async_python_tasks","title":"Async Python Tasks \u26a1","text":"<p>Execute async functions with streaming support in async pipelines:</p> <p>Local Execution Only</p> <p>AsyncPythonTask and AsyncPipeline are currently only supported for local execution. They cannot be used with containerized or Kubernetes pipeline executors.</p> <pre><code>from runnable import AsyncPipeline, AsyncPythonTask\nimport asyncio\n\nasync def async_data_fetch(url: str):\n    \"\"\"Simple async function.\"\"\"\n    await asyncio.sleep(1)  # Simulate API call\n    return {\"data\": f\"fetched from {url}\", \"status\": \"success\"}\n\ndef main():\n    task = AsyncPythonTask(\n        name=\"fetch_data\",\n        function=async_data_fetch,\n        returns=[\"result\"]\n    )\n\n    pipeline = AsyncPipeline(steps=[task])\n\n    # In async context: await pipeline.execute()\n    return pipeline\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Perfect for: LLM inference, real-time streaming, async APIs, long-running operations</p> <p>Streaming Support</p> <p>AsyncPythonTask supports AsyncGenerator functions for real-time streaming:</p> <pre><code>from typing import AsyncGenerator\n\nasync def stream_llm_response(prompt: str) -&gt; AsyncGenerator[dict, None]:\n    # Stream events in real-time\n    yield {\"type\": \"status\", \"message\": \"Processing\"}\n    await asyncio.sleep(0.5)\n\n    # Stream incremental results\n    words = [\"Hello\", \"from\", \"the\", \"LLM\"]\n    for word in words:\n        yield {\"type\": \"chunk\", \"text\": word}\n        await asyncio.sleep(0.1)\n\n    # Final result for pipeline\n    yield {\"type\": \"done\", \"response\": \" \".join(words)}\n\ndef main():\n    task = AsyncPythonTask(\n        name=\"llm_stream\",\n        function=stream_llm_response,\n        returns=[\"response\"],\n        stream_end_type=\"done\"  # Extract values from \"done\" events\n    )\n\n    return AsyncPipeline(steps=[task])\n</code></pre> <p>See Async &amp; Streaming for complete streaming patterns and FastAPI integration.</p>"},{"location":"pipelines/task-types/#pipeline_task_execution_context","title":"Pipeline Task Execution Context","text":"<p>All task types share the same rich pipeline execution features:</p>"},{"location":"pipelines/task-types/#common_features_across_all_task_types","title":"Common Features Across All Task Types","text":"<ul> <li>Parameter flow: Tasks receive parameters from previous steps and configuration</li> <li>Return values: Tasks can return data to subsequent steps</li> <li>Cross-step data passing: Use the catalog system for file-based data sharing</li> <li>Mixed execution: Combine different task types in the same pipeline</li> <li>Environment agnostic: Run on local, container, or Kubernetes environments</li> </ul>"},{"location":"pipelines/task-types/#example_mixed_task_pipeline","title":"Example: Mixed Task Pipeline","text":"<pre><code>from runnable import Pipeline, PythonTask, NotebookTask, ShellTask\n\ndef main():\n    pipeline = Pipeline(steps=[\n        PythonTask(function=extract_data, name=\"extract\", returns=[\"raw_df\"]),\n        NotebookTask(name=\"clean\", notebook=\"clean.ipynb\", returns=[\"clean_df\"]),\n        ShellTask(name=\"analyze\", command=\"./analyze.sh\", returns=[\"report_path\"])\n    ])\n    pipeline.execute()\n    return pipeline  # REQUIRED: Always return the pipeline object\n</code></pre> <p>Each task type provides the same capabilities:</p> <ul> <li>Parameter access: <code>{previous_step_return}</code> interpolation</li> <li>Configuration: Same YAML/environment variable system</li> <li>Catalog integration: File storage and retrieval</li> <li>Execution tracking: Complete run logs and metadata</li> </ul>"},{"location":"pipelines/task-types/#the_plugin_system","title":"The Plugin System","text":"<p>Task types are pluggable - runnable automatically discovers and loads custom task types via entry points.</p>"},{"location":"pipelines/task-types/#how_pipeline_tasks_work_internally","title":"How Pipeline Tasks Work Internally","text":"<p>Every task type follows the same pattern:</p> <ol> <li>Task class: Provides the pipeline API (<code>PythonTask</code>, <code>ShellTask</code>, etc.)</li> <li>Task type: Handles the actual execution (<code>PythonTaskType</code>, <code>ShellTaskType</code>, etc.)</li> <li>Entry point registration: Makes it discoverable</li> </ol> <pre><code># Built-in task types are registered like this:\n[project.entry-points.'tasks']\n\"python\" = \"runnable.tasks:PythonTaskType\"\n\"shell\" = \"runnable.tasks:ShellTaskType\"\n\"notebook\" = \"runnable.tasks:NotebookTaskType\"\n</code></pre>"},{"location":"pipelines/task-types/#building_custom_task_types_for_pipelines","title":"Building Custom Task Types for Pipelines","text":"<p>Create new task types for your specific pipeline needs:</p>"},{"location":"pipelines/task-types/#1_implement_the_task_type_same_as_jobs","title":"1. Implement the Task Type (same as Jobs)","text":"<pre><code># my_package/tasks.py\nfrom runnable.tasks import BaseTaskType\n\nclass RTaskType(BaseTaskType):\n    \"\"\"Execute R scripts with full runnable integration\"\"\"\n    task_type: str = \"r\"\n    script_path: str = Field(...)\n\n    # Any pydantic validators\n\n    def execute_command(\n        self,\n        map_variable: MapVariableType = None,\n    ) -&gt; StepAttempt:\n        # Your R execution logic\n        command = f\"Rscript {self.script_path}\"\n        # Run command and return StepAttempt\n        pass\n</code></pre>"},{"location":"pipelines/task-types/#2_create_the_pipeline_task_wrapper","title":"2. Create the Pipeline Task Wrapper","text":"<pre><code># my_package/tasks.py\nfrom runnable.sdk import BaseTask\n\nclass RTask(BaseTask):\n    \"\"\"R script execution in pipelines\"\"\"\n    # The fields should match the fields of the corresponding task\n    script_path: str = Field(...)\n\n    # Should match to the key used in the plugin\n    command_type: str = Field(default=\"r\")\n</code></pre>"},{"location":"pipelines/task-types/#3_register_the_task_type","title":"3. Register the Task Type","text":"<pre><code># pyproject.toml\n[project.entry-points.'tasks']\n\"r\" = \"my_package.tasks:RTaskType\"\n</code></pre>"},{"location":"pipelines/task-types/#4_use_your_custom_task_in_pipelines","title":"4. Use Your Custom Task in Pipelines","text":"<pre><code>from my_package.tasks import RTask\nfrom runnable import Pipeline\n\ndef main():\n    pipeline = Pipeline(steps=[\n        RTask(name=\"analysis\", script_path=\"analysis.R\"),\n        PythonTask(name=\"postprocess\", function=process_r_results)\n    ])\n    pipeline.execute()\n    return pipeline  # REQUIRED: Always return the pipeline object\n</code></pre>"},{"location":"pipelines/task-types/#integration_advantage","title":"Integration Advantage","text":"<p>\ud83d\udd11 Key Benefit: Custom task types live entirely in your codebase, enabling domain-specific pipeline steps.</p>"},{"location":"pipelines/task-types/#complete_control_customization","title":"Complete Control &amp; Customization","text":"<pre><code># In your private repository\n# company-analytics/tasks/proprietary_tasks.py\n\nclass CompanyAnalyticsTask(BaseTask):\n    \"\"\"Execute proprietary analytics in pipelines\"\"\"\n    dataset_id: str = Field(...)\n    compliance_level: str = Field(default=\"confidential\")\n\n    def create_job(self) -&gt; CompanyAnalyticsTaskType:\n        # Your proprietary task implementation\n        pass\n</code></pre> <p>Integration benefits:</p> <ul> <li>\ud83d\udd12 Proprietary Tools: Connect pipelines to internal platforms, databases, and tools</li> <li>\ud83c\udfe2 Domain-Specific: Create task types for your specific business processes</li> <li>\ud83d\udcbc Compliance: Implement organization-specific governance and audit requirements</li> <li>\ud83d\udd27 Standardization: Reusable task types across teams and projects</li> </ul>"},{"location":"pipelines/task-types/#reusable_task_libraries","title":"Reusable Task Libraries","text":"<pre><code># Internal package: company-runnable-tasks\nfrom company_runnable_tasks import (\n    DataValidationTask,       # Company data quality checks\n    ComplianceReportTask,     # Regulatory reporting\n    MLModelTrainingTask,      # Your ML platform integration\n    CustomerSegmentationTask, # CRM analytics integration\n)\n\n# Teams build standardized pipelines\npipeline = Pipeline(steps=[\n    DataValidationTask(name=\"validate\", dataset=\"customer_data\"),\n    CustomerSegmentationTask(name=\"segment\", model_type=\"rfm\"),\n    ComplianceReportTask(name=\"report\", format=\"sox_compliance\")\n])\n</code></pre> <p>This makes runnable a platform for building your company's custom pipeline ecosystem - standardized, compliant, and tailored to your business logic.</p>"},{"location":"pipelines/task-types/#need_help","title":"Need Help?","text":"<p>Custom task types involve understanding both the task execution model and your target tool's integration requirements.</p> <p>Get Support</p> <p>We're here to help you succeed! Building custom task types involves:</p> <ul> <li>Understanding runnable's task execution lifecycle and pipeline integration</li> <li>Integrating with external tools and platforms</li> <li>Proper parameter flow and data passing between pipeline steps</li> <li>Plugin registration and discovery</li> </ul> <p>Don't hesitate to reach out:</p> <ul> <li>\ud83d\udce7 Contact the team for architecture guidance and integration support</li> <li>\ud83e\udd1d Collaboration opportunities - we're interested in supporting domain-specific integrations</li> <li>\ud83d\udcd6 Documentation feedback - help us improve these guides based on your implementation experience</li> </ul> <p>Your success with custom task types helps the entire runnable community!</p>"},{"location":"pipelines/visualization/","title":"Pipeline Visualization \ud83d\udcca","text":"<p>Visualize your pipeline execution history with interactive timelines that show execution flow, timing, and hierarchical structure.</p>"},{"location":"pipelines/visualization/#why_visualize_pipelines","title":"Why Visualize Pipelines?","text":"<p>Pipeline visualization helps you:</p> <ul> <li>Debug execution flows - See exactly how your pipeline executed</li> <li>Identify bottlenecks - Find slow tasks and optimization opportunities</li> <li>Understand parallel execution - Visualize concurrent branches and timing</li> <li>Monitor production runs - Track pipeline performance over time</li> <li>Document workflows - Share visual pipeline reports with stakeholders</li> </ul>"},{"location":"pipelines/visualization/#quick_start","title":"Quick Start","text":"<pre><code># Run any pipeline to generate execution logs\nuv run examples/02-sequential/traversal.py\n\n# Visualize the execution (console + HTML + browser)\nuv run runnable timeline ancient-pike-2335\n</code></pre>"},{"location":"pipelines/visualization/#timeline_command","title":"Timeline Command","text":""},{"location":"pipelines/visualization/#basic_usage","title":"Basic Usage","text":"<pre><code>runnable timeline [RUN_ID_OR_PATH]\n</code></pre>"},{"location":"pipelines/visualization/#input_options","title":"Input Options","text":"<p>Using Run ID (looks in <code>.run_log_store/</code>): <pre><code>uv run runnable timeline forgiving-joliot-0645\n</code></pre></p> <p>Using JSON file path: <pre><code>uv run runnable timeline .run_log_store/pipeline-run.json\nuv run runnable timeline /path/to/my-run.json\n</code></pre></p>"},{"location":"pipelines/visualization/#output_control","title":"Output Control","text":"Option Description Default <code>--output</code>, <code>-o</code> Custom HTML file path <code>{run_id}_timeline.html</code> <code>--console</code> / <code>--no-console</code> Show console output <code>true</code> <code>--open</code> / <code>--no-open</code> Auto-open in browser <code>true</code>"},{"location":"pipelines/visualization/#console_timeline_output","title":"Console Timeline Output","text":""},{"location":"pipelines/visualization/#sequential_pipeline","title":"Sequential Pipeline","text":"<pre><code>\ud83d\udd04 Pipeline Timeline - ancient-pike-2335\nStatus: SUCCESS\n================================================================================\n  \ud83d\udcdd \u2705 hello stub (0.0ms)\n  \u2699\ufe0f \u2705 hello python (2.2ms)\n     \ud83d\udcdd PYTHON: examples.common.functions.hello\n  \u2699\ufe0f \u2705 hello shell (16.5ms)\n     \ud83d\udcdd SHELL: echo 'Hello World!'\n  \u2699\ufe0f \u2705 hello notebook (4325.8ms)\n     \ud83d\udcdd NOTEBOOK: examples/common/simple_notebook.ipynb\n  \u2705 \u2705 success (0.0ms)\n================================================================================\n</code></pre>"},{"location":"pipelines/visualization/#parallel_pipeline","title":"Parallel Pipeline","text":"<pre><code># Run parallel example first\nuv run examples/06-parallel/parallel.py\nuv run runnable timeline fried-pasteur-2336 --no-open\n</code></pre> <pre><code>\ud83d\udd04 Pipeline Timeline - fried-pasteur-2336\nStatus: SUCCESS\n================================================================================\n\ud83d\udd00 parallel_step (parallel)\n  \u251c\u2500 Branch: branch1\n      \ud83d\udcdd \u2705 hello stub (0.0ms)\n      \u2699\ufe0f \u2705 hello python (2.0ms)\n      \u2699\ufe0f \u2705 hello shell (19.6ms)\n      \u2699\ufe0f \u2705 hello notebook (1269.7ms)\n      \u2705 \u2705 success (0.0ms)\n  \u251c\u2500 Branch: branch2\n      \ud83d\udcdd \u2705 hello stub (0.0ms)\n      \u2699\ufe0f \u2705 hello python (2.4ms)\n      \u2699\ufe0f \u2705 hello shell (16.5ms)\n      \u2699\ufe0f \u2705 hello notebook (23.3ms)\n      \u2705 \u2705 success (0.0ms)\n  \ud83d\udcdd \u2705 continue to (0.0ms)\n  \u2705 \u2705 success (0.0ms)\n================================================================================\n</code></pre>"},{"location":"pipelines/visualization/#visual_elements","title":"Visual Elements","text":"Symbol Meaning \ud83d\udcdd Stub task \u2699\ufe0f Executable task (Python, Shell, Notebook) \u2705 Success node \u274c Failure node \ud83d\udd00 Parallel execution block \u251c\u2500 Branch indicator"},{"location":"pipelines/visualization/#interactive_html_timeline","title":"Interactive HTML Timeline","text":"<p>The HTML output provides rich interactive features:</p> <ul> <li>Hover tooltips - Detailed task information</li> <li>Expandable sections - Collapse/expand parallel branches</li> <li>Rich metadata - Commands, parameters, execution details</li> <li>Visual timeline - Graphical execution flow</li> <li>Responsive design - Works on all devices</li> </ul>"},{"location":"pipelines/visualization/#example_commands","title":"Example Commands","text":"<pre><code># Default: Console + HTML + Browser\nuv run runnable timeline my-pipeline-run\n\n# Custom HTML file\nuv run runnable timeline complex-pipeline --output report.html\n\n# Console only (no browser)\nuv run runnable timeline debug-run --no-open\n\n# HTML only (no console)\nuv run runnable timeline prod-run --no-console\n</code></pre>"},{"location":"pipelines/visualization/#practical_examples","title":"Practical Examples","text":""},{"location":"pipelines/visualization/#development_debugging","title":"\ud83d\udd0d Development Debugging","text":"<pre><code># Quick console feedback during development\nuv run examples/02-sequential/traversal.py\nuv run runnable timeline $(ls .run_log_store/ | tail -1 | cut -d. -f1) --no-open\n</code></pre>"},{"location":"pipelines/visualization/#performance_analysis","title":"\ud83d\udcca Performance Analysis","text":"<pre><code># Compare sequential vs parallel execution\nuv run examples/02-sequential/traversal.py\nuv run runnable timeline sequential-run --output sequential.html --no-open\n\nuv run examples/06-parallel/parallel.py\nuv run runnable timeline parallel-run --output parallel.html --no-open\n</code></pre>"},{"location":"pipelines/visualization/#failure_investigation","title":"\ud83d\udc1b Failure Investigation","text":"<pre><code># Run a pipeline that might fail\nuv run examples/02-sequential/default_fail.py\nuv run runnable timeline failed-run --output failure-analysis.html\n</code></pre>"},{"location":"pipelines/visualization/#production_monitoring","title":"\ud83d\udccb Production Monitoring","text":"<pre><code># Generate timeline reports for production runs\nuv run runnable timeline $PROD_RUN_ID --output \"prod-$(date +%Y%m%d).html\" --no-open\n</code></pre>"},{"location":"pipelines/visualization/#supported_pipeline_types","title":"Supported Pipeline Types","text":"<p>Timeline visualization works with all pipeline patterns:</p> <ul> <li>\u2705 Sequential workflows - Linear task execution</li> <li>\u2705 Parallel execution - Multi-branch concurrent processing</li> <li>\u2705 Map operations - Iterative data processing</li> <li>\u2705 Conditional workflows - Branching logic based on parameters</li> <li>\u2705 Nested structures - Complex hierarchical pipelines</li> <li>\u2705 Mixed task types - Python, Shell, Notebook, Stub tasks</li> <li>\u2705 Error scenarios - Failed executions with clear indicators</li> </ul>"},{"location":"pipelines/visualization/#integration_with_pipeline_development","title":"Integration with Pipeline Development","text":""},{"location":"pipelines/visualization/#during_pipeline_design","title":"During Pipeline Design","text":"<pre><code># Test your pipeline structure\nuv run your_pipeline.py\nuv run runnable timeline latest-run --no-open  # Quick console check\n</code></pre>"},{"location":"pipelines/visualization/#during_optimization","title":"During Optimization","text":"<pre><code># Before optimization\nuv run slow_pipeline.py\nuv run runnable timeline baseline --output baseline.html --no-open\n\n# After optimization\nuv run optimized_pipeline.py\nuv run runnable timeline optimized --output optimized.html --no-open\n</code></pre>"},{"location":"pipelines/visualization/#for_documentation","title":"For Documentation","text":"<pre><code># Generate visual documentation\nuv run example_pipeline.py\nuv run runnable timeline demo-run --output pipeline-demo.html --no-open\n</code></pre>"},{"location":"pipelines/visualization/#tips_and_troubleshooting","title":"Tips and Troubleshooting","text":""},{"location":"pipelines/visualization/#best_practices","title":"\ud83c\udfaf Best Practices","text":"<ul> <li>Use <code>--no-open</code> during development to avoid browser spam</li> <li>Use custom output names for comparison: <code>--output baseline.html</code></li> <li>Check console output first before opening HTML for quick debugging</li> </ul>"},{"location":"pipelines/visualization/#common_issues","title":"\ud83d\udd27 Common Issues","text":"<ul> <li>Empty timeline: Pipeline might not have completed - check run logs</li> <li>Run ID not found: Check <code>.run_log_store/</code> directory or use full JSON path</li> <li>No timing data: Ensure pipeline actually executed (not just validated)</li> </ul>"},{"location":"pipelines/visualization/#whats_next","title":"What's Next?","text":"<ul> <li>Parallel Execution - Create parallel workflows to visualize</li> <li>Map Patterns - Build iterative pipelines with rich timelines</li> <li>Conditional Workflows - Visualize branching logic</li> <li>Failure Handling - Debug error scenarios with timelines</li> </ul> <p>Ready to visualize your pipelines? Run any example and explore its timeline! \ud83d\ude80</p>"},{"location":"plans/2024-11-29-k8s-cronjob-scheduling/","title":"K8s CronJob Scheduling Implementation Plan","text":"<p>For Claude: REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.</p> <p>Goal: Add optional cron scheduling to K8sJobExecutor that creates Kubernetes CronJobs instead of Jobs when schedule is configured.</p> <p>Architecture: Extend existing K8sJobExecutor configuration with optional <code>schedule</code> field. When present, create CronJob using existing job spec logic wrapped in CronJob template. Display scheduled job details to console without immediate execution.</p> <p>Tech Stack: Python, Pydantic, Kubernetes Python Client, pytest</p>"},{"location":"plans/2024-11-29-k8s-cronjob-scheduling/#task_1_add_schedule_configuration_field","title":"Task 1: Add Schedule Configuration Field","text":"<p>Files: - Modify: <code>extensions/job_executor/k8s.py:170-176</code> (GenericK8sJobExecutor class) - Test: <code>tests/extensions/job_executor/test_k8s_scheduling.py</code> (new file)</p> <p>Step 1: Write the failing test for schedule field validation</p> <pre><code>import pytest\nfrom extensions.job_executor.k8s import GenericK8sJobExecutor, Spec\nfrom pydantic import ValidationError\n\ndef test_schedule_field_accepts_valid_cron_expression():\n    \"\"\"Test that schedule field accepts valid cron expressions\"\"\"\n    config = {\n        \"job_spec\": {\"template\": {\"spec\": {\"container\": {\"image\": \"test\"}}}},\n        \"schedule\": \"0 2 * * *\"\n    }\n    executor = GenericK8sJobExecutor(**config)\n    assert executor.schedule == \"0 2 * * *\"\n\ndef test_schedule_field_defaults_to_none():\n    \"\"\"Test that schedule field defaults to None for backward compatibility\"\"\"\n    config = {\n        \"job_spec\": {\"template\": {\"spec\": {\"container\": {\"image\": \"test\"}}}}\n    }\n    executor = GenericK8sJobExecutor(**config)\n    assert executor.schedule is None\n\ndef test_schedule_field_validates_cron_format():\n    \"\"\"Test that invalid cron expressions are rejected\"\"\"\n    config = {\n        \"job_spec\": {\"template\": {\"spec\": {\"container\": {\"image\": \"test\"}}}},\n        \"schedule\": \"invalid cron\"\n    }\n    with pytest.raises(ValidationError, match=\"valid cron expression\"):\n        GenericK8sJobExecutor(**config)\n</code></pre> <p>Step 2: Run test to verify it fails</p> <p>Run: <code>pytest tests/extensions/job_executor/test_k8s_scheduling.py::test_schedule_field_accepts_valid_cron_expression -v</code> Expected: FAIL with \"test_k8s_scheduling.py not found\" or \"schedule field not found\"</p> <p>Step 3: Create test file</p> <p>Create file: <code>tests/extensions/job_executor/test_k8s_scheduling.py</code> with the test code above.</p> <p>Step 4: Run test again to verify field doesn't exist</p> <p>Run: <code>pytest tests/extensions/job_executor/test_k8s_scheduling.py -v</code> Expected: FAIL with \"schedule field not found\" or similar</p> <p>Step 5: Add schedule field to GenericK8sJobExecutor</p> <p>In <code>extensions/job_executor/k8s.py</code>, modify the GenericK8sJobExecutor class:</p> <pre><code>import re\nfrom pydantic import BaseModel, ConfigDict, Field, PlainSerializer, PrivateAttr, field_validator\n\nclass GenericK8sJobExecutor(GenericJobExecutor):\n    service_name: str = \"k8s-job\"\n    config_path: Optional[str] = None\n    job_spec: Spec\n    mock: bool = False\n    namespace: str = Field(default=\"default\")\n    schedule: Optional[str] = Field(default=None, description=\"Cron expression for scheduling (e.g., '0 2 * * *')\")\n\n    @field_validator('schedule')\n    @classmethod\n    def validate_schedule(cls, v):\n        if v is not None:\n            # Validate cron expression format (5 fields: minute hour day month weekday)\n            if not re.match(r'^(\\S+\\s+){4}\\S+$', v):\n                raise ValueError(\"Schedule must be a valid cron expression with 5 fields (minute hour day month weekday)\")\n        return v\n</code></pre> <p>Step 6: Run test to verify it passes</p> <p>Run: <code>pytest tests/extensions/job_executor/test_k8s_scheduling.py -v</code> Expected: PASS (all 3 tests)</p> <p>Step 7: Commit</p> <pre><code>git add extensions/job_executor/k8s.py tests/extensions/job_executor/test_k8s_scheduling.py\ngit commit -m \"feat: add optional schedule field to K8sJobExecutor\"\n</code></pre>"},{"location":"plans/2024-11-29-k8s-cronjob-scheduling/#task_2_implement_cronjob_creation_method","title":"Task 2: Implement CronJob Creation Method","text":"<p>Files: - Modify: <code>extensions/job_executor/k8s.py:254-349</code> (add submit_k8s_cronjob method) - Test: <code>tests/extensions/job_executor/test_k8s_scheduling.py</code> (add cronjob creation tests)</p> <p>Step 1: Write the failing test for CronJob creation</p> <p>Add to <code>tests/extensions/job_executor/test_k8s_scheduling.py</code>:</p> <pre><code>from unittest.mock import Mock, patch\nfrom runnable.tasks import BaseTaskType\n\ndef test_submit_k8s_cronjob_creates_cronjob_instead_of_job():\n    \"\"\"Test that submit_k8s_cronjob creates a CronJob with the schedule\"\"\"\n    config = {\n        \"job_spec\": {\"template\": {\"spec\": {\"container\": {\"image\": \"test\"}}}},\n        \"schedule\": \"0 2 * * *\",\n        \"mock\": True  # Use mock mode to avoid actual K8s calls\n    }\n    executor = GenericK8sJobExecutor(**config)\n\n    # Mock the context and task\n    mock_task = Mock(spec=BaseTaskType)\n    executor._context = Mock()\n    executor._context.run_id = \"test-run-123\"\n    executor._context.get_job_callable_command.return_value = \"python test.py\"\n\n    # This should not raise an exception and should handle CronJob creation\n    executor.submit_k8s_cronjob(mock_task)\n\ndef test_cronjob_has_correct_schedule_and_job_template():\n    \"\"\"Test that the CronJob contains the correct schedule and wraps the job spec\"\"\"\n    config = {\n        \"job_spec\": {\n            \"template\": {\n                \"spec\": {\n                    \"container\": {\"image\": \"test-image\"}\n                }\n            }\n        },\n        \"schedule\": \"0 2 * * *\",\n        \"mock\": True\n    }\n    executor = GenericK8sJobExecutor(**config)\n    executor._context = Mock()\n    executor._context.run_id = \"test-run-123\"\n    executor._context.get_job_callable_command.return_value = \"python test.py\"\n\n    # Mock the Kubernetes client\n    with patch.object(executor, '_client') as mock_client:\n        mock_batch_api = Mock()\n        mock_client.BatchV1Api.return_value = mock_batch_api\n\n        mock_task = Mock(spec=BaseTaskType)\n        executor.submit_k8s_cronjob(mock_task)\n\n        # Verify CronJob creation was called\n        mock_batch_api.create_namespaced_cron_job.assert_called_once()\n\n        # Get the CronJob that was created\n        call_args = mock_batch_api.create_namespaced_cron_job.call_args\n        cronjob = call_args[1]['body']  # body parameter\n\n        assert cronjob.spec.schedule == \"0 2 * * *\"\n        assert cronjob.spec.job_template is not None\n</code></pre> <p>Step 2: Run test to verify it fails</p> <p>Run: <code>pytest tests/extensions/job_executor/test_k8s_scheduling.py::test_submit_k8s_cronjob_creates_cronjob_instead_of_job -v</code> Expected: FAIL with \"submit_k8s_cronjob method not found\"</p> <p>Step 3: Implement submit_k8s_cronjob method</p> <p>Add to <code>extensions/job_executor/k8s.py</code> in the GenericK8sJobExecutor class:</p> <pre><code>def submit_k8s_cronjob(self, task: BaseTaskType):\n    \"\"\"\n    Submit a Kubernetes CronJob instead of a regular Job.\n    Reuses the existing job spec building logic but wraps it in a CronJob.\n    \"\"\"\n    # Reuse existing volume mount logic\n    if self.job_spec.template.spec.container.volume_mounts:\n        self._volume_mounts += self.job_spec.template.spec.container.volume_mounts\n\n    container_volume_mounts = [\n        self._client.V1VolumeMount(**vol.model_dump())\n        for vol in self._volume_mounts\n    ]\n\n    assert isinstance(self._context, context.JobContext)\n    command = self._context.get_job_callable_command()\n\n    container_env = [\n        self._client.V1EnvVar(**env.model_dump())\n        for env in self.job_spec.template.spec.container.env\n    ]\n\n    # Build container (reuse existing logic)\n    base_container = self._client.V1Container(\n        command=shlex.split(command),\n        env=container_env,\n        name=\"default\",\n        volume_mounts=container_volume_mounts,\n        resources=self.job_spec.template.spec.container.resources.model_dump(\n            by_alias=True, exclude_none=True\n        ),\n        **self.job_spec.template.spec.container.model_dump(\n            exclude_none=True,\n            exclude={\"volume_mounts\", \"command\", \"env\", \"resources\"},\n        ),\n    )\n\n    # Build volumes (reuse existing logic)\n    if self.job_spec.template.spec.volumes:\n        self._volumes += self.job_spec.template.spec.volumes\n\n    spec_volumes = [\n        self._client.V1Volume(**vol.model_dump()) for vol in self._volumes\n    ]\n\n    tolerations = None\n    if self.job_spec.template.spec.tolerations:\n        tolerations = [\n            self._client.V1Toleration(**toleration.model_dump())\n            for toleration in self.job_spec.template.spec.tolerations\n        ]\n\n    # Build pod spec (reuse existing logic)\n    pod_spec = self._client.V1PodSpec(\n        containers=[base_container],\n        volumes=spec_volumes,\n        tolerations=tolerations,\n        **self.job_spec.template.spec.model_dump(\n            exclude_none=True, exclude={\"container\", \"volumes\", \"tolerations\"}\n        ),\n    )\n\n    pod_template_metadata = None\n    if self.job_spec.template.metadata:\n        pod_template_metadata = self._client.V1ObjectMeta(\n            **self.job_spec.template.metadata.model_dump(exclude_none=True)\n        )\n\n    pod_template = self._client.V1PodTemplateSpec(\n        spec=pod_spec,\n        metadata=pod_template_metadata,\n    )\n\n    # Build job spec (reuse existing logic)\n    job_spec = client.V1JobSpec(\n        template=pod_template,\n        **self.job_spec.model_dump(exclude_none=True, exclude={\"template\"}),\n    )\n\n    # Build CronJob spec (new part)\n    cronjob_spec = client.V1CronJobSpec(\n        schedule=self.schedule,\n        job_template=client.V1JobTemplateSpec(\n            spec=job_spec\n        )\n    )\n\n    # Build CronJob (new part)\n    cronjob = client.V1CronJob(\n        api_version=\"batch/v1\",\n        kind=\"CronJob\",\n        metadata=client.V1ObjectMeta(name=self._context.run_id),\n        spec=cronjob_spec,\n    )\n\n    logger.info(f\"Submitting CronJob: {cronjob.__dict__}\")\n    self._display_scheduled_job_info(cronjob)\n\n    if self.mock:\n        logger.info(cronjob.__dict__)\n        return\n\n    try:\n        k8s_batch = self._client.BatchV1Api()\n        response = k8s_batch.create_namespaced_cron_job(\n            body=cronjob,\n            namespace=self.namespace,\n        )\n        logger.debug(f\"Kubernetes CronJob response: {response}\")\n    except Exception as e:\n        logger.exception(e)\n        print(e)\n        raise\n\ndef _display_scheduled_job_info(self, cronjob):\n    \"\"\"Display information about the scheduled CronJob to the console\"\"\"\n    from runnable import console\n\n    console.print(\"\u2713 CronJob scheduled successfully\")\n    console.print(f\"  Name: {cronjob.metadata.name}\")\n    console.print(f\"  Namespace: {self.namespace}\")\n    console.print(f\"  Schedule: {cronjob.spec.schedule}\")\n    console.print(\"\")\n    console.print(\"  Job Spec:\")\n    console.print(f\"  - Image: {self.job_spec.template.spec.container.image}\")\n    console.print(f\"  - Resources: {self.job_spec.template.spec.container.resources.model_dump()}\")\n</code></pre> <p>Step 4: Run test to verify it passes</p> <p>Run: <code>pytest tests/extensions/job_executor/test_k8s_scheduling.py::test_submit_k8s_cronjob_creates_cronjob_instead_of_job -v</code> Expected: PASS</p> <p>Step 5: Run all scheduling tests</p> <p>Run: <code>pytest tests/extensions/job_executor/test_k8s_scheduling.py -v</code> Expected: PASS (all tests)</p> <p>Step 6: Commit</p> <pre><code>git add extensions/job_executor/k8s.py tests/extensions/job_executor/test_k8s_scheduling.py\ngit commit -m \"feat: implement CronJob creation method with schedule support\"\n</code></pre>"},{"location":"plans/2024-11-29-k8s-cronjob-scheduling/#task_3_update_submit_job_method_to_branch_on_schedule","title":"Task 3: Update submit_job Method to Branch on Schedule","text":"<p>Files: - Modify: <code>extensions/job_executor/k8s.py:191-204</code> (submit_job method) - Test: <code>tests/extensions/job_executor/test_k8s_scheduling.py</code> (add integration tests)</p> <p>Step 1: Write the failing integration test</p> <p>Add to <code>tests/extensions/job_executor/test_k8s_scheduling.py</code>:</p> <pre><code>def test_submit_job_creates_cronjob_when_schedule_present():\n    \"\"\"Test that submit_job calls submit_k8s_cronjob when schedule is configured\"\"\"\n    config = {\n        \"job_spec\": {\"template\": {\"spec\": {\"container\": {\"image\": \"test\"}}}},\n        \"schedule\": \"0 2 * * *\",\n        \"mock\": True\n    }\n    executor = GenericK8sJobExecutor(**config)\n    executor._context = Mock()\n    executor._context.run_id = \"test-run-123\"\n    executor._context.run_log_store = Mock()\n    executor._context.run_log_store.create_job_log.return_value = Mock()\n\n    # Mock the methods we'll call\n    executor._set_up_run_log = Mock()\n    executor._create_volumes = Mock()\n    executor.submit_k8s_cronjob = Mock()\n    executor.submit_k8s_job = Mock()\n\n    mock_task = Mock(spec=BaseTaskType)\n    executor.submit_job(mock_task, catalog_settings=[])\n\n    # Should call cronjob method, not regular job method\n    executor.submit_k8s_cronjob.assert_called_once_with(mock_task)\n    executor.submit_k8s_job.assert_not_called()\n\ndef test_submit_job_creates_regular_job_when_no_schedule():\n    \"\"\"Test that submit_job calls submit_k8s_job when schedule is not configured\"\"\"\n    config = {\n        \"job_spec\": {\"template\": {\"spec\": {\"container\": {\"image\": \"test\"}}}},\n        \"mock\": True\n    }\n    executor = GenericK8sJobExecutor(**config)\n    executor._context = Mock()\n    executor._context.run_id = \"test-run-123\"\n    executor._context.run_log_store = Mock()\n    executor._context.run_log_store.create_job_log.return_value = Mock()\n\n    # Mock the methods we'll call\n    executor._set_up_run_log = Mock()\n    executor._create_volumes = Mock()\n    executor.submit_k8s_cronjob = Mock()\n    executor.submit_k8s_job = Mock()\n\n    mock_task = Mock(spec=BaseTaskType)\n    executor.submit_job(mock_task, catalog_settings=[])\n\n    # Should call regular job method, not cronjob method\n    executor.submit_k8s_job.assert_called_once_with(mock_task)\n    executor.submit_k8s_cronjob.assert_not_called()\n</code></pre> <p>Step 2: Run test to verify it fails</p> <p>Run: <code>pytest tests/extensions/job_executor/test_k8s_scheduling.py::test_submit_job_creates_cronjob_when_schedule_present -v</code> Expected: FAIL because submit_job doesn't branch on schedule yet</p> <p>Step 3: Update submit_job method to branch on schedule</p> <p>In <code>extensions/job_executor/k8s.py</code>, modify the submit_job method:</p> <pre><code>def submit_job(self, job: BaseTaskType, catalog_settings=Optional[List[str]]):\n    \"\"\"\n    This method gets invoked by the CLI.\n    \"\"\"\n    self._set_up_run_log()\n\n    # Call the container job\n    job_log = self._context.run_log_store.create_job_log()\n    self._context.run_log_store.add_job_log(\n        run_id=self._context.run_id, job_log=job_log\n    )\n    # create volumes and volume mounts for the job\n    self._create_volumes()\n\n    # Branch based on whether scheduling is configured\n    if self.schedule:\n        self.submit_k8s_cronjob(job)\n    else:\n        self.submit_k8s_job(job)\n</code></pre> <p>Step 4: Run test to verify it passes</p> <p>Run: <code>pytest tests/extensions/job_executor/test_k8s_scheduling.py::test_submit_job_creates_cronjob_when_schedule_present -v</code> Expected: PASS</p> <p>Step 5: Run all scheduling tests</p> <p>Run: <code>pytest tests/extensions/job_executor/test_k8s_scheduling.py -v</code> Expected: PASS (all tests)</p> <p>Step 6: Test backward compatibility with existing tests</p> <p>Run: <code>pytest tests/extensions/job_executor/ -k \"k8s\" -v</code> Expected: PASS (existing K8s tests still work)</p> <p>Step 7: Commit</p> <pre><code>git add extensions/job_executor/k8s.py tests/extensions/job_executor/test_k8s_scheduling.py\ngit commit -m \"feat: branch submit_job to create CronJob when schedule is configured\"\n</code></pre>"},{"location":"plans/2024-11-29-k8s-cronjob-scheduling/#task_4_update_derived_classes_to_inherit_schedule_support","title":"Task 4: Update Derived Classes to Inherit Schedule Support","text":"<p>Files: - Verify: <code>extensions/job_executor/k8s.py:366-433</code> (MiniK8sJobExecutor class) - Verify: <code>extensions/job_executor/k8s.py:434-515</code> (K8sJobExecutor class) - Test: <code>tests/extensions/job_executor/test_k8s_scheduling.py</code> (add derived class tests)</p> <p>Step 1: Write tests for derived classes</p> <p>Add to <code>tests/extensions/job_executor/test_k8s_scheduling.py</code>:</p> <pre><code>from extensions.job_executor.k8s import MiniK8sJobExecutor, K8sJobExecutor\n\ndef test_mini_k8s_job_executor_inherits_schedule_support():\n    \"\"\"Test that MiniK8sJobExecutor supports scheduling\"\"\"\n    config = {\n        \"job_spec\": {\"template\": {\"spec\": {\"container\": {\"image\": \"test\"}}}},\n        \"schedule\": \"0 2 * * *\"\n    }\n    executor = MiniK8sJobExecutor(**config)\n    assert executor.schedule == \"0 2 * * *\"\n\ndef test_k8s_job_executor_inherits_schedule_support():\n    \"\"\"Test that K8sJobExecutor supports scheduling\"\"\"\n    config = {\n        \"job_spec\": {\"template\": {\"spec\": {\"container\": {\"image\": \"test\"}}}},\n        \"pvc_claim_name\": \"test-pvc\",\n        \"schedule\": \"0 2 * * *\"\n    }\n    executor = K8sJobExecutor(**config)\n    assert executor.schedule == \"0 2 * * *\"\n\ndef test_derived_classes_submit_job_calls_parent_implementation():\n    \"\"\"Test that derived classes use the parent's submit_job logic\"\"\"\n    # MiniK8sJobExecutor test\n    config = {\n        \"job_spec\": {\"template\": {\"spec\": {\"container\": {\"image\": \"test\"}}}},\n        \"schedule\": \"0 2 * * *\",\n        \"mock\": True\n    }\n    executor = MiniK8sJobExecutor(**config)\n    executor._context = Mock()\n    executor._context.run_id = \"test-run-123\"\n    executor._context.run_log_store = Mock()\n    executor._context.run_log_store.create_job_log.return_value = Mock()\n\n    executor._set_up_run_log = Mock()\n    executor._create_volumes = Mock()\n    executor.submit_k8s_cronjob = Mock()\n\n    mock_task = Mock(spec=BaseTaskType)\n    executor.submit_job(mock_task, catalog_settings=[])\n\n    # Should call the CronJob method since schedule is set\n    executor.submit_k8s_cronjob.assert_called_once_with(mock_task)\n</code></pre> <p>Step 2: Run test to verify derived classes work</p> <p>Run: <code>pytest tests/extensions/job_executor/test_k8s_scheduling.py::test_mini_k8s_job_executor_inherits_schedule_support -v</code> Expected: PASS (inheritance should work automatically)</p> <p>Step 3: Verify no changes needed to derived classes</p> <p>Check that MiniK8sJobExecutor and K8sJobExecutor inherit the schedule field and submit_job behavior automatically since they inherit from GenericK8sJobExecutor.</p> <p>Step 4: Run all derived class tests</p> <p>Run: <code>pytest tests/extensions/job_executor/test_k8s_scheduling.py -k \"derived\" -v</code> Expected: PASS (all derived class tests)</p> <p>Step 5: Commit (if any changes were needed)</p> <pre><code>git add tests/extensions/job_executor/test_k8s_scheduling.py\ngit commit -m \"test: verify derived K8s executors inherit schedule support\"\n</code></pre>"},{"location":"plans/2024-11-29-k8s-cronjob-scheduling/#task_5_add_example_configuration_file","title":"Task 5: Add Example Configuration File","text":"<p>Files: - Create: <code>examples/11-jobs/k8s-scheduled-job.yaml</code> - Test: Manual verification of configuration</p> <p>Step 1: Create example scheduled job configuration</p> <p>Create file: <code>examples/11-jobs/k8s-scheduled-job.yaml</code></p> <pre><code>job-executor:\n  type: \"k8s-job\"\n  config:\n    pvc_claim_name: runnable\n    config_path:\n    mock: false\n    namespace: enterprise-mlops\n    schedule: \"0 2 * * *\"  # Run daily at 2 AM\n    jobSpec:\n      activeDeadlineSeconds: 32000\n      template:\n        spec:\n          activeDeadlineSeconds: 86400\n          container:\n            image: harbor.csis.astrazeneca.net/mlops/runnable:latest\n            resources:\n              limits:\n                cpu: \"1\"\n                memory: \"2Gi\"\n              requests:\n                cpu: \"500m\"\n                memory: \"1Gi\"\n</code></pre> <p>Step 2: Create test to verify example config is valid</p> <p>Add to <code>tests/extensions/job_executor/test_k8s_scheduling.py</code>:</p> <pre><code>import yaml\nfrom pathlib import Path\n\ndef test_example_scheduled_config_is_valid():\n    \"\"\"Test that the example scheduled job config file is valid\"\"\"\n    config_path = Path(\"examples/11-jobs/k8s-scheduled-job.yaml\")\n    assert config_path.exists(), \"Example config file should exist\"\n\n    with open(config_path) as f:\n        config = yaml.safe_load(f)\n\n    job_executor_config = config[\"job-executor\"][\"config\"]\n\n    # Should be able to create executor with this config\n    executor = K8sJobExecutor(**job_executor_config)\n    assert executor.schedule == \"0 2 * * *\"\n    assert executor.pvc_claim_name == \"runnable\"\n</code></pre> <p>Step 3: Run test to verify example config works</p> <p>Run: <code>pytest tests/extensions/job_executor/test_k8s_scheduling.py::test_example_scheduled_config_is_valid -v</code> Expected: PASS</p> <p>Step 4: Commit example configuration</p> <pre><code>git add examples/11-jobs/k8s-scheduled-job.yaml tests/extensions/job_executor/test_k8s_scheduling.py\ngit commit -m \"docs: add example configuration for scheduled K8s jobs\"\n</code></pre>"},{"location":"plans/2024-11-29-k8s-cronjob-scheduling/#task_6_add_documentation","title":"Task 6: Add Documentation","text":"<p>Files: - Create: <code>docs/job-executors/k8s-scheduling.md</code> - Modify: <code>docs/job-executors/k8s.md</code> (add scheduling section if it exists)</p> <p>Step 1: Create scheduling documentation</p> <p>Create file: <code>docs/job-executors/k8s-scheduling.md</code></p> <pre><code># Kubernetes Job Scheduling\n\nThe Kubernetes job executors support optional cron-based scheduling using Kubernetes CronJobs.\n\n## Configuration\n\nAdd a `schedule` field to your K8s job executor configuration:\n\n```yaml\njob-executor:\n  type: \"k8s-job\"\n  config:\n    schedule: \"0 2 * * *\"  # Daily at 2 AM UTC\n    # ... other configuration\n</code></pre>"},{"location":"plans/2024-11-29-k8s-cronjob-scheduling/#schedule_format","title":"Schedule Format","text":"<p>The schedule field accepts standard cron expressions with 5 fields:</p> <ul> <li><code>minute</code> (0-59)</li> <li><code>hour</code> (0-23)</li> <li><code>day of month</code> (1-31)</li> <li><code>month</code> (1-12)</li> <li><code>day of week</code> (0-6, Sunday=0)</li> </ul>"},{"location":"plans/2024-11-29-k8s-cronjob-scheduling/#examples","title":"Examples","text":"<pre><code># Every day at 2 AM\nschedule: \"0 2 * * *\"\n\n# Every hour\nschedule: \"0 * * * *\"\n\n# Every Monday at 9 AM\nschedule: \"0 9 * * 1\"\n\n# Every 15 minutes\nschedule: \"*/15 * * * *\"\n</code></pre>"},{"location":"plans/2024-11-29-k8s-cronjob-scheduling/#behavior","title":"Behavior","text":""},{"location":"plans/2024-11-29-k8s-cronjob-scheduling/#when_schedule_is_present","title":"When Schedule is Present","text":"<ul> <li>Creates a Kubernetes CronJob instead of a regular Job</li> <li>Displays scheduled job information to console</li> <li>No immediate execution - job runs according to schedule</li> <li>Kubernetes handles the scheduling automatically</li> </ul>"},{"location":"plans/2024-11-29-k8s-cronjob-scheduling/#when_schedule_is_absent","title":"When Schedule is Absent","text":"<ul> <li>Normal behavior (immediate Job execution)</li> <li>Backward compatible with existing configurations</li> </ul>"},{"location":"plans/2024-11-29-k8s-cronjob-scheduling/#example","title":"Example","text":"<p>Complete configuration example:</p> <pre><code>job-executor:\n  type: \"k8s-job\"\n  config:\n    pvc_claim_name: runnable\n    config_path:\n    mock: false\n    namespace: enterprise-mlops\n    schedule: \"* * * * *\"  # Run daily at 2 AM UTC\n    jobSpec:\n      activeDeadlineSeconds: 32000\n      template:\n        spec:\n          activeDeadlineSeconds: 86400\n          container:\n            image: harbor.csis.astrazeneca.net/mlops/runnable:latest\n            resources:\n              limits:\n                cpu: \"1\"\n                memory: \"2Gi\"\n              requests:\n                cpu: \"500m\"\n                memory: \"1Gi\"\n</code></pre> <p>To schedule this job:</p> <pre><code>runnable execute pipeline.yaml --config k8s-scheduled-job.yaml\n</code></pre> <p>Output: <pre><code>\u2713 CronJob scheduled successfully\n  Name: run-20231129-143022-123\n  Namespace: enterprise-mlops\n  Schedule: 0 2 * * *\n\n  Job Spec:\n  - Image: harbor.csis.astrazeneca.net/mlops/runnable:latest\n  - Resources: {'limits': {'cpu': '1', 'memory': '2Gi'}}\n</code></pre></p>"},{"location":"plans/2024-11-29-k8s-cronjob-scheduling/#supported_executors","title":"Supported Executors","text":"<p>All K8s job executor variants support scheduling:</p> <ul> <li><code>K8sJobExecutor</code> - Production with PVC</li> <li><code>MiniK8sJobExecutor</code> - Local minikube development</li> <li><code>GenericK8sJobExecutor</code> - Base class</li> </ul>"},{"location":"plans/2024-11-29-k8s-cronjob-scheduling/#troubleshooting","title":"Troubleshooting","text":""},{"location":"plans/2024-11-29-k8s-cronjob-scheduling/#invalid_cron_expression","title":"Invalid Cron Expression","text":"<p><pre><code>ValidationError: Schedule must be a valid cron expression with 5 fields\n</code></pre> Ensure your cron expression has exactly 5 space-separated fields.</p>"},{"location":"plans/2024-11-29-k8s-cronjob-scheduling/#permission_issues","title":"Permission Issues","text":"<p><pre><code>Forbidden: CronJob creation not allowed in namespace\n</code></pre> Ensure your Kubernetes service account has permissions to create CronJobs in the target namespace. <pre><code>**Step 2: Add test to ensure documentation example works**\n\nAdd to `tests/extensions/job_executor/test_k8s_scheduling.py`:\n\n```python\ndef test_documentation_examples_are_valid():\n    \"\"\"Test that cron expressions in documentation are valid\"\"\"\n    examples = [\n        \"0 2 * * *\",    # Daily at 2 AM\n        \"0 * * * *\",    # Every hour\n        \"0 9 * * 1\",    # Monday at 9 AM\n        \"*/15 * * * *\"  # Every 15 minutes\n    ]\n\n    for schedule in examples:\n        config = {\n            \"job_spec\": {\"template\": {\"spec\": {\"container\": {\"image\": \"test\"}}}},\n            \"schedule\": schedule\n        }\n        executor = GenericK8sJobExecutor(**config)\n        assert executor.schedule == schedule\n</code></pre></p> <p>Step 3: Run documentation tests</p> <p>Run: <code>pytest tests/extensions/job_executor/test_k8s_scheduling.py::test_documentation_examples_are_valid -v</code> Expected: PASS</p> <p>Step 4: Commit documentation</p> <pre><code>git add docs/job-executors/k8s-scheduling.md tests/extensions/job_executor/test_k8s_scheduling.py\ngit commit -m \"docs: add comprehensive K8s job scheduling documentation\"\n</code></pre>"},{"location":"plans/2024-11-29-k8s-cronjob-scheduling/#task_7_integration_testing_and_validation","title":"Task 7: Integration Testing and Validation","text":"<p>Files: - Test: <code>tests/extensions/job_executor/test_k8s_scheduling.py</code> (add end-to-end tests) - Verify: Run full test suite</p> <p>Step 1: Add comprehensive integration tests</p> <p>Add to <code>tests/extensions/job_executor/test_k8s_scheduling.py</code>:</p> <pre><code>def test_end_to_end_scheduled_job_creation():\n    \"\"\"Test complete flow from configuration to CronJob creation\"\"\"\n    config = {\n        \"job_spec\": {\n            \"activeDeadlineSeconds\": 3600,\n            \"template\": {\n                \"spec\": {\n                    \"activeDeadlineSeconds\": 3600,\n                    \"restartPolicy\": \"Never\",\n                    \"container\": {\n                        \"image\": \"python:3.9\",\n                        \"env\": [{\"name\": \"TEST\", \"value\": \"value\"}],\n                        \"resources\": {\n                            \"limits\": {\"cpu\": \"1\", \"memory\": \"1Gi\"},\n                            \"requests\": {\"cpu\": \"500m\", \"memory\": \"512Mi\"}\n                        }\n                    }\n                }\n            }\n        },\n        \"schedule\": \"0 3 * * *\",\n        \"namespace\": \"test-namespace\",\n        \"mock\": True\n    }\n\n    executor = GenericK8sJobExecutor(**config)\n    executor._context = Mock()\n    executor._context.run_id = \"integration-test-run\"\n    executor._context.get_job_callable_command.return_value = \"python main.py\"\n\n    with patch.object(executor, '_client') as mock_client:\n        mock_batch_api = Mock()\n        mock_client.BatchV1Api.return_value = mock_batch_api\n\n        mock_task = Mock(spec=BaseTaskType)\n        executor.submit_k8s_cronjob(mock_task)\n\n        # Verify the call was made\n        mock_batch_api.create_namespaced_cron_job.assert_called_once()\n\n        # Verify the CronJob structure\n        call_args = mock_batch_api.create_namespaced_cron_job.call_args\n        cronjob = call_args[1]['body']\n\n        assert cronjob.kind == \"CronJob\"\n        assert cronjob.api_version == \"batch/v1\"\n        assert cronjob.metadata.name == \"integration-test-run\"\n        assert cronjob.spec.schedule == \"0 3 * * *\"\n\n        # Verify job template contains our job spec\n        job_template = cronjob.spec.job_template\n        assert job_template.spec.active_deadline_seconds == 3600\n        assert job_template.spec.template.spec.restart_policy == \"Never\"\n\ndef test_error_handling_in_cronjob_creation():\n    \"\"\"Test that CronJob creation errors are handled properly\"\"\"\n    config = {\n        \"job_spec\": {\"template\": {\"spec\": {\"container\": {\"image\": \"test\"}}}},\n        \"schedule\": \"0 2 * * *\",\n        \"mock\": False  # Use real mode to test error handling\n    }\n\n    executor = GenericK8sJobExecutor(**config)\n    executor._context = Mock()\n    executor._context.run_id = \"error-test-run\"\n    executor._context.get_job_callable_command.return_value = \"python test.py\"\n\n    with patch.object(executor, '_client') as mock_client:\n        mock_batch_api = Mock()\n        mock_client.BatchV1Api.return_value = mock_batch_api\n\n        # Simulate K8s API error\n        from kubernetes.client.exceptions import ApiException\n        mock_batch_api.create_namespaced_cron_job.side_effect = ApiException(\n            status=403, reason=\"Forbidden\"\n        )\n\n        mock_task = Mock(spec=BaseTaskType)\n\n        # Should re-raise the exception\n        with pytest.raises(ApiException):\n            executor.submit_k8s_cronjob(mock_task)\n</code></pre> <p>Step 2: Run integration tests</p> <p>Run: <code>pytest tests/extensions/job_executor/test_k8s_scheduling.py::test_end_to_end_scheduled_job_creation -v</code> Expected: PASS</p> <p>Step 3: Run full test suite for K8s executors</p> <p>Run: <code>pytest tests/extensions/job_executor/ -k \"k8s\" -v</code> Expected: PASS (all existing and new tests)</p> <p>Step 4: Run project test suite to ensure no regressions</p> <p>Run: <code>pytest tests/ -x</code> Expected: PASS (no breaking changes to other components)</p> <p>Step 5: Manual testing with mock mode</p> <p>Create a simple test script to verify console output:</p> <pre><code># test_console_output.py\nfrom extensions.job_executor.k8s import GenericK8sJobExecutor\nfrom unittest.mock import Mock\n\nconfig = {\n    \"job_spec\": {\"template\": {\"spec\": {\"container\": {\"image\": \"test:latest\"}}}},\n    \"schedule\": \"0 2 * * *\",\n    \"mock\": True\n}\n\nexecutor = GenericK8sJobExecutor(**config)\nexecutor._context = Mock()\nexecutor._context.run_id = \"manual-test-run\"\nexecutor._context.get_job_callable_command.return_value = \"python pipeline.py\"\nexecutor._context.run_log_store = Mock()\nexecutor._context.run_log_store.create_job_log.return_value = Mock()\n\nmock_task = Mock()\nprint(\"Testing scheduled job console output:\")\nexecutor.submit_job(mock_task, catalog_settings=[])\n</code></pre> <p>Run: <code>python test_console_output.py</code> Expected: Console output showing scheduled job information</p> <p>Step 6: Commit integration tests</p> <pre><code>git add tests/extensions/job_executor/test_k8s_scheduling.py\ngit commit -m \"test: add comprehensive integration tests for K8s job scheduling\"\n</code></pre>"},{"location":"plans/2024-11-29-k8s-cronjob-scheduling/#task_8_final_verification_and_documentation_update","title":"Task 8: Final Verification and Documentation Update","text":"<p>Files: - Verify: All tests pass - Update: Any main project documentation if needed</p> <p>Step 1: Run complete test suite</p> <p>Run: <code>pytest</code> Expected: PASS (all tests including new scheduling tests)</p> <p>Step 2: Verify example configuration works</p> <p>Run: `python -c \" import yaml from extensions.job_executor.k8s import K8sJobExecutor</p> <p>with open('examples/11-jobs/k8s-scheduled-job.yaml') as f:     config = yaml.safe_load(f)</p> <p>executor = K8sJobExecutor(**config['job-executor']['config']) print(f'Schedule: {executor.schedule}') print('Configuration validation: PASSED') \"` Expected: Successful configuration loading</p> <p>Step 3: Check for any missing imports</p> <p>Verify that all necessary imports are present in <code>extensions/job_executor/k8s.py</code>:</p> <pre><code>import logging\nimport re\nimport shlex\nfrom enum import Enum\nfrom typing import Annotated, List, Optional\n\nfrom kubernetes import client\nfrom kubernetes import config as k8s_config\nfrom pydantic import BaseModel, ConfigDict, Field, PlainSerializer, PrivateAttr, field_validator\n</code></pre> <p>Step 4: Update main README or documentation if needed</p> <p>Check if the main project documentation mentions job executors and add a reference to the new scheduling capability if appropriate.</p> <p>Step 5: Final commit</p> <pre><code>git add .\ngit commit -m \"feat: complete K8s CronJob scheduling implementation\n\n- Add optional schedule field to all K8s job executors\n- Create CronJobs instead of Jobs when schedule is configured\n- Maintain full backward compatibility\n- Include comprehensive tests and documentation\n- Add example configuration file\"\n</code></pre> <p>Step 6: Create summary of changes</p> <p>The implementation adds:</p> <ol> <li>Configuration: Optional <code>schedule</code> field with cron validation</li> <li>CronJob Support: Creates Kubernetes CronJobs when scheduled</li> <li>Console Output: Shows scheduled job information</li> <li>Backward Compatibility: No breaking changes to existing functionality</li> <li>Documentation: Complete usage guide and examples</li> <li>Testing: Comprehensive test coverage including edge cases</li> </ol> <p>Verification Commands: <pre><code># Run all tests\npytest\n\n# Test new scheduling functionality specifically\npytest tests/extensions/job_executor/test_k8s_scheduling.py -v\n\n# Verify no regressions in existing K8s functionality\npytest tests/extensions/job_executor/ -k \"k8s\" -v\n\n# Validate example configuration\npython -c \"import yaml; from extensions.job_executor.k8s import K8sJobExecutor; K8sJobExecutor(**yaml.safe_load(open('examples/11-jobs/k8s-scheduled-job.yaml'))['job-executor']['config'])\"\n</code></pre></p>"},{"location":"plans/2024-11-29-k8s-cronjob-scheduling/#plan_summary","title":"Plan Summary","text":"<p>This implementation adds optional cron scheduling to Kubernetes job executors through:</p> <ol> <li>Schedule Configuration Field - Optional cron expression field with validation</li> <li>CronJob Creation Logic - Method to create K8s CronJobs instead of Jobs</li> <li>Execution Flow Branching - submit_job method branches based on schedule presence</li> <li>Inheritance Support - All derived executor classes automatically support scheduling</li> <li>Example Configuration - Complete working example with documentation</li> <li>Comprehensive Documentation - Usage guide with examples and troubleshooting</li> <li>Integration Testing - End-to-end tests ensuring reliability</li> <li>Final Verification - Complete test suite validation</li> </ol> <p>The implementation maintains full backward compatibility while adding powerful scheduling capabilities that leverage Kubernetes' native CronJob functionality.</p>"},{"location":"plans/2025-01-03-fastapi-llm-streaming-design/","title":"FastAPI LLM Streaming Design","text":""},{"location":"plans/2025-01-03-fastapi-llm-streaming-design/#overview","title":"Overview","text":"<p>Add a new FastAPI example demonstrating native async streaming from pipeline functions to SSE clients, mocking an LLM call with token-by-token streaming.</p>"},{"location":"plans/2025-01-03-fastapi-llm-streaming-design/#context","title":"Context","text":"<ul> <li>Existing telemetry example: Uses sync pipelines + thread pool executor + Queue for SSE streaming</li> <li>New async capability: <code>AsyncPipeline</code> + <code>AsyncPythonTask</code> support native async execution</li> <li>Goal: Stream domain data (LLM tokens) directly from async functions to clients</li> </ul>"},{"location":"plans/2025-01-03-fastapi-llm-streaming-design/#key_design_decisions","title":"Key Design Decisions","text":""},{"location":"plans/2025-01-03-fastapi-llm-streaming-design/#telemetry_vs_llm_streaming_separate_concerns","title":"Telemetry vs LLM Streaming: Separate Concerns","text":"Concern Purpose Mechanism Scope Telemetry Infrastructure observability <code>logfire.span()</code>, <code>set_stream_queue()</code> All executors LLM Streaming Domain data to clients <code>event_callback</code> parameter Local async only <p>These are independent paths that do not interfere with each other.</p>"},{"location":"plans/2025-01-03-fastapi-llm-streaming-design/#streaming_pattern_asyncgenerator_pass-through","title":"Streaming Pattern: AsyncGenerator Pass-through","text":"<p>The <code>execute_streaming()</code> method returns an <code>AsyncGenerator</code> that any async code can consume:</p> <pre><code>async for event in pipeline.execute_streaming():\n    # FastAPI SSE, WebSocket, CLI, Jupyter, tests, etc.\n</code></pre> <p>This is the standard Python async iteration protocol - not FastAPI-specific.</p>"},{"location":"plans/2025-01-03-fastapi-llm-streaming-design/#implementation_callback_queue_asyncgenerator_wrapper","title":"Implementation: Callback \u2192 Queue \u2192 AsyncGenerator Wrapper","text":"<p>Minimal changes (~20 lines) by wrapping existing callback mechanism:</p> <pre><code>async def execute_streaming(self):\n    queue = asyncio.Queue()\n\n    async def run():\n        await self.execute(event_callback=queue.put_nowait)\n        await queue.put(None)  # sentinel\n\n    task = asyncio.create_task(run())\n    while (event := await queue.get()) is not None:\n        yield event\n</code></pre> <p>The queue is an internal implementation detail. The public API is a clean AsyncGenerator.</p>"},{"location":"plans/2025-01-03-fastapi-llm-streaming-design/#architecture","title":"Architecture","text":""},{"location":"plans/2025-01-03-fastapi-llm-streaming-design/#new_example_structure","title":"New Example Structure","text":"<pre><code>examples/fastapi-llm/\n\u251c\u2500\u2500 main.py          # FastAPI app with SSE endpoint\n\u251c\u2500\u2500 pipelines.py     # AsyncPipeline definitions\n\u2514\u2500\u2500 llm_mock.py      # Mock LLM async generator functions\n</code></pre>"},{"location":"plans/2025-01-03-fastapi-llm-streaming-design/#data_flow","title":"Data Flow","text":"<pre><code>Client POST /chat\n    \u2193\nFastAPI async endpoint\n    \u2193\npipeline.execute_streaming()\n    \u2193\nAsyncPipeline with event_callback\n    \u2193\nAsyncPythonTask runs mock_llm_stream()\n    \u2193\nAsyncGenerator yields {\"type\": \"chunk\", \"text\": \"...\"}\n    \u2193\nevent_callback pushes to internal queue\n    \u2193\nexecute_streaming() yields from queue\n    \u2193\nSSE streams to client\n</code></pre>"},{"location":"plans/2025-01-03-fastapi-llm-streaming-design/#event_structure","title":"Event Structure","text":"<p>Simple typed events (inspired by Anthropic's streaming API):</p> <pre><code>{\"type\": \"status\", \"status\": \"thinking\"}   # Progress\n{\"type\": \"chunk\", \"text\": \"Hello\"}         # Token\n{\"type\": \"chunk\", \"text\": \" world\"}        # Token\n{\"type\": \"done\", \"full_text\": \"Hello world\"}  # Completion\n</code></pre>"},{"location":"plans/2025-01-03-fastapi-llm-streaming-design/#components","title":"Components","text":""},{"location":"plans/2025-01-03-fastapi-llm-streaming-design/#1_mock_llm_function_llm_mockpy","title":"1. Mock LLM Function (<code>llm_mock.py</code>)","text":"<pre><code>async def mock_llm_stream(prompt: str) -&gt; AsyncGenerator[dict, None]:\n    yield {\"type\": \"status\", \"status\": \"thinking\"}\n    await asyncio.sleep(0.3)\n\n    response = f\"Response to: {prompt[:50]}...\"\n\n    yield {\"type\": \"status\", \"status\": \"generating\"}\n\n    for word in response.split():\n        yield {\"type\": \"chunk\", \"text\": word + \" \"}\n        await asyncio.sleep(0.05)\n\n    yield {\"type\": \"done\", \"full_text\": response}\n</code></pre>"},{"location":"plans/2025-01-03-fastapi-llm-streaming-design/#2_pipeline_definition_pipelinespy","title":"2. Pipeline Definition (<code>pipelines.py</code>)","text":"<pre><code>def chat_pipeline(prompt: str) -&gt; AsyncPipeline:\n    return AsyncPipeline(\n        name=\"chat\",\n        steps=[\n            AsyncPythonTask(\n                name=\"llm_response\",\n                function=mock_llm_stream,\n                returns=[\"full_text\"],\n            )\n        ],\n    )\n</code></pre>"},{"location":"plans/2025-01-03-fastapi-llm-streaming-design/#3_fastapi_endpoint_mainpy","title":"3. FastAPI Endpoint (<code>main.py</code>)","text":"<pre><code>@app.post(\"/chat\")\nasync def chat(request: ChatRequest):\n    async def event_stream():\n        pipeline = chat_pipeline(request.prompt)\n        async for event in pipeline.execute_streaming():\n            yield f\"data: {json.dumps(event)}\\n\\n\"\n\n    return StreamingResponse(event_stream(), media_type=\"text/event-stream\")\n</code></pre>"},{"location":"plans/2025-01-03-fastapi-llm-streaming-design/#4_sdk_changes_runnablesdkpy","title":"4. SDK Changes (<code>runnable/sdk.py</code>)","text":"<p>Add <code>execute_streaming()</code> method to <code>AsyncPipeline</code>:</p> <pre><code>async def execute_streaming(self, ...):\n    \"\"\"Execute pipeline and yield events as AsyncGenerator.\"\"\"\n    queue = asyncio.Queue()\n\n    async def run():\n        await self.execute(..., event_callback=queue.put_nowait)\n        await queue.put(None)\n\n    task = asyncio.create_task(run())\n    while (event := await queue.get()) is not None:\n        yield event\n</code></pre>"},{"location":"plans/2025-01-03-fastapi-llm-streaming-design/#changes_required","title":"Changes Required","text":""},{"location":"plans/2025-01-03-fastapi-llm-streaming-design/#sdk_runnablesdkpy","title":"SDK (<code>runnable/sdk.py</code>)","text":"<ul> <li>Add <code>execute_streaming()</code> method to <code>AsyncPipeline</code> class</li> </ul>"},{"location":"plans/2025-01-03-fastapi-llm-streaming-design/#context_runnablecontextpy","title":"Context (<code>runnable/context.py</code>)","text":"<ul> <li>Add <code>event_callback</code> parameter threading through <code>AsyncPipelineContext.execute_async()</code></li> </ul>"},{"location":"plans/2025-01-03-fastapi-llm-streaming-design/#executor_extensionspipeline_executor_init_py","title":"Executor (<code>extensions/pipeline_executor/__init__.py</code>)","text":"<ul> <li>Add <code>event_callback</code> parameter to async execution methods</li> </ul>"},{"location":"plans/2025-01-03-fastapi-llm-streaming-design/#new_files","title":"New Files","text":"<ul> <li><code>examples/fastapi-llm/main.py</code></li> <li><code>examples/fastapi-llm/pipelines.py</code></li> <li><code>examples/fastapi-llm/llm_mock.py</code></li> </ul>"},{"location":"plans/2025-01-03-fastapi-llm-streaming-design/#testing","title":"Testing","text":"<ul> <li>Run FastAPI server: <code>uvicorn examples.fastapi_llm.main:app --reload</code></li> <li>Test with curl: <code>curl -N -X POST http://localhost:8000/chat -H \"Content-Type: application/json\" -d '{\"prompt\": \"Hello\"}'</code></li> <li>Verify events stream token-by-token</li> </ul>"},{"location":"plans/2025-01-03-fastapi-llm-streaming-design/#non-goals","title":"Non-Goals","text":"<ul> <li>Real LLM integration (mock only)</li> <li>Telemetry queue changes (separate concern)</li> <li>Non-local executor support for streaming</li> </ul>"},{"location":"plans/2025-01-03-fastapi-llm-streaming-plan/","title":"FastAPI LLM Streaming Implementation Plan","text":"<p>For Claude: REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.</p> <p>Goal: Add <code>execute_streaming()</code> to AsyncPipeline and create a FastAPI example demonstrating LLM token streaming.</p> <p>Architecture: Add <code>_event_callback</code> attribute to executor, set at execution start. Task types access callback via executor. Wrap with AsyncGenerator at SDK level.</p> <p>Tech Stack: Python asyncio, FastAPI, SSE (Server-Sent Events), AsyncGenerator</p>"},{"location":"plans/2025-01-03-fastapi-llm-streaming-plan/#task_1_add_event_callback_attribute_to_basepipelineexecutor","title":"Task 1: Add _event_callback attribute to BasePipelineExecutor","text":"<p>Files:</p> <ul> <li>Modify: <code>runnable/executor.py:184-188</code></li> </ul> <p>Step 1: Add the _event_callback PrivateAttr</p> <p>After line 188 (<code>_context_node</code>), add:</p> <pre><code>_event_callback: Optional[Callable[[dict], None]] = PrivateAttr(default=None)\n</code></pre> <p>Step 2: Update imports</p> <p>At top of file, update the typing import:</p> <pre><code>from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional\n</code></pre> <p>Step 3: Run tests</p> <p>Run: <code>uv run pytest tests/ -k \"executor\" -v --tb=short</code> Expected: Tests pass (no behavior change yet)</p> <p>Step 4: Commit</p> <pre><code>git add runnable/executor.py\ngit commit -m \"feat: add _event_callback attribute to BasePipelineExecutor\"\n</code></pre>"},{"location":"plans/2025-01-03-fastapi-llm-streaming-plan/#task_2_use_executors_event_callback_in_tasknodeexecute_async","title":"Task 2: Use executor's event_callback in TaskNode.execute_async","text":"<p>Files:</p> <ul> <li>Modify: <code>extensions/nodes/task.py:101-144</code></li> </ul> <p>Step 1: Update execute_async to use executor's callback</p> <p>Replace the <code>execute_async</code> method:</p> <pre><code>async def execute_async(\n    self,\n    map_variable: MapVariableType = None,\n    attempt_number: int = 1,\n    mock: bool = False,\n) -&gt; StepLog:\n    \"\"\"Async task execution with fallback to sync.\"\"\"\n    step_log = self._context.run_log_store.get_step_log(\n        self._get_step_log_name(map_variable), self._context.run_id\n    )\n\n    if not mock:\n        # Get event_callback from executor\n        event_callback = self._context.pipeline_executor._event_callback\n\n        # Try async first, fall back to sync\n        try:\n            attempt_log = await self.executable.execute_command_async(\n                map_variable=map_variable,\n                event_callback=event_callback,\n            )\n        except NotImplementedError:\n            # Task doesn't support async, fall back to sync\n            attempt_log = self.executable.execute_command(map_variable=map_variable)\n\n        attempt_log.attempt_number = attempt_number\n        attempt_log.retry_indicator = self._context.retry_indicator\n    else:\n        attempt_log = datastore.StepAttempt(\n            status=defaults.SUCCESS,\n            start_time=str(datetime.now()),\n            end_time=str(datetime.now()),\n            attempt_number=attempt_number,\n            retry_indicator=self._context.retry_indicator,\n        )\n\n    # Add code identities to the attempt\n    self._context.pipeline_executor.add_code_identities(\n        node=self, attempt_log=attempt_log\n    )\n\n    logger.info(f\"attempt_log: {attempt_log}\")\n    logger.info(f\"Step {self.name} completed with status: {attempt_log.status}\")\n\n    step_log.status = attempt_log.status\n    step_log.attempts.append(attempt_log)\n\n    return step_log\n</code></pre> <p>Step 2: Run tests</p> <p>Run: <code>uv run pytest tests/ -k \"async\" -v --tb=short</code> Expected: All async tests pass</p> <p>Step 3: Commit</p> <pre><code>git add extensions/nodes/task.py\ngit commit -m \"feat: TaskNode.execute_async uses executor's event_callback\"\n</code></pre>"},{"location":"plans/2025-01-03-fastapi-llm-streaming-plan/#task_3_add_execute_streaming_to_asyncpipeline","title":"Task 3: Add execute_streaming to AsyncPipeline","text":"<p>Files:</p> <ul> <li>Modify: <code>runnable/sdk.py</code> (after line 1116, end of AsyncPipeline.execute)</li> </ul> <p>Step 1: Add asyncio import</p> <p>At top of file, add to imports:</p> <pre><code>import asyncio\n</code></pre> <p>Step 2: Add execute_streaming method</p> <p>After the <code>execute</code> method in <code>AsyncPipeline</code> class, add:</p> <pre><code>async def execute_streaming(\n    self,\n    configuration_file: str = \"\",\n    run_id: str = \"\",\n    tag: str = \"\",\n    parameters_file: str = \"\",\n    log_level: str = defaults.LOG_LEVEL,\n):\n    \"\"\"\n    Execute the async pipeline and yield events as an AsyncGenerator.\n\n    This method allows streaming events from AsyncGenerator functions\n    directly to the caller, enabling patterns like SSE streaming.\n\n    Usage:\n        async for event in pipeline.execute_streaming():\n            print(event)\n\n    Yields:\n        dict: Events yielded by AsyncGenerator functions in the pipeline.\n    \"\"\"\n    from runnable import context\n\n    logger.setLevel(log_level)\n\n    service_configurations = context.ServiceConfigurations(\n        configuration_file=configuration_file,\n        execution_context=context.ExecutionContext.PIPELINE,\n    )\n\n    configurations = {\n        \"dag\": self.return_dag(),\n        \"parameters_file\": parameters_file,\n        \"tag\": tag,\n        \"run_id\": run_id,\n        \"configuration_file\": configuration_file,\n        **service_configurations.services,\n    }\n\n    run_context = context.AsyncPipelineContext.model_validate(configurations)\n    context.set_run_context(run_context)\n\n    # Use asyncio.Queue to bridge callback to AsyncGenerator\n    queue: asyncio.Queue = asyncio.Queue()\n\n    # Set the callback on the executor\n    run_context.pipeline_executor._event_callback = queue.put_nowait\n\n    async def run_pipeline():\n        try:\n            await run_context.execute_async()\n        finally:\n            await queue.put(None)  # Sentinel to signal completion\n\n    # Start pipeline execution in background\n    task = asyncio.create_task(run_pipeline())\n\n    # Yield events as they arrive\n    while True:\n        event = await queue.get()\n        if event is None:\n            break\n        yield event\n\n    # Ensure task completed (will raise if there was an exception)\n    await task\n</code></pre> <p>Step 3: Run tests</p> <p>Run: <code>uv run pytest tests/test_pipeline_examples.py -k \"async\" -v</code> Expected: Async tests pass</p> <p>Step 4: Commit</p> <pre><code>git add runnable/sdk.py\ngit commit -m \"feat: add execute_streaming method to AsyncPipeline\"\n</code></pre>"},{"location":"plans/2025-01-03-fastapi-llm-streaming-plan/#task_4_create_mock_llm_function","title":"Task 4: Create mock LLM function","text":"<p>Files:</p> <ul> <li>Create: <code>examples/fastapi_llm/__init__.py</code></li> <li>Create: <code>examples/fastapi_llm/llm_mock.py</code></li> </ul> <p>Step 1: Create package init</p> <p>Create <code>examples/fastapi_llm/__init__.py</code>:</p> <pre><code>\"\"\"FastAPI LLM streaming example.\"\"\"\n</code></pre> <p>Step 2: Create llm_mock.py</p> <p>Create <code>examples/fastapi_llm/llm_mock.py</code>:</p> <pre><code>\"\"\"Mock LLM functions for demonstrating async streaming.\"\"\"\n\nimport asyncio\nfrom typing import AsyncGenerator\n\n\nasync def mock_llm_stream(prompt: str) -&gt; AsyncGenerator[dict, None]:\n    \"\"\"\n    Mock LLM that streams responses token-by-token.\n\n    This demonstrates the AsyncGenerator pattern for streaming events\n    through the pipeline to SSE clients.\n\n    Args:\n        prompt: The user's input prompt\n\n    Yields:\n        dict: Events in the format:\n            - {\"type\": \"status\", \"status\": \"thinking\"} - Processing status\n            - {\"type\": \"chunk\", \"text\": \"...\"} - Token/word chunk\n            - {\"type\": \"done\", \"full_text\": \"...\"} - Completion with full response\n    \"\"\"\n    # Simulate initial \"thinking\" delay\n    yield {\"type\": \"status\", \"status\": \"thinking\"}\n    await asyncio.sleep(0.3)\n\n    # Build response based on prompt\n    prompt_preview = prompt[:50] + (\"...\" if len(prompt) &gt; 50 else \"\")\n    response = (\n        f\"I received your prompt: '{prompt_preview}'. \"\n        \"Here is my simulated response with multiple words to demonstrate \"\n        \"token-by-token streaming in action.\"\n    )\n\n    yield {\"type\": \"status\", \"status\": \"generating\"}\n    await asyncio.sleep(0.1)\n\n    # Stream word by word\n    words = response.split()\n    for i, word in enumerate(words):\n        text = word + (\" \" if i &lt; len(words) - 1 else \"\")\n        yield {\"type\": \"chunk\", \"text\": text}\n        await asyncio.sleep(0.05)  # 50ms per token\n\n    yield {\"type\": \"done\", \"full_text\": response}\n</code></pre> <p>Step 3: Commit</p> <pre><code>git add examples/fastapi_llm/\ngit commit -m \"feat: add mock LLM streaming functions\"\n</code></pre>"},{"location":"plans/2025-01-03-fastapi-llm-streaming-plan/#task_5_create_pipeline_definitions","title":"Task 5: Create pipeline definitions","text":"<p>Files:</p> <ul> <li>Create: <code>examples/fastapi_llm/pipelines.py</code></li> </ul> <p>Step 1: Create pipelines.py</p> <pre><code>\"\"\"Pipeline definitions for FastAPI LLM streaming example.\"\"\"\n\nfrom runnable import AsyncPipeline, AsyncPythonTask\n\nfrom examples.fastapi_llm.llm_mock import mock_llm_stream\n\n\ndef chat_pipeline() -&gt; AsyncPipeline:\n    \"\"\"\n    Single-task pipeline that streams LLM response.\n\n    The mock_llm_stream function is an AsyncGenerator that yields\n    events which flow through the pipeline to the SSE client.\n    \"\"\"\n    return AsyncPipeline(\n        name=\"chat\",\n        steps=[\n            AsyncPythonTask(\n                name=\"llm_response\",\n                function=mock_llm_stream,\n                returns=[\"full_text\"],\n            )\n        ],\n    )\n</code></pre> <p>Step 2: Commit</p> <pre><code>git add examples/fastapi_llm/pipelines.py\ngit commit -m \"feat: add AsyncPipeline definition for LLM streaming\"\n</code></pre>"},{"location":"plans/2025-01-03-fastapi-llm-streaming-plan/#task_6_create_fastapi_application","title":"Task 6: Create FastAPI application","text":"<p>Files:</p> <ul> <li>Create: <code>examples/fastapi_llm/main.py</code></li> </ul> <p>Step 1: Create main.py</p> <pre><code>\"\"\"\nFastAPI LLM streaming example.\n\nDemonstrates native async streaming from AsyncPipeline functions to SSE clients.\n\nStart the server:\n    uv run uvicorn examples.fastapi_llm.main:app --reload\n\nTest with curl:\n    curl -N -X POST http://localhost:8000/chat \\\n        -H \"Content-Type: application/json\" \\\n        -d '{\"prompt\": \"Hello, how are you?\"}'\n\"\"\"\n\nimport json\nimport os\n\nfrom fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse\nfrom pydantic import BaseModel\n\nfrom examples.fastapi_llm.pipelines import chat_pipeline\n\napp = FastAPI(title=\"Async LLM Streaming Demo\")\n\n\nclass ChatRequest(BaseModel):\n    prompt: str\n\n\n@app.get(\"/\")\nasync def root():\n    \"\"\"Health check and usage info.\"\"\"\n    return {\n        \"status\": \"ok\",\n        \"endpoints\": {\n            \"/chat\": \"POST - Stream LLM response via SSE\",\n        },\n    }\n\n\n@app.post(\"/chat\")\nasync def chat(request: ChatRequest):\n    \"\"\"\n    Stream LLM response via Server-Sent Events.\n\n    The AsyncPipeline's execute_streaming() yields events from the\n    AsyncGenerator function, which are streamed directly to the client.\n    \"\"\"\n\n    async def event_stream():\n        # Set prompt as environment variable for pipeline parameter\n        os.environ[\"RUNNABLE_PRM_prompt\"] = request.prompt\n\n        try:\n            pipeline = chat_pipeline()\n            async for event in pipeline.execute_streaming():\n                yield f\"data: {json.dumps(event)}\\n\\n\"\n        finally:\n            os.environ.pop(\"RUNNABLE_PRM_prompt\", None)\n\n    return StreamingResponse(\n        event_stream(),\n        media_type=\"text/event-stream\",\n        headers={\"Cache-Control\": \"no-cache\", \"X-Accel-Buffering\": \"no\"},\n    )\n\n\nif __name__ == \"__main__\":\n    import uvicorn\n\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n</code></pre> <p>Step 2: Commit</p> <pre><code>git add examples/fastapi_llm/main.py\ngit commit -m \"feat: add FastAPI app with SSE streaming endpoint\"\n</code></pre>"},{"location":"plans/2025-01-03-fastapi-llm-streaming-plan/#task_7_manual_integration_test","title":"Task 7: Manual integration test","text":"<p>Step 1: Start the server</p> <p>Run in one terminal: <pre><code>uv run uvicorn examples.fastapi_llm.main:app --reload\n</code></pre> Expected: Server starts on http://localhost:8000</p> <p>Step 2: Test streaming endpoint</p> <p>In another terminal: <pre><code>curl -N -X POST http://localhost:8000/chat \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"prompt\": \"Hello, how are you?\"}'\n</code></pre></p> <p>Expected output (streamed line by line): <pre><code>data: {\"type\": \"status\", \"status\": \"thinking\"}\n\ndata: {\"type\": \"status\", \"status\": \"generating\"}\n\ndata: {\"type\": \"chunk\", \"text\": \"I \"}\n\ndata: {\"type\": \"chunk\", \"text\": \"received \"}\n\n... (more chunks)\n\ndata: {\"type\": \"done\", \"full_text\": \"I received your prompt: ...\"}\n</code></pre></p> <p>Step 3: Stop server</p> <p>Ctrl+C to stop the server</p>"},{"location":"plans/2025-01-03-fastapi-llm-streaming-plan/#task_8_add_readme_and_final_commit","title":"Task 8: Add README and final commit","text":"<p>Files:</p> <ul> <li>Create: <code>examples/fastapi_llm/README.md</code></li> </ul> <p>Step 1: Create README</p> <pre><code># FastAPI LLM Streaming Example\n\nDemonstrates native async streaming from `AsyncPipeline` functions to SSE clients.\n\n## Key Concepts\n\n- **AsyncPipeline**: Pipeline that executes async functions natively\n- **AsyncPythonTask**: Task wrapper for async functions\n- **execute_streaming()**: Returns AsyncGenerator for event streaming\n- **AsyncGenerator functions**: Yield events that flow to SSE clients\n\n## Running\n\n```bash\nuv run uvicorn examples.fastapi_llm.main:app --reload\n</code></pre>"},{"location":"plans/2025-01-03-fastapi-llm-streaming-plan/#testing","title":"Testing","text":"<pre><code>curl -N -X POST http://localhost:8000/chat \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"prompt\": \"Hello!\"}'\n</code></pre>"},{"location":"plans/2025-01-03-fastapi-llm-streaming-plan/#event_types","title":"Event Types","text":"<ul> <li><code>{\"type\": \"status\", \"status\": \"thinking\"}</code> - Processing status</li> <li><code>{\"type\": \"chunk\", \"text\": \"...\"}</code> - Token/word chunk</li> <li><code>{\"type\": \"done\", \"full_text\": \"...\"}</code> - Completion <pre><code>**Step 2: Commit**\n\n```bash\ngit add examples/fastapi_llm/README.md\ngit commit -m \"docs: add README for FastAPI LLM streaming example\"\n</code></pre></li> </ul>"},{"location":"plans/2025-01-03-fastapi-llm-streaming-plan/#task_9_run_all_tests_and_push","title":"Task 9: Run all tests and push","text":"<p>Step 1: Run all tests</p> <p><pre><code>uv run pytest tests/ -v --tb=short\n</code></pre> Expected: All tests pass</p> <p>Step 2: Push</p> <p><pre><code>git push origin async-execution\n</code></pre> Expected: Push succeeds</p>"},{"location":"plans/2025-01-17-argo-retry-design/","title":"Argo Workflow Retry Design","text":""},{"location":"plans/2025-01-17-argo-retry-design/#overview","title":"Overview","text":"<p>Extend the existing Argo executor to support workflow-level retry functionality through template memoization, complementing the existing RetryStrategy for step-level retries.</p>"},{"location":"plans/2025-01-17-argo-retry-design/#architecture","title":"Architecture","text":""},{"location":"plans/2025-01-17-argo-retry-design/#two-tier_retry_system","title":"Two-Tier Retry System","text":"<ul> <li>Step-level: Existing RetryStrategy handles retries within a single workflow execution</li> <li>Workflow-level: New memoization approach handles resubmission of entire workflows</li> </ul>"},{"location":"plans/2025-01-17-argo-retry-design/#core_components","title":"Core Components","text":""},{"location":"plans/2025-01-17-argo-retry-design/#required_parameters","title":"Required Parameters","text":"<ul> <li><code>run_id</code>: Now mandatory for all Argo workflows (defaults to workflow UID if not provided)</li> <li><code>retry_run_id</code>: Optional parameter indicating this is a retry of another run</li> <li><code>retry_indicator</code>: Optional parameter for tracking attempt numbers (defaults to empty string)</li> </ul>"},{"location":"plans/2025-01-17-argo-retry-design/#template_enhancements","title":"Template Enhancements","text":"<p>Every generated template receives: <pre><code>memoize:\n  key: \"{{workflow.parameters.run_id}}\"\n  cache:\n    configMap:\n      name: \"runnable-abc123\"  # Generated per workflow\ncontainer:\n  env:\n  - name: RETRY_RUN_ID\n    value: \"{{workflow.parameters.run_id}}\"\n  - name: RETRY_INDICATOR\n    value: \"{{workflow.parameters.retry_indicator}}\"\n</code></pre></p>"},{"location":"plans/2025-01-17-argo-retry-design/#workflow_parameters","title":"Workflow Parameters","text":"<pre><code>spec:\n  arguments:\n    parameters:\n    - name: run_id          # Required\n    - name: retry_run_id    # Optional, empty string default\n    - name: retry_indicator # Optional, empty string default\n</code></pre>"},{"location":"plans/2025-01-17-argo-retry-design/#user_experience","title":"User Experience","text":""},{"location":"plans/2025-01-17-argo-retry-design/#initial_execution","title":"Initial Execution","text":"<pre><code>argo submit workflow.yaml -p run_id=my-pipeline-run-001\n</code></pre>"},{"location":"plans/2025-01-17-argo-retry-design/#retry_execution","title":"Retry Execution","text":"<pre><code>argo resubmit &lt;workflow-name&gt; --memoized \\\n  -p retry_run_id=my-pipeline-run-001 \\\n  -p retry_indicator=2\n</code></pre>"},{"location":"plans/2025-01-17-argo-retry-design/#implementation_strategy","title":"Implementation Strategy","text":""},{"location":"plans/2025-01-17-argo-retry-design/#integration_approach","title":"Integration Approach","text":"<ul> <li>Extend existing <code>extensions/pipeline_executor/argo.py</code></li> <li>Maintain backward compatibility</li> <li>Leverage existing workflow generation patterns</li> <li>Preserve existing RetryStrategy functionality</li> </ul>"},{"location":"plans/2025-01-17-argo-retry-design/#validation","title":"Validation","text":"<ul> <li><code>retry_run_id</code> validation occurs naturally when first failed step executes during resubmit</li> <li>Existing run log store validation handles invalid retry references</li> <li>No additional validation needed at workflow generation time</li> </ul>"},{"location":"plans/2025-01-17-argo-retry-design/#environment_variables","title":"Environment Variables","text":"<ul> <li><code>RETRY_RUN_ID</code>: Always contains current <code>run_id</code> (provides run context)</li> <li><code>RETRY_INDICATOR</code>: Contains attempt chain information for run log tracking</li> </ul>"},{"location":"plans/2025-01-17-argo-retry-design/#benefits","title":"Benefits","text":"<ol> <li>Seamless Integration: Works with Argo's native resubmit and memoization</li> <li>Granular Recovery: Failed steps re-execute, successful steps are skipped</li> <li>Consistent Interface: Uses familiar Argo parameter patterns</li> <li>Backward Compatible: Existing workflows continue to work</li> <li>Run Log Continuity: Proper attempt chaining through retry_indicator</li> </ol>"},{"location":"plans/2025-01-17-argo-retry-design/#technical_notes","title":"Technical Notes","text":"<ul> <li>Template memoization uses shared <code>run_id</code> key across all templates</li> <li>Template names provide natural differentiation within Argo</li> <li>Environment variables present in all templates for consistency</li> <li>Validation leverages existing run log store mechanisms</li> </ul>"},{"location":"plans/2025-01-26-getting-started-tutorial/","title":"Getting Started Tutorial Implementation Plan","text":"<p>For Claude: REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.</p> <p>Goal: Create a comprehensive getting started tutorial that takes users from a simple ML function to a complete, production-ready pipeline through problem-driven chapters.</p> <p>Architecture: Tutorial follows progressive complexity with working ML example throughout. Each chapter builds on the same core functions, showing different Runnable patterns. All code examples are executable and tested. Documentation integrates with existing mkdocs structure.</p> <p>Tech Stack: MkDocs, Python, scikit-learn, pandas, working examples in <code>examples/tutorials/getting-started/</code></p>"},{"location":"plans/2025-01-26-getting-started-tutorial/#task_1_create_tutorial_navigation_structure","title":"Task 1: Create Tutorial Navigation Structure","text":"<p>Files: - Modify: <code>mkdocs.yml:113-161</code> (nav section) - Create: <code>docs/tutorial/index.md</code></p> <p>Step 1: Add tutorial section to navigation</p> <p>In <code>mkdocs.yml</code>, add new section after \"Jobs\" and before \"Pipelines\":</p> <pre><code>nav:\n  - \"Home\": \"index.md\"\n  - \"Jobs\":\n      - \"jobs/index.md\"\n      - \"Your First Job\": \"jobs/first-job.md\"\n      - \"Working with Data\": \"jobs/working-with-data.md\"\n      - \"Parameters &amp; Environment\": \"jobs/parameters.md\"\n      - \"File Storage\": \"jobs/file-storage.md\"\n      - \"Job Types\": \"jobs/job-types.md\"\n  - \"Tutorial\":\n      - \"tutorial/index.md\"\n      - \"Starting Point\": \"tutorial/01-starting-point.md\"\n      - \"Making It Reproducible\": \"tutorial/02-making-it-reproducible.md\"\n      - \"Adding Flexibility\": \"tutorial/03-adding-flexibility.md\"\n      - \"Connecting the Workflow\": \"tutorial/04-connecting-workflow.md\"\n      - \"Handling Large Datasets\": \"tutorial/05-handling-datasets.md\"\n      - \"Sharing Results\": \"tutorial/06-sharing-results.md\"\n      - \"Running Anywhere\": \"tutorial/07-running-anywhere.md\"\n  - \"Pipelines\":\n      - \"Jobs vs Pipelines\": \"pipelines/jobs-vs-pipelines.md\"\n      # ... rest unchanged\n</code></pre> <p>Step 2: Create tutorial index page</p> <p>Create <code>docs/tutorial/index.md</code>:</p> <pre><code># Getting Started Tutorial\n\nTransform a simple machine learning function into a production-ready pipeline, solving real challenges along the way.\n\n## What You'll Build\n\nBy the end of this tutorial, you'll have:\n\n- \u2705 **Reproducible ML pipeline**: Automatic tracking of all runs and results\n- \u2705 **Configurable experiments**: Change parameters without touching code\n- \u2705 **Multi-step workflow**: Data loading \u2192 preprocessing \u2192 training \u2192 evaluation\n- \u2705 **Large dataset handling**: Efficient storage and retrieval of data artifacts\n- \u2705 **Shareable results**: Model artifacts and metrics that persist between runs\n- \u2705 **Deployment ready**: Same pipeline runs on laptop, containers, or Kubernetes\n\n## The Journey\n\nEach chapter tackles a real problem you'll face moving from \"works on my laptop\" to production:\n\n1. **[The Starting Point](01-starting-point.md)** - A typical ML function with common problems\n2. **[Making It Reproducible](02-making-it-reproducible.md)** - Track everything automatically\n3. **[Adding Flexibility](03-adding-flexibility.md)** - Configure without code changes\n4. **[Connecting the Workflow](04-connecting-workflow.md)** - Multi-step ML pipeline\n5. **[Handling Large Datasets](05-handling-datasets.md)** - Efficient data management\n6. **[Sharing Results](06-sharing-results.md)** - Persistent model artifacts and metrics\n7. **[Running Anywhere](07-running-anywhere.md)** - Same code, different environments\n\n## Prerequisites\n\n- Basic Python knowledge\n- Familiarity with scikit-learn (we'll use simple examples)\n- Python environment with runnable installed: `pip install runnable[examples]`\n\n**Time Investment**: ~30-45 minutes total, designed for step-by-step learning\n\n---\n\n**Ready to start?** \u2192 [The Starting Point](01-starting-point.md)\n</code></pre> <p>Step 3: Commit navigation structure</p> <pre><code>git add mkdocs.yml docs/tutorial/index.md\ngit commit -m \"feat(tutorial): add getting started tutorial navigation structure\"\n</code></pre>"},{"location":"plans/2025-01-26-getting-started-tutorial/#task_2_create_core_ml_functions","title":"Task 2: Create Core ML Functions","text":"<p>Files: - Create: <code>examples/tutorials/getting-started/functions.py</code> - Create: <code>examples/tutorials/getting-started/requirements.txt</code> - Create: <code>examples/tutorials/getting-started/__init__.py</code></p> <p>Step 1: Create tutorial directory structure</p> <pre><code>mkdir -p examples/tutorials/getting-started\ntouch examples/tutorials/getting-started/__init__.py\n</code></pre> <p>Step 2: Create requirements file</p> <p>Create <code>examples/tutorials/getting-started/requirements.txt</code>:</p> <pre><code>scikit-learn&gt;=1.3.0\npandas&gt;=2.0.0\nnumpy&gt;=1.21.0\n</code></pre> <p>Step 3: Create core ML functions</p> <p>Create <code>examples/tutorials/getting-started/functions.py</code>:</p> <pre><code>\"\"\"\nCore ML functions for the getting started tutorial.\n\nThese functions represent a realistic ML workflow that progressively\ngets wrapped with Runnable patterns throughout the tutorial.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nimport pickle\nimport json\nimport os\nfrom pathlib import Path\n\n\ndef create_sample_dataset(n_samples=1000, n_features=20, random_state=42):\n    \"\"\"Create a sample classification dataset.\"\"\"\n    X, y = make_classification(\n        n_samples=n_samples,\n        n_features=n_features,\n        n_informative=15,\n        n_redundant=5,\n        n_classes=2,\n        random_state=random_state\n    )\n\n    # Convert to DataFrame for more realistic data handling\n    feature_names = [f\"feature_{i}\" for i in range(n_features)]\n    df = pd.DataFrame(X, columns=feature_names)\n    df['target'] = y\n\n    return df\n\n\ndef load_data(data_path=\"data.csv\"):\n    \"\"\"Load dataset from file or create if doesn't exist.\"\"\"\n    if os.path.exists(data_path):\n        return pd.read_csv(data_path)\n    else:\n        # Create sample data if file doesn't exist\n        df = create_sample_dataset()\n        df.to_csv(data_path, index=False)\n        return df\n\n\ndef preprocess_data(df, test_size=0.2, random_state=42):\n    \"\"\"Preprocess data for training.\"\"\"\n    # Separate features and target\n    X = df.drop('target', axis=1)\n    y = df['target']\n\n    # Train-test split\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=test_size, random_state=random_state\n    )\n\n    # Simple preprocessing - could be much more complex in real scenarios\n    # For now, just ensure no missing values\n    X_train = X_train.fillna(X_train.mean())\n    X_test = X_test.fillna(X_train.mean())  # Use training means\n\n    return {\n        'X_train': X_train,\n        'X_test': X_test,\n        'y_train': y_train,\n        'y_test': y_test\n    }\n\n\ndef train_model(preprocessed_data, n_estimators=100, random_state=42):\n    \"\"\"Train a Random Forest model.\"\"\"\n    X_train = preprocessed_data['X_train']\n    y_train = preprocessed_data['y_train']\n\n    model = RandomForestClassifier(\n        n_estimators=n_estimators,\n        random_state=random_state\n    )\n\n    model.fit(X_train, y_train)\n\n    return {\n        'model': model,\n        'feature_names': list(X_train.columns)\n    }\n\n\ndef evaluate_model(model_data, preprocessed_data):\n    \"\"\"Evaluate the trained model.\"\"\"\n    model = model_data['model']\n    X_test = preprocessed_data['X_test']\n    y_test = preprocessed_data['y_test']\n\n    # Make predictions\n    y_pred = model.predict(X_test)\n    y_pred_proba = model.predict_proba(X_test)\n\n    # Calculate metrics\n    accuracy = accuracy_score(y_test, y_pred)\n    report = classification_report(y_test, y_pred, output_dict=True)\n\n    return {\n        'accuracy': accuracy,\n        'classification_report': report,\n        'predictions': y_pred.tolist(),\n        'probabilities': y_pred_proba.tolist()\n    }\n\n\ndef save_model(model_data, file_path=\"model.pkl\"):\n    \"\"\"Save trained model to file.\"\"\"\n    with open(file_path, 'wb') as f:\n        pickle.dump(model_data, f)\n    return file_path\n\n\ndef save_results(evaluation_results, file_path=\"results.json\"):\n    \"\"\"Save evaluation results to file.\"\"\"\n    with open(file_path, 'w') as f:\n        json.dump(evaluation_results, f, indent=2)\n    return file_path\n\n\n# The \"starting point\" function that combines everything\ndef train_ml_model_basic():\n    \"\"\"\n    Basic ML training function - works locally but has typical problems:\n    - Hardcoded parameters\n    - No tracking of runs\n    - Results get overwritten\n    - No reproducibility guarantees\n    \"\"\"\n    print(\"Loading data...\")\n    df = load_data(\"data.csv\")\n\n    print(\"Preprocessing...\")\n    preprocessed = preprocess_data(df, test_size=0.2, random_state=42)\n\n    print(\"Training model...\")\n    model_data = train_model(preprocessed, n_estimators=100, random_state=42)\n\n    print(\"Evaluating...\")\n    results = evaluate_model(model_data, preprocessed)\n\n    print(f\"Accuracy: {results['accuracy']:.4f}\")\n\n    # Save everything (gets overwritten each run!)\n    save_model(model_data, \"model.pkl\")\n    save_results(results, \"results.json\")\n\n    return results\n\n\nif __name__ == \"__main__\":\n    # This is the \"before Runnable\" version\n    results = train_ml_model_basic()\n    print(\"Done! Check model.pkl and results.json\")\n</code></pre> <p>Step 4: Commit core functions</p> <pre><code>git add examples/tutorials/getting-started/\ngit commit -m \"feat(tutorial): add core ML functions for getting started tutorial\"\n</code></pre>"},{"location":"plans/2025-01-26-getting-started-tutorial/#task_3_create_chapter_1_-_the_starting_point","title":"Task 3: Create Chapter 1 - The Starting Point","text":"<p>Files: - Create: <code>docs/tutorial/01-starting-point.md</code> - Create: <code>examples/tutorials/getting-started/01_starting_point.py</code></p> <p>Step 1: Create the example script</p> <p>Create <code>examples/tutorials/getting-started/01_starting_point.py</code>:</p> <pre><code>\"\"\"\nChapter 1: The Starting Point\n\nThis is a typical ML function that \"works on my laptop\" but has common problems\nwe'll solve throughout the tutorial.\n\"\"\"\n\nfrom functions import train_ml_model_basic\n\ndef main():\n    \"\"\"Run the basic ML training - notice the problems this creates.\"\"\"\n    print(\"=\" * 50)\n    print(\"Chapter 1: The Starting Point\")\n    print(\"=\" * 50)\n\n    # This works, but has problems:\n    # - No tracking of when it ran or what the results were\n    # - Results get overwritten each time\n    # - Parameters are hardcoded\n    # - No way to reproduce exact results later\n    # - Hard to share or deploy\n\n    results = train_ml_model_basic()\n\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Problems with this approach:\")\n    print(\"- Results overwrite each time (no history)\")\n    print(\"- Parameters hardcoded in function\")\n    print(\"- No tracking of execution details\")\n    print(\"- Hard to reproduce exact results\")\n    print(\"- Difficult to share or deploy\")\n    print(\"=\" * 50)\n\n    return results\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Step 2: Create documentation chapter</p> <p>Create <code>docs/tutorial/01-starting-point.md</code>:</p> <pre><code># The Starting Point\n\nLet's start with a realistic scenario: you have a machine learning function that works great on your laptop, but suffers from common problems that prevent it from being production-ready.\n\n## The Problem\n\nHere's a typical ML training function that many data scientists write:\n\n```python title=\"examples/tutorials/getting-started/functions.py (excerpt)\"\ndef train_ml_model_basic():\n    \"\"\"\n    Basic ML training function - works locally but has typical problems:\n    - Hardcoded parameters\n    - No tracking of runs\n    - Results get overwritten\n    - No reproducibility guarantees\n    \"\"\"\n    print(\"Loading data...\")\n    df = load_data(\"data.csv\")\n\n    print(\"Preprocessing...\")\n    preprocessed = preprocess_data(df, test_size=0.2, random_state=42)\n\n    print(\"Training model...\")\n    model_data = train_model(preprocessed, n_estimators=100, random_state=42)\n\n    print(\"Evaluating...\")\n    results = evaluate_model(model_data, preprocessed)\n\n    print(f\"Accuracy: {results['accuracy']:.4f}\")\n\n    # Save everything (gets overwritten each run!)\n    save_model(model_data, \"model.pkl\")\n    save_results(results, \"results.json\")\n\n    return results\n</code></pre> <p>Try it yourself:</p> <pre><code>uv run examples/tutorials/getting-started/01_starting_point.py\n</code></pre>"},{"location":"plans/2025-01-26-getting-started-tutorial/#whats_wrong_here","title":"What's Wrong Here?","text":"<p>This function works, but it has several problems that will bite you in production:</p>"},{"location":"plans/2025-01-26-getting-started-tutorial/#no_execution_tracking","title":"\ud83d\udeab No Execution Tracking","text":"<ul> <li>When did you run this?</li> <li>What were the exact parameters?</li> <li>Which version of the code produced these results?</li> </ul>"},{"location":"plans/2025-01-26-getting-started-tutorial/#results_get_overwritten","title":"\ud83d\udeab Results Get Overwritten","text":"<ul> <li>Run it twice \u2192 lose the first results</li> <li>No way to compare different experiments</li> <li>Can't track model performance over time</li> </ul>"},{"location":"plans/2025-01-26-getting-started-tutorial/#hardcoded_parameters","title":"\ud83d\udeab Hardcoded Parameters","text":"<ul> <li>Want to try different <code>n_estimators</code>? Edit the code</li> <li>Want different train/test split? Edit the code</li> <li>Testing becomes cumbersome and error-prone</li> </ul>"},{"location":"plans/2025-01-26-getting-started-tutorial/#no_reproducibility","title":"\ud83d\udeab No Reproducibility","text":"<ul> <li>Even with <code>random_state</code>, environment differences can cause variations</li> <li>No record of what Python packages were used</li> <li>Impossible to recreate exact results months later</li> </ul>"},{"location":"plans/2025-01-26-getting-started-tutorial/#hard_to_share_and_deploy","title":"\ud83d\udeab Hard to Share and Deploy","text":"<ul> <li>How do you run this in a container?</li> <li>What about on Kubernetes?</li> <li>Sharing with colleagues means sharing your entire environment</li> </ul>"},{"location":"plans/2025-01-26-getting-started-tutorial/#the_real_impact","title":"The Real Impact","text":"<p>These aren't just theoretical problems. In real projects, this leads to:</p> <ul> <li>\"Which model was that?\" - Lost track of good results</li> <li>\"I can't reproduce the paper results\" - Different environments, different outcomes</li> <li>\"It worked yesterday\" - No history of what changed</li> <li>\"How do I run this in production?\" - Deployment becomes a separate project</li> </ul>"},{"location":"plans/2025-01-26-getting-started-tutorial/#what_well_build","title":"What We'll Build","text":"<p>Throughout this tutorial, we'll transform this exact function into a production-ready ML pipeline that solves all these problems:</p> <p>\u2705 Automatic execution tracking - Every run logged with timestamps and parameters \u2705 Result preservation - All experiments saved and easily comparable \u2705 Flexible configuration - Change parameters without touching code \u2705 Full reproducibility - Recreate exact results anytime, anywhere \u2705 Deploy anywhere - Same code runs on laptop, containers, Kubernetes</p> <p>Your functions won't change - we'll just wrap them with Runnable patterns.</p> <p>Next: Making It Reproducible - Add automatic tracking without changing your ML logic <pre><code>**Step 3: Test the example**\n\n```bash\ncd examples/tutorials/getting-started\nuv run 01_starting_point.py\n</code></pre></p> <p>Expected: Script runs successfully, creates <code>model.pkl</code> and <code>results.json</code>, shows problems summary</p> <p>Step 4: Commit Chapter 1</p> <pre><code>git add docs/tutorial/01-starting-point.md examples/tutorials/getting-started/01_starting_point.py\ngit commit -m \"feat(tutorial): add Chapter 1 - The Starting Point\"\n</code></pre>"},{"location":"plans/2025-01-26-getting-started-tutorial/#task_4_create_chapter_2_-_making_it_reproducible","title":"Task 4: Create Chapter 2 - Making It Reproducible","text":"<p>Files: - Create: <code>docs/tutorial/02-making-it-reproducible.md</code> - Create: <code>examples/tutorials/getting-started/02_making_it_reproducible.py</code></p> <p>Step 1: Create the example script</p> <p>Create <code>examples/tutorials/getting-started/02_making_it_reproducible.py</code>:</p> <pre><code>\"\"\"\nChapter 2: Making It Reproducible\n\nSame ML function, now wrapped as a Runnable Job for automatic tracking.\n\"\"\"\n\nfrom runnable import PythonJob\nfrom functions import train_ml_model_basic\n\ndef main():\n    \"\"\"Transform the basic function into a tracked, reproducible job.\"\"\"\n    print(\"=\" * 50)\n    print(\"Chapter 2: Making It Reproducible\")\n    print(\"=\" * 50)\n\n    # Same function, now wrapped as a Job\n    job = PythonJob(function=train_ml_model_basic)\n    job.execute()\n\n    print(\"\\n\" + \"=\" * 50)\n    print(\"What Runnable added automatically:\")\n    print(\"- \ud83d\udcdd Execution logged with timestamp\")\n    print(\"- \ud83d\udd0d Full run details saved to .runnable/run-log-store/\")\n    print(\"- \u267b\ufe0f  Results preserved (never overwritten)\")\n    print(\"- \ud83c\udfaf Reproducible anywhere with same code\")\n    print(\"=\" * 50)\n\n    return job\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Step 2: Create documentation chapter</p> <p>Create <code>docs/tutorial/02-making-it-reproducible.md</code>:</p> <pre><code># Making It Reproducible\n\nNow let's solve the first major problem: lack of execution tracking. We'll transform our basic function into a reproducible, tracked job without changing the ML logic at all.\n\n## The Solution: PythonJob\n\nInstead of calling our function directly, we'll wrap it with Runnable's `PythonJob`:\n\n```python title=\"examples/tutorials/getting-started/02_making_it_reproducible.py\"\nfrom runnable import PythonJob\nfrom functions import train_ml_model_basic\n\ndef main():\n    # Same function, now wrapped as a Job\n    job = PythonJob(function=train_ml_model_basic)\n    job.execute()\n    return job\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Try it:</p> <pre><code>uv run examples/tutorials/getting-started/02_making_it_reproducible.py\n</code></pre>"},{"location":"plans/2025-01-26-getting-started-tutorial/#what_just_happened","title":"What Just Happened?","text":"<p>Your ML function ran exactly the same way, but Runnable automatically added powerful tracking capabilities:</p>"},{"location":"plans/2025-01-26-getting-started-tutorial/#execution_logging","title":"\ud83d\udcdd Execution Logging","text":"<p>Every run gets logged with complete details:</p> <pre><code>ls .runnable/run-log-store/\n# Shows directories with timestamps: 2024-01-26T10-30-45-123456/\n</code></pre> <p>Each run directory contains: - Execution metadata: when it ran, how long it took - Environment info: Python version, package versions - Results: function return values - Status: success/failure with any error details</p>"},{"location":"plans/2025-01-26-getting-started-tutorial/#result_preservation","title":"\u267b\ufe0f Result Preservation","text":"<p>Unlike the basic version that overwrote <code>model.pkl</code> and <code>results.json</code>, each Runnable execution gets its own directory. Your results are never lost.</p>"},{"location":"plans/2025-01-26-getting-started-tutorial/#full_reproducibility","title":"\ud83d\udd0d Full Reproducibility","text":"<p>Each run captures everything needed to reproduce it: - Exact timestamp - Code version (if using git) - Environment details - Input parameters (we'll add those next!)</p>"},{"location":"plans/2025-01-26-getting-started-tutorial/#zero_code_changes","title":"\ud83c\udfaf Zero Code Changes","text":"<p>Notice that <code>train_ml_model_basic()</code> didn't change at all. Runnable works with your existing functions - no decorators, no API changes, no refactoring required.</p>"},{"location":"plans/2025-01-26-getting-started-tutorial/#run_it_multiple_times","title":"Run It Multiple Times","text":"<p>Try running the script several times:</p> <pre><code>uv run examples/tutorials/getting-started/02_making_it_reproducible.py\nuv run examples/tutorials/getting-started/02_making_it_reproducible.py\nuv run examples/tutorials/getting-started/02_making_it_reproducible.py\n</code></pre> <p>Each run creates a separate log entry in <code>.runnable/run-log-store/</code>. You now have a complete history of all your experiments!</p>"},{"location":"plans/2025-01-26-getting-started-tutorial/#compare_before_vs_after","title":"Compare: Before vs After","text":"<p>Before (Chapter 1): - \u274c Results overwritten each time - \u274c No execution history - \u274c No timestamps or metadata - \u274c Hard to track what worked</p> <p>After (Chapter 2): - \u2705 Every run preserved with timestamp - \u2705 Complete execution history - \u2705 Full metadata captured automatically - \u2705 Easy to see what worked when</p>"},{"location":"plans/2025-01-26-getting-started-tutorial/#whats_still_missing","title":"What's Still Missing?","text":"<p>We solved execution tracking, but we still have: - Parameters hardcoded in the function - No easy way to run experiments with different settings</p> <p>Next chapter: We'll make the function configurable without changing the ML logic.</p> <p>Next: Adding Flexibility - Configure experiments without touching code <pre><code>**Step 3: Test the example**\n\n```bash\ncd examples/tutorials/getting-started\nuv run 02_making_it_reproducible.py\n</code></pre></p> <p>Expected: Script runs, shows same ML output, creates <code>.runnable/run-log-store/</code> directory with timestamped execution logs</p> <p>Step 4: Commit Chapter 2</p> <pre><code>git add docs/tutorial/02-making-it-reproducible.md examples/tutorials/getting-started/02_making_it_reproducible.py\ngit commit -m \"feat(tutorial): add Chapter 2 - Making It Reproducible\"\n</code></pre>"},{"location":"plans/2025-01-26-getting-started-tutorial/#task_5_create_chapter_3_-_adding_flexibility","title":"Task 5: Create Chapter 3 - Adding Flexibility","text":"<p>Files: - Create: <code>docs/tutorial/03-adding-flexibility.md</code> - Create: <code>examples/tutorials/getting-started/03_adding_flexibility.py</code> - Create: <code>examples/tutorials/getting-started/functions_parameterized.py</code> - Create: <code>examples/tutorials/getting-started/experiment_configs/basic.yaml</code> - Create: <code>examples/tutorials/getting-started/experiment_configs/large_forest.yaml</code></p> <p>Step 1: Create parameterized functions</p> <p>Create <code>examples/tutorials/getting-started/functions_parameterized.py</code>:</p> <pre><code>\"\"\"\nParameterized versions of ML functions for Chapter 3.\n\nThese functions accept parameters instead of having hardcoded values,\nmaking them flexible for different experiments.\n\"\"\"\n\nfrom functions import load_data, preprocess_data, train_model, evaluate_model, save_model, save_results\n\n\ndef train_ml_model_flexible(\n    data_path=\"data.csv\",\n    test_size=0.2,\n    n_estimators=100,\n    random_state=42,\n    model_path=\"model.pkl\",\n    results_path=\"results.json\"\n):\n    \"\"\"\n    Flexible ML training function that accepts parameters.\n\n    Same logic as train_ml_model_basic, but now configurable!\n    \"\"\"\n    print(\"Loading data...\")\n    df = load_data(data_path)\n\n    print(\"Preprocessing...\")\n    preprocessed = preprocess_data(df, test_size=test_size, random_state=random_state)\n\n    print(f\"Training model with {n_estimators} estimators...\")\n    model_data = train_model(preprocessed, n_estimators=n_estimators, random_state=random_state)\n\n    print(\"Evaluating...\")\n    results = evaluate_model(model_data, preprocessed)\n\n    print(f\"Accuracy: {results['accuracy']:.4f}\")\n\n    # Save with custom paths\n    save_model(model_data, model_path)\n    save_results(results, results_path)\n\n    return results\n</code></pre> <p>Step 2: Create experiment configurations</p> <pre><code>mkdir -p examples/tutorials/getting-started/experiment_configs\n</code></pre> <p>Create <code>examples/tutorials/getting-started/experiment_configs/basic.yaml</code>:</p> <pre><code># Basic experiment configuration\ntest_size: 0.2\nn_estimators: 50\nrandom_state: 42\nmodel_path: \"models/basic_model.pkl\"\nresults_path: \"results/basic_results.json\"\n</code></pre> <p>Create <code>examples/tutorials/getting-started/experiment_configs/large_forest.yaml</code>:</p> <pre><code># Large Random Forest experiment\ntest_size: 0.25\nn_estimators: 200\nrandom_state: 123\nmodel_path: \"models/large_forest.pkl\"\nresults_path: \"results/large_forest_results.json\"\n</code></pre> <p>Step 3: Create the example script</p> <p>Create <code>examples/tutorials/getting-started/03_adding_flexibility.py</code>:</p> <pre><code>\"\"\"\nChapter 3: Adding Flexibility\n\nSame ML function, now parameterized and configurable without code changes.\n\"\"\"\n\nfrom runnable import PythonJob\nfrom functions_parameterized import train_ml_model_flexible\n\ndef main():\n    \"\"\"Show how to run the same function with different parameters.\"\"\"\n    print(\"=\" * 50)\n    print(\"Chapter 3: Adding Flexibility\")\n    print(\"=\" * 50)\n\n    # Same function, now accepts parameters from environment or config files\n    job = PythonJob(function=train_ml_model_flexible)\n    job.execute()\n\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Parameter flexibility added:\")\n    print(\"- \ud83d\udd27 Function accepts parameters\")\n    print(\"- \ud83c\udf0d Parameters from environment variables\")\n    print(\"- \ud83d\udcc1 Parameters from YAML config files\")\n    print(\"- \ud83e\uddea Run different experiments without code changes\")\n    print(\"\\nTry these commands:\")\n    print(\"RUNNABLE_PRM_n_estimators=200 uv run 03_adding_flexibility.py\")\n    print(\"uv run 03_adding_flexibility.py --parameters-file experiment_configs/basic.yaml\")\n    print(\"=\" * 50)\n\n    return job\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Step 4: Create documentation chapter</p> <p>Create <code>docs/tutorial/03-adding-flexibility.md</code>:</p> <pre><code># Adding Flexibility\n\nNow let's solve another major problem: hardcoded parameters. We'll make our ML function configurable so you can run different experiments without touching any code.\n\n## The Problem with Hardcoded Parameters\n\nIn Chapter 2, our function still had hardcoded values:\n\n```python\n# Fixed values - need code changes for experiments\npreprocessed = preprocess_data(df, test_size=0.2, random_state=42)\nmodel_data = train_model(preprocessed, n_estimators=100, random_state=42)\n</code></pre> <p>Want to try <code>n_estimators=200</code>? Edit the code. Different train/test split? Edit the code. This doesn't scale for experimentation.</p>"},{"location":"plans/2025-01-26-getting-started-tutorial/#the_solution_parameterized_functions","title":"The Solution: Parameterized Functions","text":"<p>Let's create a flexible version that accepts parameters:</p> examples/tutorials/getting-started/functions_parameterized.py<pre><code>def train_ml_model_flexible(\n    data_path=\"data.csv\",\n    test_size=0.2,\n    n_estimators=100,\n    random_state=42,\n    model_path=\"model.pkl\",\n    results_path=\"results.json\"\n):\n    \"\"\"Same ML logic, now configurable!\"\"\"\n    print(\"Loading data...\")\n    df = load_data(data_path)\n\n    print(\"Preprocessing...\")\n    preprocessed = preprocess_data(df, test_size=test_size, random_state=random_state)\n\n    print(f\"Training model with {n_estimators} estimators...\")\n    model_data = train_model(preprocessed, n_estimators=n_estimators, random_state=random_state)\n\n    # ... rest unchanged but uses parameters\n</code></pre>"},{"location":"plans/2025-01-26-getting-started-tutorial/#running_with_parameters","title":"Running with Parameters","text":"<p>Now you can run different experiments without changing code:</p>"},{"location":"plans/2025-01-26-getting-started-tutorial/#environment_variables","title":"\ud83c\udf0d Environment Variables","text":"<pre><code># Default parameters\nuv run examples/tutorials/getting-started/03_adding_flexibility.py\n\n# Large forest experiment\nRUNNABLE_PRM_n_estimators=200 uv run examples/tutorials/getting-started/03_adding_flexibility.py\n\n# Different train/test split\nRUNNABLE_PRM_test_size=0.3 RUNNABLE_PRM_n_estimators=150 uv run examples/tutorials/getting-started/03_adding_flexibility.py\n</code></pre>"},{"location":"plans/2025-01-26-getting-started-tutorial/#configuration_files","title":"\ud83d\udcc1 Configuration Files","text":"<p>Create experiment configurations:</p> examples/tutorials/getting-started/experiment_configs/basic.yaml<pre><code>test_size: 0.2\nn_estimators: 50\nrandom_state: 42\nmodel_path: \"models/basic_model.pkl\"\nresults_path: \"results/basic_results.json\"\n</code></pre> examples/tutorials/getting-started/experiment_configs/large_forest.yaml<pre><code>test_size: 0.25\nn_estimators: 200\nrandom_state: 123\nmodel_path: \"models/large_forest.pkl\"\nresults_path: \"results/large_forest_results.json\"\n</code></pre> <p>Run different experiments:</p> <pre><code># Basic experiment\nuv run examples/tutorials/getting-started/03_adding_flexibility.py --parameters-file experiment_configs/basic.yaml\n\n# Large forest experiment\nuv run examples/tutorials/getting-started/03_adding_flexibility.py --parameters-file experiment_configs/large_forest.yaml\n</code></pre>"},{"location":"plans/2025-01-26-getting-started-tutorial/#parameter_precedence","title":"Parameter Precedence","text":"<p>Runnable handles parameter conflicts intelligently:</p> <ol> <li>Environment variables (highest priority): <code>RUNNABLE_PRM_n_estimators=300</code></li> <li>Command line config: <code>--parameters-file config.yaml</code></li> <li>Function defaults (lowest priority): What you defined in the function signature</li> </ol> <p>This means you can have a base configuration file but override specific values with environment variables.</p>"},{"location":"plans/2025-01-26-getting-started-tutorial/#what_you_get_now","title":"What You Get Now","text":""},{"location":"plans/2025-01-26-getting-started-tutorial/#easy_experimentation","title":"\ud83e\uddea Easy Experimentation","text":"<ul> <li>Test different hyperparameters instantly</li> <li>Compare multiple approaches without code changes</li> <li>Save each experiment configuration for reproducibility</li> </ul>"},{"location":"plans/2025-01-26-getting-started-tutorial/#automatic_experiment_tracking","title":"\ud83d\udcca Automatic Experiment Tracking","text":"<p>Every run gets logged with the exact parameters used:</p> <pre><code>ls .runnable/run-log-store/\n# Each timestamped directory contains the parameters for that run\n</code></pre>"},{"location":"plans/2025-01-26-getting-started-tutorial/#reproducible_experiments","title":"\ud83d\udd04 Reproducible Experiments","text":"<p>Want to recreate that great result from last week? Just rerun with the same config file.</p>"},{"location":"plans/2025-01-26-getting-started-tutorial/#clean_separation","title":"\ud83c\udfaf Clean Separation","text":"<ul> <li>Your ML logic: Stays in the function, unchanged</li> <li>Experiment configuration: Lives in config files or environment variables</li> <li>Execution tracking: Handled automatically by Runnable</li> </ul>"},{"location":"plans/2025-01-26-getting-started-tutorial/#try_it_yourself","title":"Try It Yourself","text":"<p>Run these experiments and watch how each gets tracked separately:</p> <pre><code>cd examples/tutorials/getting-started\n\n# Experiment 1: Default\nuv run 03_adding_flexibility.py\n\n# Experiment 2: Large forest\nRUNNABLE_PRM_n_estimators=200 uv run 03_adding_flexibility.py\n\n# Experiment 3: From config file\nuv run 03_adding_flexibility.py --parameters-file experiment_configs/large_forest.yaml\n\n# Check the logs - each run preserved with its parameters\nls .runnable/run-log-store/\n</code></pre>"},{"location":"plans/2025-01-26-getting-started-tutorial/#compare_before_vs_after_1","title":"Compare: Before vs After","text":"<p>Before: - \u274c Parameters hardcoded in functions - \u274c Code changes needed for experiments - \u274c Hard to track which parameters produced which results</p> <p>After: - \u2705 Functions accept parameters with sensible defaults - \u2705 Experiments configurable via environment or config files - \u2705 Every run logged with exact parameters used - \u2705 Easy to reproduce any experiment</p> <p>Next: We'll break our monolithic function into a proper multi-step ML pipeline.</p> <p>Next: Connecting the Workflow - Multi-step ML pipeline with automatic data flow <pre><code>**Step 5: Test the example**\n\n```bash\ncd examples/tutorials/getting-started\nuv run 03_adding_flexibility.py\nRUNNABLE_PRM_n_estimators=200 uv run 03_adding_flexibility.py\n</code></pre></p> <p>Expected: Both runs work, second shows \"Training model with 200 estimators\", both logged separately</p> <p>Step 6: Commit Chapter 3</p> <pre><code>git add docs/tutorial/03-adding-flexibility.md examples/tutorials/getting-started/03_adding_flexibility.py examples/tutorials/getting-started/functions_parameterized.py examples/tutorials/getting-started/experiment_configs/\ngit commit -m \"feat(tutorial): add Chapter 3 - Adding Flexibility\"\n</code></pre>"},{"location":"plans/2025-01-26-getting-started-tutorial/#task_6_create_chapter_4_-_connecting_the_workflow","title":"Task 6: Create Chapter 4 - Connecting the Workflow","text":"<p>Files: - Create: <code>docs/tutorial/04-connecting-workflow.md</code> - Create: <code>examples/tutorials/getting-started/04_connecting_workflow.py</code></p> <p>Step 1: Create the example script</p> <p>Create <code>examples/tutorials/getting-started/04_connecting_workflow.py</code>:</p> <pre><code>\"\"\"\nChapter 4: Connecting the Workflow\n\nBreak the monolithic function into a proper multi-step ML pipeline\nwith automatic data flow between steps.\n\"\"\"\n\nfrom runnable import Pipeline, PythonTask, pickled\nfrom functions import load_data, preprocess_data, train_model, evaluate_model\n\n\ndef main():\n    \"\"\"Transform monolithic function into multi-step pipeline.\"\"\"\n    print(\"=\" * 50)\n    print(\"Chapter 4: Connecting the Workflow\")\n    print(\"=\" * 50)\n\n    # Same functions, now as separate pipeline steps\n    pipeline = Pipeline(steps=[\n        PythonTask(\n            function=load_data,\n            name=\"load_data\",\n            returns=[pickled(\"dataset\")]\n        ),\n        PythonTask(\n            function=preprocess_data,\n            name=\"preprocess\",\n            returns=[pickled(\"preprocessed_data\")]\n        ),\n        PythonTask(\n            function=train_model,\n            name=\"train\",\n            returns=[pickled(\"model_data\")]\n        ),\n        PythonTask(\n            function=evaluate_model,\n            name=\"evaluate\",\n            returns=[pickled(\"evaluation_results\")]\n        )\n    ])\n\n    pipeline.execute()\n\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Pipeline benefits:\")\n    print(\"- \ud83d\udd17 Automatic data flow between steps\")\n    print(\"- \u26a1 Can resume from any failed step\")\n    print(\"- \ud83d\udcca Individual step tracking and timing\")\n    print(\"- \ud83d\udd0d Intermediate results preserved\")\n    print(\"- \ud83c\udfaf Better debugging and development\")\n    print(\"=\" * 50)\n\n    return pipeline\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Step 2: Create documentation chapter</p> <p>Create <code>docs/tutorial/04-connecting-workflow.md</code>:</p> <pre><code># Connecting the Workflow\n\nSo far we've been treating ML training as one big function. In reality, ML workflows have distinct steps: data loading, preprocessing, training, and evaluation. Let's break our monolithic function into a proper pipeline.\n\n## Why Break It Up?\n\nOur current approach has limitations:\n\n```python\ndef train_ml_model_flexible():\n    # All steps in one function\n    df = load_data()           # Step 1\n    preprocessed = preprocess_data()  # Step 2\n    model = train_model()      # Step 3\n    results = evaluate_model() # Step 4\n    return results\n</code></pre> <p>Problems: - If training fails, you lose preprocessing work - Hard to debug specific steps - Can't reuse preprocessing for different models - No visibility into step-by-step progress</p>"},{"location":"plans/2025-01-26-getting-started-tutorial/#the_solution_pipeline_with_tasks","title":"The Solution: Pipeline with Tasks","text":"<p>Let's use the individual functions we already have and connect them as a pipeline:</p> examples/tutorials/getting-started/04_connecting_workflow.py<pre><code>from runnable import Pipeline, PythonTask, pickled\nfrom functions import load_data, preprocess_data, train_model, evaluate_model\n\ndef main():\n    pipeline = Pipeline(steps=[\n        PythonTask(\n            function=load_data,\n            name=\"load_data\",\n            returns=[pickled(\"dataset\")]\n        ),\n        PythonTask(\n            function=preprocess_data,\n            name=\"preprocess\",\n            returns=[pickled(\"preprocessed_data\")]\n        ),\n        PythonTask(\n            function=train_model,\n            name=\"train\",\n            returns=[pickled(\"model_data\")]\n        ),\n        PythonTask(\n            function=evaluate_model,\n            name=\"evaluate\",\n            returns=[pickled(\"evaluation_results\")]\n        )\n    ])\n\n    pipeline.execute()\n    return pipeline\n</code></pre> <p>Try it:</p> <pre><code>uv run examples/tutorials/getting-started/04_connecting_workflow.py\n</code></pre>"},{"location":"plans/2025-01-26-getting-started-tutorial/#how_data_flows_automatically","title":"How Data Flows Automatically","text":"<p>Notice something magical: we didn't write any glue code! Runnable automatically connects the steps:</p> <ol> <li><code>load_data()</code> returns a DataFrame</li> <li><code>preprocess_data(df)</code> - gets the DataFrame automatically (parameter name matches!)</li> <li><code>train_model(preprocessed_data)</code> - gets preprocessing results automatically</li> <li><code>evaluate_model(model_data, preprocessed_data)</code> - gets both model and data automatically</li> </ol> <p>The secret: Parameter names in your functions determine data flow. If <code>train_model()</code> expects a parameter called <code>preprocessed_data</code>, and a previous step returns something called <code>preprocessed_data</code>, they get connected automatically.</p>"},{"location":"plans/2025-01-26-getting-started-tutorial/#what_you_get_with_pipelines","title":"What You Get with Pipelines","text":""},{"location":"plans/2025-01-26-getting-started-tutorial/#step-by-step_execution","title":"\u26a1 Step-by-Step Execution","text":"<p>Each step runs individually and you can see progress: <pre><code>load_data: \u2705 Completed in 0.1s\npreprocess: \u2705 Completed in 0.3s\ntrain: \u2705 Completed in 2.4s\nevaluate: \u2705 Completed in 0.2s\n</code></pre></p>"},{"location":"plans/2025-01-26-getting-started-tutorial/#intermediate_results_preserved","title":"\ud83d\udd0d Intermediate Results Preserved","text":"<p>Each step's output is saved. You can inspect intermediate results without rerunning expensive steps:</p> <pre><code># Check what the preprocessing step produced\nls .runnable/\n</code></pre>"},{"location":"plans/2025-01-26-getting-started-tutorial/#better_debugging","title":"\ud83d\udee0\ufe0f Better Debugging","text":"<p>If training fails, you don't lose your preprocessing work. You can debug just the training step.</p>"},{"location":"plans/2025-01-26-getting-started-tutorial/#individual_step_tracking","title":"\ud83d\udcca Individual Step Tracking","text":"<p>See timing and resource usage for each step, helping identify bottlenecks.</p>"},{"location":"plans/2025-01-26-getting-started-tutorial/#advanced_parameters_in_pipelines","title":"Advanced: Parameters in Pipelines","text":"<p>You can still use parameters, but now at the step level:</p> <pre><code># Add parameters to specific steps\npipeline = Pipeline(steps=[\n    PythonTask(function=load_data, name=\"load_data\", returns=[pickled(\"dataset\")]),\n    PythonTask(function=preprocess_data, name=\"preprocess\", returns=[pickled(\"preprocessed_data\")]),\n    PythonTask(function=train_model, name=\"train\", returns=[pickled(\"model_data\")]),\n    PythonTask(function=evaluate_model, name=\"evaluate\", returns=[pickled(\"results\")])\n])\n\n# Parameters still work the same way\n# RUNNABLE_PRM_test_size=0.3 uv run 04_connecting_workflow.py\n</code></pre> <p>Parameters get passed to the appropriate functions based on their parameter names.</p>"},{"location":"plans/2025-01-26-getting-started-tutorial/#compare_monolithic_vs_pipeline","title":"Compare: Monolithic vs Pipeline","text":"<p>Monolithic Function (Chapters 1-3): - \u274c All-or-nothing execution - \u274c Hard to debug failed steps - \u274c Expensive to rerun everything - \u274c No intermediate result visibility</p> <p>Pipeline (Chapter 4): - \u2705 Step-by-step execution with progress - \u2705 Intermediate results preserved - \u2705 Resume from failed steps - \u2705 Better debugging and development - \u2705 Automatic data flow between steps</p>"},{"location":"plans/2025-01-26-getting-started-tutorial/#your_functions_didnt_change","title":"Your Functions Didn't Change","text":"<p>Notice that we're using the exact same functions from earlier: - <code>load_data()</code> - <code>preprocess_data()</code> - <code>train_model()</code> - <code>evaluate_model()</code></p> <p>No refactoring required. Runnable works with your existing functions - you just organize them into steps.</p>"},{"location":"plans/2025-01-26-getting-started-tutorial/#whats_next","title":"What's Next?","text":"<p>We have a great pipeline, but we're still dealing with everything in memory. What about large datasets that don't fit in RAM? Or sharing intermediate results with teammates?</p> <p>Next chapter: We'll add efficient data management for large-scale ML workflows.</p> <p>Next: Handling Large Datasets - Efficient storage and retrieval of data artifacts <pre><code>**Step 3: Test the example**\n\n```bash\ncd examples/tutorials/getting-started\nuv run 04_connecting_workflow.py\n</code></pre></p> <p>Expected: Pipeline runs step-by-step, shows progress for each task, creates individual step logs</p> <p>Step 4: Commit Chapter 4</p> <pre><code>git add docs/tutorial/04-connecting-workflow.md examples/tutorials/getting-started/04_connecting_workflow.py\ngit commit -m \"feat(tutorial): add Chapter 4 - Connecting the Workflow\"\n</code></pre>"},{"location":"plans/2025-01-26-getting-started-tutorial/#task_7_update_todowrite_progress","title":"Task 7: Update TodoWrite Progress","text":"<p>Files: - Update todo list</p> <p>Step 1: Mark current task as complete and continue</p> <pre><code># Continue with remaining chapters\n</code></pre> <p>Update todo:</p> <pre><code>[\n    {\"content\": \"Create comprehensive implementation plan for getting started tutorial\", \"status\": \"completed\", \"activeForm\": \"Creating comprehensive implementation plan for getting started tutorial\"},\n    {\"content\": \"Create remaining tutorial chapters (5-7)\", \"status\": \"in_progress\", \"activeForm\": \"Creating remaining tutorial chapters (5-7)\"},\n    {\"content\": \"Test all tutorial examples work correctly\", \"status\": \"pending\", \"activeForm\": \"Testing all tutorial examples work correctly\"},\n    {\"content\": \"Update mkdocs navigation and test docs build\", \"status\": \"pending\", \"activeForm\": \"Updating mkdocs navigation and testing docs build\"}\n]\n</code></pre>"},{"location":"plans/2025-01-26-getting-started-tutorial/#task_8_create_chapters_5-7_combined_for_brevity","title":"Task 8: Create Chapters 5-7 (Combined for Brevity)","text":"<p>Files: - Create: <code>docs/tutorial/05-handling-datasets.md</code> - Create: <code>docs/tutorial/06-sharing-results.md</code> - Create: <code>docs/tutorial/07-running-anywhere.md</code> - Create: <code>examples/tutorials/getting-started/05_handling_datasets.py</code> - Create: <code>examples/tutorials/getting-started/06_sharing_results.py</code> - Create: <code>examples/tutorials/getting-started/07_running_anywhere.py</code></p> <p>[Content abbreviated for brevity - each chapter follows same pattern with working examples and comprehensive documentation]</p>"},{"location":"plans/2025-01-26-getting-started-tutorial/#task_9_test_all_examples","title":"Task 9: Test All Examples","text":"<p>Files: - Test: All example scripts - Verify: Documentation builds correctly</p> <p>Step 1: Test all tutorial examples</p> <pre><code>cd examples/tutorials/getting-started\nuv run 01_starting_point.py\nuv run 02_making_it_reproducible.py\nuv run 03_adding_flexibility.py\nuv run 04_connecting_workflow.py\n# ... etc for all chapters\n</code></pre> <p>Step 2: Test documentation builds</p> <pre><code>uv run mkdocs serve\n# Visit http://localhost:8000/tutorial/ to verify all pages load correctly\n</code></pre> <p>Step 3: Commit final tests</p> <pre><code>git add .\ngit commit -m \"feat(tutorial): complete getting started tutorial implementation\"\n</code></pre>"},{"location":"plans/2025-01-26-getting-started-tutorial/#final_implementation_notes","title":"Final Implementation Notes","text":"<ol> <li>All code examples are executable - Every code snippet comes from working examples</li> <li>Progressive complexity - Each chapter builds on previous concepts</li> <li>Problem-driven structure - Each chapter solves a real ML engineering challenge</li> <li>Documentation integration - Seamlessly fits into existing mkdocs structure</li> <li>Consistent patterns - Follows established documentation style and conventions</li> </ol> <p>Total estimated time: 2-3 hours for complete implementation Testing time: 30 minutes for verification Documentation review: 15 minutes for final polish</p>"},{"location":"plans/2025-01-30-telemetry-design/","title":"Telemetry Design for Runnable","text":"<p>Date: 2025-01-30 Status: Approved Scope: Phase 1 - Pipeline and Task level telemetry with FastAPI streaming support</p>"},{"location":"plans/2025-01-30-telemetry-design/#overview","title":"Overview","text":"<p>Add optional telemetry to runnable that emits OpenTelemetry spans during execution. This provides real-time visibility into workflow execution while preserving existing run log functionality for detailed post-hoc review.</p>"},{"location":"plans/2025-01-30-telemetry-design/#goals","title":"Goals","text":"<ul> <li>Execution times for pipelines and tasks</li> <li>Input/output parameters (truncated to ~256 bytes)</li> <li>Execution logs as span events</li> <li>Function calls as they happen through the workflow</li> <li>Real-time streaming to UI via FastAPI SSE</li> <li>Self-hosted OpenTelemetry backend support (Jaeger, Tempo, etc.)</li> </ul>"},{"location":"plans/2025-01-30-telemetry-design/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                          Your Infrastructure                            \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502   Your UI    \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502   Backend    \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502  Collector   \u2502    \u2502\n\u2502  \u2502  (React/etc) \u2502  query  \u2502 (Jaeger/Tempo\u2502  store  \u2502   (OTEL)     \u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502   /Grafana)  \u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2502         \u2502                 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                \u25b2            \u2502\n\u2502         \u2502                                                 \u2502            \u2502\n\u2502         \u2502 SSE stream                           emit spans \u2502            \u2502\n\u2502         \u25bc                                                 \u2502            \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2510         \u2502\n\u2502  \u2502   FastAPI    \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502         runnable pipeline       \u2502         \u2502\n\u2502  \u2502   Service    \u2502 execute \u2502  (logfire-api emits to collector)\u2502         \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u2502                                                                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"plans/2025-01-30-telemetry-design/#key_design_decisions","title":"Key Design Decisions","text":"<ol> <li> <p>Built into core with automatic kill switch: Uses <code>logfire-api</code> shim package - if <code>logfire</code> is not installed, all calls are no-ops. No manual enable/disable needed.</p> </li> <li> <p>Dual output via custom SpanProcessor: Spans are always exported to collector (if configured) AND streamed to SSE queue (if FastAPI connection is active).</p> </li> <li> <p>Additive, not replacing: Telemetry runs alongside existing run log store. Run logs remain the source of truth for detailed review.</p> </li> </ol>"},{"location":"plans/2025-01-30-telemetry-design/#span_hierarchy","title":"Span Hierarchy","text":"<pre><code>HTTP Request (FastAPI middleware - if present)\n\u2514\u2500\u2500 pipeline:{name}                             \u2190 runnable/context.py\n    \u251c\u2500\u2500 task:{name}                             \u2190 runnable/tasks.py\n    \u2502   \u2514\u2500\u2500 events: started, completed/failed\n    \u251c\u2500\u2500 task:{name}\n    \u2502   \u2514\u2500\u2500 events: started, completed/failed\n    \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"plans/2025-01-30-telemetry-design/#span_attributes","title":"Span Attributes","text":"<p>Pipeline span: - <code>pipeline_name</code>: Name of the pipeline - <code>run_id</code>: Unique execution ID - <code>executor</code>: Executor class name</p> <p>Task span: - <code>task_name</code>: Name of the task - <code>task_type</code>: python, notebook, shell - <code>command</code>: Function path or command - <code>inputs</code>: Truncated input parameters (JSON, max 256 bytes) - <code>outputs</code>: Truncated output parameters (JSON, max 256 bytes) - <code>status</code>: success/fail - <code>error</code>: Error message (on failure)</p>"},{"location":"plans/2025-01-30-telemetry-design/#implementation","title":"Implementation","text":""},{"location":"plans/2025-01-30-telemetry-design/#new_file_runnabletelemetrypy","title":"New File: <code>runnable/telemetry.py</code>","text":"<pre><code>\"\"\"\nTelemetry support for runnable pipelines.\n\nUses logfire-api for zero-dependency instrumentation.\nIf logfire is installed, spans are emitted. If not, all calls are no-ops.\n\nFor real-time streaming (e.g., FastAPI SSE), use StreamingSpanProcessor.\n\"\"\"\nimport json\nfrom contextvars import ContextVar\nfrom queue import Queue\nfrom typing import Any\n\nimport logfire_api as logfire\n\n# Optional OTEL imports for streaming processor\ntry:\n    from opentelemetry.sdk.trace import SpanProcessor, ReadableSpan\n    OTEL_AVAILABLE = True\nexcept ImportError:\n    OTEL_AVAILABLE = False\n\n# Context var for active stream queue (set by FastAPI when SSE is active)\n_stream_queue: ContextVar[Queue | None] = ContextVar(\"stream_queue\", default=None)\n\n\ndef truncate_value(value: Any, max_bytes: int = 256) -&gt; str:\n    \"\"\"Truncate serialized value to max_bytes.\"\"\"\n    try:\n        serialized = json.dumps(value, default=str)\n        if len(serialized) &gt; max_bytes:\n            return serialized[:max_bytes - 3] + \"...\"\n        return serialized\n    except Exception:\n        return f\"&lt;unserializable: {type(value).__name__}&gt;\"\n\n\ndef set_stream_queue(q: Queue | None):\n    \"\"\"Set the queue for streaming spans (called by FastAPI).\"\"\"\n    _stream_queue.set(q)\n\n\ndef get_stream_queue() -&gt; Queue | None:\n    \"\"\"Get the current stream queue.\"\"\"\n    return _stream_queue.get()\n\n\nif OTEL_AVAILABLE:\n    class StreamingSpanProcessor(SpanProcessor):\n        \"\"\"\n        SpanProcessor that:\n        1. Always forwards to base processor (collector export)\n        2. Also pushes to stream queue if SSE is active\n        \"\"\"\n        def __init__(self, base_processor: SpanProcessor | None = None):\n            self.base_processor = base_processor\n\n        def on_start(self, span, parent_context=None):\n            if self.base_processor:\n                self.base_processor.on_start(span, parent_context)\n\n            q = _stream_queue.get()\n            if q is not None:\n                q.put_nowait({\n                    \"type\": \"span_start\",\n                    \"name\": span.name,\n                    \"span_id\": format(span.context.span_id, \"016x\"),\n                })\n\n        def on_end(self, span: ReadableSpan):\n            if self.base_processor:\n                self.base_processor.on_end(span)\n\n            q = _stream_queue.get()\n            if q is not None:\n                q.put_nowait({\n                    \"type\": \"span_end\",\n                    \"name\": span.name,\n                    \"span_id\": format(span.context.span_id, \"016x\"),\n                    \"status\": span.status.status_code.name,\n                    \"duration_ms\": (span.end_time - span.start_time) / 1_000_000,\n                    \"attributes\": dict(span.attributes) if span.attributes else {},\n                })\n\n        def shutdown(self):\n            if self.base_processor:\n                self.base_processor.shutdown()\n\n        def force_flush(self, timeout_millis=None):\n            if self.base_processor:\n                self.base_processor.force_flush(timeout_millis)\n</code></pre>"},{"location":"plans/2025-01-30-telemetry-design/#integration_runnablecontextpy","title":"Integration: <code>runnable/context.py</code>","text":"<pre><code>import logfire_api as logfire\n\nclass PipelineContext(BaseContext):\n    def execute(self, ...):\n        with logfire.span(\n            \"pipeline:{pipeline_name}\",\n            pipeline_name=self.pipeline.name,\n            run_id=self.run_id,\n            executor=self.pipeline_executor.__class__.__name__,\n        ):\n            logfire.info(\"Pipeline started\")\n\n            try:\n                self.pipeline_executor._set_up_run_log(exists_ok=False)\n                self.pipeline_executor.execute_graph(dag=self.pipeline.dag)\n\n                logfire.info(\"Pipeline completed\", status=\"success\")\n            except Exception as e:\n                logfire.error(\"Pipeline failed\", error=str(e))\n                raise\n</code></pre>"},{"location":"plans/2025-01-30-telemetry-design/#integration_runnabletaskspy","title":"Integration: <code>runnable/tasks.py</code>","text":"<pre><code>import logfire_api as logfire\nfrom runnable.telemetry import truncate_value\n\nclass PythonTaskType(BaseTaskType):\n    def execute_command(self, map_variable: MapVariableType = None) -&gt; StepAttempt:\n        attempt_log = StepAttempt(status=defaults.FAIL, ...)\n        task_name = self._context.pipeline_executor._context_node.name\n\n        with logfire.span(\n            \"task:{task_name}\",\n            task_name=task_name,\n            task_type=self.task_type,\n            command=self.command,\n        ):\n            with self.execution_context(map_variable=map_variable) as params:\n                logfire.info(\"Task started\", inputs=truncate_value(params))\n\n                try:\n                    # ... existing function call logic ...\n                    user_set_parameters = f(**filtered_parameters)\n\n                    if self.returns:\n                        # ... existing return handling ...\n                        logfire.info(\"Task completed\",\n                                    outputs=truncate_value(output_parameters),\n                                    status=\"success\")\n\n                    attempt_log.status = defaults.SUCCESS\n\n                except Exception as e:\n                    logfire.error(\"Task failed\", error=str(e)[:256])\n                    # ... existing error handling ...\n\n        return attempt_log\n</code></pre> <p>Same pattern applies to <code>NotebookTaskType</code> and <code>ShellTaskType</code>.</p>"},{"location":"plans/2025-01-30-telemetry-design/#fastapi_integration_example","title":"FastAPI Integration Example","text":"<p>Example code for users to integrate runnable with FastAPI and SSE streaming:</p> <pre><code>import asyncio\nimport json\nimport os\nfrom concurrent.futures import ThreadPoolExecutor\nfrom queue import Queue, Empty\n\nfrom fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse\nfrom pydantic import BaseModel\n\nfrom runnable import Pipeline\nfrom runnable.telemetry import set_stream_queue\n\napp = FastAPI()\nexecutor = ThreadPoolExecutor(max_workers=4)\n\n# Pipeline registry - knows the design-level config\nPIPELINE_REGISTRY = {\n    \"ml-training\": {\n        \"builder\": build_ml_training_pipeline,\n        \"parameters_file\": \"configs/ml-training.yaml\",\n    },\n}\n\n\nclass WorkflowRequest(BaseModel):\n    pipeline_name: str\n    user_parameters: dict = {}  # Runtime overrides -&gt; RUNNABLE_PRM_*\n\n\ndef run_pipeline(pipeline: Pipeline, parameters_file: str | None, user_params: dict):\n    \"\"\"Runs in thread pool (synchronous execution).\"\"\"\n    # Set user parameters as environment variables\n    for key, value in user_params.items():\n        env_key = f\"RUNNABLE_PRM_{key}\"\n        if isinstance(value, (dict, list)):\n            os.environ[env_key] = json.dumps(value)\n        else:\n            os.environ[env_key] = str(value)\n\n    try:\n        return pipeline.execute(parameters_file=parameters_file)\n    finally:\n        # Clean up env vars\n        for key in user_params:\n            env_key = f\"RUNNABLE_PRM_{key}\"\n            if env_key in os.environ:\n                del os.environ[env_key]\n\n\n@app.post(\"/run-workflow\")\nasync def run_workflow(request: WorkflowRequest):\n    span_queue = Queue()\n    set_stream_queue(span_queue)\n\n    pipeline_def = PIPELINE_REGISTRY[request.pipeline_name]\n    pipeline = pipeline_def[\"builder\"]()\n    parameters_file = pipeline_def[\"parameters_file\"]\n\n    async def event_stream():\n        loop = asyncio.get_event_loop()\n\n        future = loop.run_in_executor(\n            executor,\n            run_pipeline,\n            pipeline,\n            parameters_file,\n            request.user_parameters,\n        )\n\n        while not future.done():\n            try:\n                span_data = span_queue.get_nowait()\n                yield f\"data: {json.dumps(span_data)}\\n\\n\"\n            except Empty:\n                await asyncio.sleep(0.05)\n\n        # Drain remaining spans\n        while True:\n            try:\n                span_data = span_queue.get_nowait()\n                yield f\"data: {json.dumps(span_data)}\\n\\n\"\n            except Empty:\n                break\n\n        try:\n            result = future.result()\n            yield f\"data: {json.dumps({'type': 'complete', 'status': 'success'})}\\n\\n\"\n        except Exception as e:\n            yield f\"data: {json.dumps({'type': 'complete', 'status': 'error', 'error': str(e)})}\\n\\n\"\n        finally:\n            set_stream_queue(None)\n\n    return StreamingResponse(\n        event_stream(),\n        media_type=\"text/event-stream\",\n        headers={\"Cache-Control\": \"no-cache\", \"X-Accel-Buffering\": \"no\"}\n    )\n</code></pre>"},{"location":"plans/2025-01-30-telemetry-design/#dependencies","title":"Dependencies","text":"<p>pyproject.toml:</p> <pre><code>[project]\ndependencies = [\n    # ... existing deps ...\n    \"logfire-api&gt;=1.0.0\",  # Zero-dep telemetry shim\n]\n\n[project.optional-dependencies]\ntelemetry = [\n    \"logfire&gt;=1.0.0\",\n    \"opentelemetry-sdk&gt;=1.20\",\n    \"opentelemetry-exporter-otlp&gt;=1.20\",\n]\n</code></pre> <p>User installation:</p> <pre><code># Basic - logfire-api is always there, but no-ops without logfire\nuv add runnable\n\n# With telemetry support (self-hosted OTEL backend)\nuv add runnable[telemetry]\n\n# With Logfire hosted\nuv add runnable logfire\n</code></pre>"},{"location":"plans/2025-01-30-telemetry-design/#configuration","title":"Configuration","text":"<p>Standard OTEL environment variables - no runnable-specific config needed:</p> Variable Description Default <code>OTEL_EXPORTER_OTLP_ENDPOINT</code> Collector endpoint <code>http://localhost:4317</code> <code>OTEL_SERVICE_NAME</code> Service name in traces <code>runnable</code> <code>LOGFIRE_TOKEN</code> If using Logfire hosted -"},{"location":"plans/2025-01-30-telemetry-design/#files_to_modify","title":"Files to Modify","text":"Component File Description Telemetry module <code>runnable/telemetry.py</code> New file - StreamingSpanProcessor, helpers Pipeline span <code>runnable/context.py</code> Wrap <code>PipelineContext.execute()</code> Task spans <code>runnable/tasks.py</code> Wrap each task type's <code>execute_command()</code> Dependency <code>pyproject.toml</code> Add <code>logfire-api</code>, optional <code>[telemetry]</code> Example <code>examples/fastapi-telemetry/</code> FastAPI + SSE streaming example"},{"location":"plans/2025-01-30-telemetry-design/#phase_2_future","title":"Phase 2 (Future)","text":"<ul> <li>Graph traversal spans (parallel branches, map iterations, conditionals)</li> <li>Argo remote execution trace propagation</li> <li>Catalog operation spans</li> </ul>"},{"location":"plans/2025-01-30-telemetry-implementation/","title":"Telemetry Implementation Plan","text":"<p>For Claude: REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.</p> <p>Goal: Add OpenTelemetry-based telemetry to runnable for pipeline and task observability with FastAPI SSE streaming support.</p> <p>Architecture: Uses <code>logfire-api</code> as a zero-dependency shim that no-ops when logfire is not installed. A custom <code>StreamingSpanProcessor</code> enables dual output: collector export AND real-time SSE streaming to UI when FastAPI is the caller.</p> <p>Tech Stack: logfire-api, opentelemetry-sdk (optional), opentelemetry-exporter-otlp (optional)</p> <p>Design Document: <code>docs/plans/2025-01-30-telemetry-design.md</code></p>"},{"location":"plans/2025-01-30-telemetry-implementation/#task_1_add_logfire-api_dependency","title":"Task 1: Add logfire-api Dependency","text":"<p>Files: - Modify: <code>pyproject.toml:10-20</code> (dependencies section)</p> <p>Step 1: Add logfire-api to core dependencies</p> <p>In <code>pyproject.toml</code>, add <code>logfire-api</code> to the dependencies list:</p> <pre><code>dependencies = [\n    \"pydantic&gt;=2.10.3\",\n    \"ruamel-yaml&gt;=0.18.6\",\n    \"stevedore&gt;=5.4.0\",\n    \"rich&gt;=13.9.4\",\n    \"dill&gt;=0.3.9\",\n    \"setuptools&gt;=75.6.0\",\n    \"python-dotenv&gt;=1.0.1\",\n    \"typer&gt;=0.17.3\",\n    \"cloudpathlib&gt;=0.20.0\",\n    \"logfire-api&gt;=2.0.0\",\n]\n</code></pre> <p>Step 2: Add telemetry optional dependency group</p> <p>After the existing optional dependencies (around line 38), add:</p> <pre><code>telemetry = [\n    \"logfire&gt;=2.0.0\",\n    \"opentelemetry-sdk&gt;=1.20.0\",\n    \"opentelemetry-exporter-otlp&gt;=1.20.0\",\n]\n</code></pre> <p>Step 3: Sync dependencies</p> <p>Run: <code>uv sync --all-extras</code> Expected: Dependencies install successfully</p> <p>Step 4: Verify logfire-api is available</p> <p>Run: <code>uv run python -c \"import logfire_api; print('OK')\"</code> Expected: <code>OK</code></p> <p>Step 5: Commit</p> <pre><code>git add pyproject.toml uv.lock\ngit commit -m \"feat: add logfire-api dependency for telemetry support\"\n</code></pre>"},{"location":"plans/2025-01-30-telemetry-implementation/#task_2_create_telemetry_module_with_helpers","title":"Task 2: Create Telemetry Module with Helpers","text":"<p>Files: - Create: <code>runnable/telemetry.py</code> - Test: <code>tests/runnable/test_telemetry.py</code></p> <p>Step 1: Write the failing tests for helpers</p> <p>Create <code>tests/runnable/test_telemetry.py</code>:</p> <pre><code>import pytest\nfrom queue import Queue\n\n\nclass TestTruncateValue:\n    \"\"\"Tests for truncate_value helper function.\"\"\"\n\n    def test_truncate_short_string(self):\n        from runnable.telemetry import truncate_value\n\n        result = truncate_value({\"key\": \"value\"})\n        assert result == '{\"key\": \"value\"}'\n\n    def test_truncate_long_string(self):\n        from runnable.telemetry import truncate_value\n\n        long_value = {\"data\": \"x\" * 500}\n        result = truncate_value(long_value, max_bytes=50)\n        assert len(result) == 50\n        assert result.endswith(\"...\")\n\n    def test_truncate_unserializable(self):\n        from runnable.telemetry import truncate_value\n\n        class Unserializable:\n            pass\n\n        result = truncate_value(Unserializable())\n        assert \"&lt;unserializable:\" in result\n\n    def test_truncate_with_default_max_bytes(self):\n        from runnable.telemetry import truncate_value\n\n        # Default is 256 bytes\n        long_value = {\"data\": \"x\" * 1000}\n        result = truncate_value(long_value)\n        assert len(result) == 256\n\n\nclass TestStreamQueue:\n    \"\"\"Tests for stream queue context var helpers.\"\"\"\n\n    def test_set_and_get_stream_queue(self):\n        from runnable.telemetry import set_stream_queue, get_stream_queue\n\n        # Initially None\n        assert get_stream_queue() is None\n\n        # Set a queue\n        q = Queue()\n        set_stream_queue(q)\n        assert get_stream_queue() is q\n\n        # Clear it\n        set_stream_queue(None)\n        assert get_stream_queue() is None\n</code></pre> <p>Step 2: Run tests to verify they fail</p> <p>Run: <code>uv run pytest tests/runnable/test_telemetry.py -v</code> Expected: FAIL with <code>ModuleNotFoundError: No module named 'runnable.telemetry'</code></p> <p>Step 3: Write the telemetry module</p> <p>Create <code>runnable/telemetry.py</code>:</p> <pre><code>\"\"\"\nTelemetry support for runnable pipelines.\n\nUses logfire-api for zero-dependency instrumentation.\nIf logfire is installed, spans are emitted. If not, all calls are no-ops.\n\nFor real-time streaming (e.g., FastAPI SSE), use StreamingSpanProcessor.\n\"\"\"\n\nimport json\nfrom contextvars import ContextVar\nfrom queue import Queue\nfrom typing import Any, Optional\n\nimport logfire_api as logfire  # noqa: F401 - re-exported for convenience\n\n# Context var for active stream queue (set by FastAPI when SSE is active)\n_stream_queue: ContextVar[Optional[Queue]] = ContextVar(\"stream_queue\", default=None)\n\n\ndef truncate_value(value: Any, max_bytes: int = 256) -&gt; str:\n    \"\"\"\n    Truncate serialized value to max_bytes.\n\n    Args:\n        value: Any JSON-serializable value\n        max_bytes: Maximum length of the returned string\n\n    Returns:\n        JSON string, truncated with \"...\" if too long\n    \"\"\"\n    try:\n        serialized = json.dumps(value, default=str)\n        if len(serialized) &gt; max_bytes:\n            return serialized[: max_bytes - 3] + \"...\"\n        return serialized\n    except Exception:\n        return f\"&lt;unserializable: {type(value).__name__}&gt;\"\n\n\ndef set_stream_queue(q: Optional[Queue]) -&gt; None:\n    \"\"\"\n    Set the queue for streaming spans.\n\n    Called by FastAPI endpoint to enable real-time span streaming.\n\n    Args:\n        q: Queue to push span data to, or None to disable streaming\n    \"\"\"\n    _stream_queue.set(q)\n\n\ndef get_stream_queue() -&gt; Optional[Queue]:\n    \"\"\"\n    Get the current stream queue.\n\n    Returns:\n        The active Queue if SSE streaming is enabled, None otherwise\n    \"\"\"\n    return _stream_queue.get()\n</code></pre> <p>Step 4: Run tests to verify they pass</p> <p>Run: <code>uv run pytest tests/runnable/test_telemetry.py -v</code> Expected: All 5 tests PASS</p> <p>Step 5: Commit</p> <pre><code>git add runnable/telemetry.py tests/runnable/test_telemetry.py\ngit commit -m \"feat: add telemetry module with helpers\"\n</code></pre>"},{"location":"plans/2025-01-30-telemetry-implementation/#task_3_add_streamingspanprocessor","title":"Task 3: Add StreamingSpanProcessor","text":"<p>Files: - Modify: <code>runnable/telemetry.py</code> - Modify: <code>tests/runnable/test_telemetry.py</code></p> <p>Step 1: Write the failing tests for StreamingSpanProcessor</p> <p>Add to <code>tests/runnable/test_telemetry.py</code>:</p> <pre><code>class TestStreamingSpanProcessor:\n    \"\"\"Tests for StreamingSpanProcessor.\"\"\"\n\n    def test_processor_available_when_otel_installed(self):\n        \"\"\"StreamingSpanProcessor should be available when OTEL is installed.\"\"\"\n        try:\n            from runnable.telemetry import StreamingSpanProcessor, OTEL_AVAILABLE\n            if OTEL_AVAILABLE:\n                assert StreamingSpanProcessor is not None\n        except ImportError:\n            pytest.skip(\"OpenTelemetry not installed\")\n\n    def test_processor_pushes_to_queue_on_span_end(self):\n        \"\"\"Processor should push span data to queue when SSE is active.\"\"\"\n        try:\n            from opentelemetry.sdk.trace import TracerProvider\n            from opentelemetry.trace import StatusCode\n        except ImportError:\n            pytest.skip(\"OpenTelemetry not installed\")\n\n        from runnable.telemetry import (\n            StreamingSpanProcessor,\n            set_stream_queue,\n            get_stream_queue,\n            OTEL_AVAILABLE,\n        )\n\n        if not OTEL_AVAILABLE:\n            pytest.skip(\"OpenTelemetry not installed\")\n\n        # Setup\n        q = Queue()\n        set_stream_queue(q)\n\n        processor = StreamingSpanProcessor(base_processor=None)\n        provider = TracerProvider()\n        provider.add_span_processor(processor)\n\n        tracer = provider.get_tracer(\"test\")\n\n        # Create a span\n        with tracer.start_as_current_span(\"test-span\") as span:\n            span.set_attribute(\"test_attr\", \"test_value\")\n\n        # Verify queue received span data\n        assert not q.empty()\n\n        # Should have span_start and span_end\n        events = []\n        while not q.empty():\n            events.append(q.get_nowait())\n\n        assert len(events) == 2\n        assert events[0][\"type\"] == \"span_start\"\n        assert events[0][\"name\"] == \"test-span\"\n        assert events[1][\"type\"] == \"span_end\"\n        assert events[1][\"name\"] == \"test-span\"\n        assert \"duration_ms\" in events[1]\n\n        # Cleanup\n        set_stream_queue(None)\n\n    def test_processor_no_queue_no_error(self):\n        \"\"\"Processor should not error when no queue is set.\"\"\"\n        try:\n            from opentelemetry.sdk.trace import TracerProvider\n        except ImportError:\n            pytest.skip(\"OpenTelemetry not installed\")\n\n        from runnable.telemetry import (\n            StreamingSpanProcessor,\n            set_stream_queue,\n            OTEL_AVAILABLE,\n        )\n\n        if not OTEL_AVAILABLE:\n            pytest.skip(\"OpenTelemetry not installed\")\n\n        # Ensure no queue is set\n        set_stream_queue(None)\n\n        processor = StreamingSpanProcessor(base_processor=None)\n        provider = TracerProvider()\n        provider.add_span_processor(processor)\n\n        tracer = provider.get_tracer(\"test\")\n\n        # Should not raise\n        with tracer.start_as_current_span(\"test-span\"):\n            pass\n</code></pre> <p>Step 2: Run tests to verify they fail</p> <p>Run: <code>uv run pytest tests/runnable/test_telemetry.py::TestStreamingSpanProcessor -v</code> Expected: FAIL with <code>ImportError: cannot import name 'StreamingSpanProcessor'</code></p> <p>Step 3: Add StreamingSpanProcessor to telemetry module</p> <p>Add to <code>runnable/telemetry.py</code> at the end:</p> <pre><code># Optional OTEL imports for streaming processor\ntry:\n    from opentelemetry.sdk.trace import SpanProcessor, ReadableSpan\n    from opentelemetry.trace import StatusCode\n\n    OTEL_AVAILABLE = True\nexcept ImportError:\n    OTEL_AVAILABLE = False\n    SpanProcessor = object  # type: ignore\n    ReadableSpan = object  # type: ignore\n\n\nif OTEL_AVAILABLE:\n\n    class StreamingSpanProcessor(SpanProcessor):\n        \"\"\"\n        SpanProcessor that:\n        1. Always forwards to base processor (collector export) if provided\n        2. Also pushes to stream queue if SSE is active\n\n        This enables dual output: persistent collector storage AND\n        real-time streaming to UI.\n        \"\"\"\n\n        def __init__(self, base_processor: Optional[SpanProcessor] = None):\n            \"\"\"\n            Initialize the streaming processor.\n\n            Args:\n                base_processor: Optional underlying processor for collector export\n            \"\"\"\n            self.base_processor = base_processor\n\n        def on_start(self, span, parent_context=None):\n            \"\"\"Called when a span starts.\"\"\"\n            if self.base_processor:\n                self.base_processor.on_start(span, parent_context)\n\n            q = _stream_queue.get()\n            if q is not None:\n                q.put_nowait(\n                    {\n                        \"type\": \"span_start\",\n                        \"name\": span.name,\n                        \"span_id\": format(span.context.span_id, \"016x\"),\n                    }\n                )\n\n        def on_end(self, span: ReadableSpan):\n            \"\"\"Called when a span ends.\"\"\"\n            if self.base_processor:\n                self.base_processor.on_end(span)\n\n            q = _stream_queue.get()\n            if q is not None:\n                q.put_nowait(\n                    {\n                        \"type\": \"span_end\",\n                        \"name\": span.name,\n                        \"span_id\": format(span.context.span_id, \"016x\"),\n                        \"status\": span.status.status_code.name,\n                        \"duration_ms\": (span.end_time - span.start_time) / 1_000_000,\n                        \"attributes\": dict(span.attributes) if span.attributes else {},\n                    }\n                )\n\n        def shutdown(self):\n            \"\"\"Shutdown the processor.\"\"\"\n            if self.base_processor:\n                self.base_processor.shutdown()\n\n        def force_flush(self, timeout_millis=None):\n            \"\"\"Force flush any pending spans.\"\"\"\n            if self.base_processor:\n                self.base_processor.force_flush(timeout_millis)\n\nelse:\n    # Placeholder when OTEL is not installed\n    StreamingSpanProcessor = None  # type: ignore\n</code></pre> <p>Step 4: Run tests to verify they pass</p> <p>Run: <code>uv run pytest tests/runnable/test_telemetry.py -v</code> Expected: All tests PASS (some may skip if OTEL not installed)</p> <p>Step 5: Install telemetry extras and re-run</p> <p>Run: <code>uv sync --extra telemetry &amp;&amp; uv run pytest tests/runnable/test_telemetry.py -v</code> Expected: All tests PASS including StreamingSpanProcessor tests</p> <p>Step 6: Commit</p> <pre><code>git add runnable/telemetry.py tests/runnable/test_telemetry.py\ngit commit -m \"feat: add StreamingSpanProcessor for SSE streaming\"\n</code></pre>"},{"location":"plans/2025-01-30-telemetry-implementation/#task_4_add_telemetry_to_pipelinecontextexecute","title":"Task 4: Add Telemetry to PipelineContext.execute()","text":"<p>Files: - Modify: <code>runnable/context.py:1-10</code> (imports) - Modify: <code>runnable/context.py:416-451</code> (PipelineContext.execute method) - Modify: <code>tests/runnable/test_context.py</code></p> <p>Step 1: Write the failing test</p> <p>Add to <code>tests/runnable/test_context.py</code>:</p> <pre><code>class TestPipelineContextTelemetry:\n    \"\"\"Test telemetry integration in PipelineContext.\"\"\"\n\n    def test_execute_emits_pipeline_span(self, monkeypatch):\n        \"\"\"Test that execute() emits a pipeline span via logfire.\"\"\"\n        from unittest.mock import MagicMock, patch\n\n        # Mock logfire.span\n        mock_span_ctx = MagicMock()\n        mock_span_ctx.__enter__ = MagicMock(return_value=None)\n        mock_span_ctx.__exit__ = MagicMock(return_value=None)\n\n        with patch(\"runnable.context.logfire\") as mock_logfire:\n            mock_logfire.span.return_value = mock_span_ctx\n            mock_logfire.info = MagicMock()\n            mock_logfire.error = MagicMock()\n\n            # We can't easily test full execution, but we can verify the import works\n            from runnable.context import PipelineContext\n            import runnable.context as ctx_module\n\n            # Verify logfire is imported in the module\n            assert hasattr(ctx_module, \"logfire\")\n</code></pre> <p>Step 2: Run test to verify it fails</p> <p>Run: <code>uv run pytest tests/runnable/test_context.py::TestPipelineContextTelemetry -v</code> Expected: FAIL with <code>AttributeError: module 'runnable.context' has no attribute 'logfire'</code></p> <p>Step 3: Add logfire import to context.py</p> <p>At the top of <code>runnable/context.py</code>, after the existing imports (around line 10), add:</p> <pre><code>import logfire_api as logfire\n</code></pre> <p>Step 4: Modify PipelineContext.execute() method</p> <p>Replace the <code>execute</code> method in <code>PipelineContext</code> (lines 416-451) with:</p> <pre><code>    def execute(self):\n        assert self.dag is not None\n\n        with logfire.span(\n            \"pipeline:{pipeline_name}\",\n            pipeline_name=self.dag.name if hasattr(self.dag, \"name\") else \"unnamed\",\n            run_id=self.run_id,\n            executor=self.pipeline_executor.__class__.__name__,\n        ):\n            logfire.info(\"Pipeline execution started\")\n\n            console.print(\"Working with context:\")\n            console.print(run_context)\n            console.rule(style=\"[dark orange]\")\n\n            # Prepare for graph execution\n            if self.pipeline_executor._should_setup_run_log_at_traversal:\n                self.pipeline_executor._set_up_run_log(exists_ok=False)\n\n            try:\n                self.pipeline_executor.execute_graph(dag=self.dag)\n                if not self.pipeline_executor._should_setup_run_log_at_traversal:\n                    # non local executors just traverse the graph and do nothing\n                    logfire.info(\"Pipeline submitted\", status=\"submitted\")\n                    return {}\n\n                run_log = run_context.run_log_store.get_run_log_by_id(\n                    run_id=run_context.run_id, full=False\n                )\n\n                if run_log.status == defaults.SUCCESS:\n                    console.print(\n                        \"Pipeline executed successfully!\", style=defaults.success_style\n                    )\n                    logfire.info(\"Pipeline completed\", status=\"success\")\n                else:\n                    console.print(\"Pipeline execution failed.\", style=defaults.error_style)\n                    logfire.error(\"Pipeline failed\", status=\"failed\")\n                    raise exceptions.ExecutionFailedError(run_context.run_id)\n            except Exception as e:  # noqa: E722\n                console.print(e, style=defaults.error_style)\n                logfire.error(\"Pipeline failed with exception\", error=str(e)[:256])\n                raise\n\n            if self.pipeline_executor._should_setup_run_log_at_traversal:\n                return run_context.run_log_store.get_run_log_by_id(\n                    run_id=run_context.run_id\n                )\n</code></pre> <p>Step 5: Run test to verify it passes</p> <p>Run: <code>uv run pytest tests/runnable/test_context.py::TestPipelineContextTelemetry -v</code> Expected: PASS</p> <p>Step 6: Run all context tests to ensure no regressions</p> <p>Run: <code>uv run pytest tests/runnable/test_context.py -v</code> Expected: All tests PASS</p> <p>Step 7: Commit</p> <pre><code>git add runnable/context.py tests/runnable/test_context.py\ngit commit -m \"feat: add telemetry spans to PipelineContext.execute\"\n</code></pre>"},{"location":"plans/2025-01-30-telemetry-implementation/#task_5_add_telemetry_to_jobcontextexecute","title":"Task 5: Add Telemetry to JobContext.execute()","text":"<p>Files: - Modify: <code>runnable/context.py:503-523</code> (JobContext.execute method)</p> <p>Step 1: Modify JobContext.execute() method</p> <p>Replace the <code>execute</code> method in <code>JobContext</code> (lines 503-523) with:</p> <pre><code>    def execute(self):\n        with logfire.span(\n            \"job:{job_name}\",\n            job_name=self.job_definition_file,\n            run_id=self.run_id,\n            executor=self.job_executor.__class__.__name__,\n        ):\n            logfire.info(\"Job execution started\")\n\n            console.print(\"Working with context:\")\n            console.print(run_context)\n            console.rule(style=\"[dark orange]\")\n\n            try:\n                self.job_executor.submit_job(\n                    job=self.job, catalog_settings=self.catalog_settings\n                )\n                logfire.info(\"Job submitted\", status=\"submitted\")\n            except Exception as e:\n                logfire.error(\"Job failed\", error=str(e)[:256])\n                raise\n            finally:\n                console.print(f\"Job execution completed for run id: {self.run_id}\")\n\n            logger.info(\n                \"Executing the job from the user. We are still in the caller's compute environment\"\n            )\n\n            if self.job_executor._should_setup_run_log_at_traversal:\n                return run_context.run_log_store.get_run_log_by_id(\n                    run_id=run_context.run_id\n                )\n</code></pre> <p>Step 2: Run all context tests</p> <p>Run: <code>uv run pytest tests/runnable/test_context.py -v</code> Expected: All tests PASS</p> <p>Step 3: Commit</p> <pre><code>git add runnable/context.py\ngit commit -m \"feat: add telemetry spans to JobContext.execute\"\n</code></pre>"},{"location":"plans/2025-01-30-telemetry-implementation/#task_6_add_telemetry_to_pythontasktype","title":"Task 6: Add Telemetry to PythonTaskType","text":"<p>Files: - Modify: <code>runnable/tasks.py:1-31</code> (imports) - Modify: <code>runnable/tasks.py:332-422</code> (PythonTaskType.execute_command) - Modify: <code>tests/runnable/test_tasks.py</code></p> <p>Step 1: Write the failing test</p> <p>Add to <code>tests/runnable/test_tasks.py</code>:</p> <pre><code>class TestTaskTelemetry:\n    \"\"\"Test telemetry integration in task types.\"\"\"\n\n    def test_python_task_emits_span(self, mock_context):\n        \"\"\"Test that PythonTaskType emits telemetry span.\"\"\"\n        from unittest.mock import MagicMock, patch\n\n        mock_span_ctx = MagicMock()\n        mock_span_ctx.__enter__ = MagicMock(return_value=None)\n        mock_span_ctx.__exit__ = MagicMock(return_value=None)\n\n        with patch(\"runnable.tasks.logfire\") as mock_logfire:\n            mock_logfire.span.return_value = mock_span_ctx\n            mock_logfire.info = MagicMock()\n            mock_logfire.error = MagicMock()\n\n            # Setup mock for _context_node\n            mock_ctx, _ = mock_context\n            mock_node = MagicMock()\n            mock_node.name = \"test_task\"\n            mock_executor = MagicMock()\n            mock_executor._context_node = mock_node\n            mock_ctx.pipeline_executor = mock_executor\n\n            task = PythonTaskType(\n                command=\"math.sqrt\",\n                returns=[TaskReturns(name=\"result\", kind=\"json\")]\n            )\n\n            with patch(\"runnable.tasks.importlib.import_module\") as mock_import:\n                mock_module = MagicMock()\n                mock_module.sqrt = lambda x: x**0.5\n                mock_import.return_value = mock_module\n\n                task.execute_command()\n\n            # Verify span was created\n            mock_logfire.span.assert_called_once()\n            call_args = mock_logfire.span.call_args\n            assert \"task:\" in call_args[0][0]\n</code></pre> <p>Step 2: Run test to verify it fails</p> <p>Run: <code>uv run pytest tests/runnable/test_tasks.py::TestTaskTelemetry -v</code> Expected: FAIL (logfire not imported in tasks.py)</p> <p>Step 3: Add imports to tasks.py</p> <p>At the top of <code>runnable/tasks.py</code>, after the existing imports (around line 30), add:</p> <pre><code>import logfire_api as logfire\nfrom runnable.telemetry import truncate_value\n</code></pre> <p>Step 4: Modify PythonTaskType.execute_command()</p> <p>Replace the <code>execute_command</code> method in <code>PythonTaskType</code> (lines 332-422) with:</p> <pre><code>    def execute_command(\n        self,\n        map_variable: MapVariableType = None,\n    ) -&gt; StepAttempt:\n        \"\"\"Execute the notebook as defined by the command.\"\"\"\n        attempt_log = StepAttempt(\n            status=defaults.FAIL,\n            start_time=str(datetime.now()),\n            retry_indicator=self._context.retry_indicator,\n        )\n\n        # Get task name from context\n        task_name = \"unknown\"\n        if hasattr(self._context, \"pipeline_executor\") and hasattr(\n            self._context.pipeline_executor, \"_context_node\"\n        ):\n            node = self._context.pipeline_executor._context_node\n            if node:\n                task_name = node.name\n\n        with logfire.span(\n            \"task:{task_name}\",\n            task_name=task_name,\n            task_type=self.task_type,\n            command=self.command,\n        ):\n            with (\n                self.execution_context(map_variable=map_variable) as params,\n                self.expose_secrets() as _,\n            ):\n                logfire.info(\n                    \"Task started\",\n                    inputs=truncate_value(\n                        {k: v.get_value() if hasattr(v, \"get_value\") else str(v) for k, v in params.items()}\n                    ),\n                )\n\n                module, func = utils.get_module_and_attr_names(self.command)\n                sys.path.insert(0, os.getcwd())  # Need to add the current directory to path\n                imported_module = importlib.import_module(module)\n                f = getattr(imported_module, func)\n\n                try:\n                    try:\n                        filtered_parameters = parameters.filter_arguments_for_func(\n                            f, params.copy(), map_variable\n                        )\n                        logger.info(\n                            f\"Calling {func} from {module} with {filtered_parameters}\"\n                        )\n                        with redirect_output(console=task_console) as (\n                            buffer,\n                            stderr_buffer,\n                        ):\n                            user_set_parameters = f(\n                                **filtered_parameters\n                            )  # This is a tuple or single value\n                    except Exception as e:\n                        raise exceptions.CommandCallError(\n                            f\"Function call: {self.command} did not succeed.\\n\"\n                        ) from e\n                    finally:\n                        attempt_log.input_parameters = params.copy()\n                        if map_variable:\n                            attempt_log.input_parameters.update(\n                                {\n                                    k: JsonParameter(value=v, kind=\"json\")\n                                    for k, v in map_variable.items()\n                                }\n                            )\n\n                    if self.returns:\n                        if not isinstance(user_set_parameters, tuple):  # make it a tuple\n                            user_set_parameters = (user_set_parameters,)\n\n                        if len(user_set_parameters) != len(self.returns):\n                            raise ValueError(\n                                \"Returns task signature does not match the function returns\"\n                            )\n\n                        output_parameters: Dict[str, Parameter] = {}\n                        metrics: Dict[str, Parameter] = {}\n\n                        for i, task_return in enumerate(self.returns):\n                            output_parameter = task_return_to_parameter(\n                                task_return=task_return,\n                                value=user_set_parameters[i],\n                            )\n\n                            if task_return.kind == \"metric\":\n                                metrics[task_return.name] = output_parameter\n\n                            param_name = task_return.name\n                            if map_variable:\n                                for _, v in map_variable.items():\n                                    param_name = f\"{v}_{param_name}\"\n\n                            output_parameters[param_name] = output_parameter\n\n                        attempt_log.output_parameters = output_parameters\n                        attempt_log.user_defined_metrics = metrics\n                        params.update(output_parameters)\n\n                        logfire.info(\n                            \"Task completed\",\n                            outputs=truncate_value(\n                                {k: v.get_value() if hasattr(v, \"get_value\") else str(v) for k, v in output_parameters.items()}\n                            ),\n                            status=\"success\",\n                        )\n                    else:\n                        logfire.info(\"Task completed\", status=\"success\")\n\n                    attempt_log.status = defaults.SUCCESS\n                except Exception as _e:\n                    msg = f\"Call to the function {self.command} did not succeed.\\n\"\n                    attempt_log.message = msg\n                    task_console.print_exception(show_locals=False)\n                    task_console.log(_e, style=defaults.error_style)\n                    logfire.error(\"Task failed\", error=str(_e)[:256])\n\n        attempt_log.end_time = str(datetime.now())\n\n        return attempt_log\n</code></pre> <p>Step 5: Run test to verify it passes</p> <p>Run: <code>uv run pytest tests/runnable/test_tasks.py::TestTaskTelemetry -v</code> Expected: PASS</p> <p>Step 6: Run all task tests</p> <p>Run: <code>uv run pytest tests/runnable/test_tasks.py -v</code> Expected: All tests PASS</p> <p>Step 7: Commit</p> <pre><code>git add runnable/tasks.py tests/runnable/test_tasks.py\ngit commit -m \"feat: add telemetry spans to PythonTaskType\"\n</code></pre>"},{"location":"plans/2025-01-30-telemetry-implementation/#task_7_add_telemetry_to_notebooktasktype","title":"Task 7: Add Telemetry to NotebookTaskType","text":"<p>Files: - Modify: <code>runnable/tasks.py:506-617</code> (NotebookTaskType.execute_command)</p> <p>Step 1: Modify NotebookTaskType.execute_command()</p> <p>Replace the <code>execute_command</code> method in <code>NotebookTaskType</code> with telemetry-instrumented version. The key changes are:</p> <ol> <li>Add task name extraction at the start</li> <li>Wrap execution in <code>logfire.span()</code></li> <li>Add <code>logfire.info()</code> for start/completion</li> <li>Add <code>logfire.error()</code> for failures</li> </ol> <pre><code>    def execute_command(\n        self,\n        map_variable: MapVariableType = None,\n    ) -&gt; StepAttempt:\n        \"\"\"Execute the python notebook as defined by the command.\"\"\"\n        attempt_log = StepAttempt(\n            status=defaults.FAIL,\n            start_time=str(datetime.now()),\n            retry_indicator=self._context.retry_indicator,\n        )\n\n        # Get task name from context\n        task_name = \"unknown\"\n        if hasattr(self._context, \"pipeline_executor\") and hasattr(\n            self._context.pipeline_executor, \"_context_node\"\n        ):\n            node = self._context.pipeline_executor._context_node\n            if node:\n                task_name = node.name\n\n        with logfire.span(\n            \"task:{task_name}\",\n            task_name=task_name,\n            task_type=self.task_type,\n            command=self.command,\n        ):\n            try:\n                import ploomber_engine as pm\n                from ploomber_engine.ipython import PloomberClient\n\n                notebook_output_path = self.get_notebook_output_path(\n                    map_variable=map_variable\n                )\n\n                with (\n                    self.execution_context(\n                        map_variable=map_variable, allow_complex=False\n                    ) as params,\n                    self.expose_secrets() as _,\n                ):\n                    logfire.info(\n                        \"Notebook task started\",\n                        inputs=truncate_value(\n                            {k: v.get_value() if hasattr(v, \"get_value\") else str(v) for k, v in params.items()}\n                        ),\n                        notebook=self.command,\n                    )\n\n                    attempt_log.input_parameters = params.copy()\n                    copy_params = copy.deepcopy(params)\n\n                    if map_variable:\n                        for key, value in map_variable.items():\n                            copy_params[key] = JsonParameter(kind=\"json\", value=value)\n\n                    # Remove any {v}_unreduced parameters from the parameters\n                    unprocessed_params = [\n                        k for k, v in copy_params.items() if not v.reduced\n                    ]\n\n                    for key in list(copy_params.keys()):\n                        if any(key.endswith(f\"_{k}\") for k in unprocessed_params):\n                            del copy_params[key]\n\n                    notebook_params = {k: v.get_value() for k, v in copy_params.items()}\n\n                    ploomber_optional_args = self.optional_ploomber_args\n\n                    kwds = {\n                        \"input_path\": self.command,\n                        \"output_path\": notebook_output_path,\n                        \"parameters\": notebook_params,\n                        \"log_output\": True,\n                        \"progress_bar\": False,\n                    }\n                    kwds.update(ploomber_optional_args)\n\n                    with redirect_output(console=task_console) as (buffer, stderr_buffer):\n                        pm.execute_notebook(**kwds)\n\n                    context.run_context.catalog.put(name=notebook_output_path)\n\n                    client = PloomberClient.from_path(path=notebook_output_path)\n                    namespace = client.get_namespace()\n\n                    output_parameters: Dict[str, Parameter] = {}\n                    try:\n                        for task_return in self.returns:\n                            param_name = Template(task_return.name).safe_substitute(\n                                map_variable  # type: ignore\n                            )\n\n                            if map_variable:\n                                for _, v in map_variable.items():\n                                    param_name = f\"{v}_{param_name}\"\n\n                            output_parameters[param_name] = task_return_to_parameter(\n                                task_return=task_return,\n                                value=namespace[task_return.name],\n                            )\n                    except PicklingError as e:\n                        logger.exception(\"Notebooks cannot return objects\")\n                        logger.exception(e)\n                        logfire.error(\"Notebook pickling error\", error=str(e)[:256])\n                        raise\n\n                    if output_parameters:\n                        attempt_log.output_parameters = output_parameters\n                        params.update(output_parameters)\n                        logfire.info(\n                            \"Notebook task completed\",\n                            outputs=truncate_value(\n                                {k: v.get_value() if hasattr(v, \"get_value\") else str(v) for k, v in output_parameters.items()}\n                            ),\n                            status=\"success\",\n                        )\n                    else:\n                        logfire.info(\"Notebook task completed\", status=\"success\")\n\n                    attempt_log.status = defaults.SUCCESS\n\n            except (ImportError, Exception) as e:\n                msg = (\n                    f\"Call to the notebook command {self.command} did not succeed.\\n\"\n                    \"Ensure that you have installed runnable with notebook extras\"\n                )\n                logger.exception(msg)\n                logger.exception(e)\n                logfire.error(\"Notebook task failed\", error=str(e)[:256])\n\n                attempt_log.status = defaults.FAIL\n\n        attempt_log.end_time = str(datetime.now())\n\n        return attempt_log\n</code></pre> <p>Step 2: Run task tests</p> <p>Run: <code>uv run pytest tests/runnable/test_tasks.py -v</code> Expected: All tests PASS</p> <p>Step 3: Commit</p> <pre><code>git add runnable/tasks.py\ngit commit -m \"feat: add telemetry spans to NotebookTaskType\"\n</code></pre>"},{"location":"plans/2025-01-30-telemetry-implementation/#task_8_add_telemetry_to_shelltasktype","title":"Task 8: Add Telemetry to ShellTaskType","text":"<p>Files: - Modify: <code>runnable/tasks.py:686-822</code> (ShellTaskType.execute_command)</p> <p>Step 1: Modify ShellTaskType.execute_command()</p> <p>Add telemetry instrumentation following the same pattern as PythonTaskType:</p> <pre><code>    def execute_command(\n        self,\n        map_variable: MapVariableType = None,\n    ) -&gt; StepAttempt:\n        \"\"\"Execute the shell command as defined by the command.\"\"\"\n        attempt_log = StepAttempt(\n            status=defaults.FAIL,\n            start_time=str(datetime.now()),\n            retry_indicator=self._context.retry_indicator,\n        )\n\n        # Get task name from context\n        task_name = \"unknown\"\n        if hasattr(self._context, \"pipeline_executor\") and hasattr(\n            self._context.pipeline_executor, \"_context_node\"\n        ):\n            node = self._context.pipeline_executor._context_node\n            if node:\n                task_name = node.name\n\n        subprocess_env = {}\n\n        # Expose RUNNABLE environment variables to be passed to the subprocess.\n        for key, value in os.environ.items():\n            if key.startswith(\"RUNNABLE_\"):\n                subprocess_env[key] = value\n\n        # Expose map variable as environment variables\n        if map_variable:\n            for key, value in map_variable.items():  # type: ignore\n                subprocess_env[key] = str(value)\n\n        # Expose secrets as environment variables\n        if self.secrets:\n            for key in self.secrets:\n                secret_value = context.run_context.secrets.get(key)\n                subprocess_env[key] = secret_value\n\n        with logfire.span(\n            \"task:{task_name}\",\n            task_name=task_name,\n            task_type=self.task_type,\n            command=self.command[:100],  # Truncate long commands\n        ):\n            try:\n                with self.execution_context(\n                    map_variable=map_variable, allow_complex=False\n                ) as params:\n                    logfire.info(\n                        \"Shell task started\",\n                        inputs=truncate_value(\n                            {k: v.get_value() if hasattr(v, \"get_value\") else str(v) for k, v in params.items()}\n                        ),\n                    )\n\n                    subprocess_env.update({k: v.get_value() for k, v in params.items()})\n\n                    attempt_log.input_parameters = params.copy()\n                    # Json dumps all runnable environment variables\n                    for key, value in subprocess_env.items():\n                        if isinstance(value, str):\n                            continue\n                        subprocess_env[key] = json.dumps(value)\n\n                    collect_delimiter = \"=== COLLECT ===\"\n\n                    command = (\n                        self.command.strip() + f\" &amp;&amp; echo '{collect_delimiter}'  &amp;&amp; env\"\n                    )\n                    logger.info(f\"Executing shell command: {command}\")\n\n                    capture = False\n                    return_keys = {x.name: x for x in self.returns}\n\n                    proc = subprocess.Popen(\n                        command,\n                        shell=True,\n                        env=subprocess_env,\n                        stdout=subprocess.PIPE,\n                        stderr=subprocess.PIPE,\n                        text=True,\n                    )\n                    result = proc.communicate()\n                    logger.debug(result)\n                    logger.info(proc.returncode)\n\n                    if proc.returncode != 0:\n                        msg = \",\".join(result[1].split(\"\\n\"))\n                        task_console.print(msg, style=defaults.error_style)\n                        raise exceptions.CommandCallError(msg)\n\n                    # for stderr\n                    for line in result[1].split(\"\\n\"):\n                        if line.strip() == \"\":\n                            continue\n                        task_console.print(line, style=defaults.warning_style)\n\n                    output_parameters: Dict[str, Parameter] = {}\n                    metrics: Dict[str, Parameter] = {}\n\n                    # only from stdout\n                    for line in result[0].split(\"\\n\"):\n                        if line.strip() == \"\":\n                            continue\n\n                        logger.info(line)\n                        task_console.print(line)\n\n                        if line.strip() == collect_delimiter:\n                            # The lines from now on should be captured\n                            capture = True\n                            continue\n\n                        if capture:\n                            key, value = line.strip().split(\"=\", 1)\n                            if key in return_keys:\n                                task_return = return_keys[key]\n\n                                try:\n                                    value = json.loads(value)\n                                except json.JSONDecodeError:\n                                    value = value\n\n                                output_parameter = task_return_to_parameter(\n                                    task_return=task_return,\n                                    value=value,\n                                )\n\n                                if task_return.kind == \"metric\":\n                                    metrics[task_return.name] = output_parameter\n\n                                param_name = task_return.name\n                                if map_variable:\n                                    for _, v in map_variable.items():\n                                        param_name = f\"{v}_{param_name}\"\n\n                                output_parameters[param_name] = output_parameter\n\n                        attempt_log.output_parameters = output_parameters\n                        attempt_log.user_defined_metrics = metrics\n                        params.update(output_parameters)\n\n                    if output_parameters:\n                        logfire.info(\n                            \"Shell task completed\",\n                            outputs=truncate_value(\n                                {k: v.get_value() if hasattr(v, \"get_value\") else str(v) for k, v in output_parameters.items()}\n                            ),\n                            status=\"success\",\n                        )\n                    else:\n                        logfire.info(\"Shell task completed\", status=\"success\")\n\n                    attempt_log.status = defaults.SUCCESS\n            except exceptions.CommandCallError as e:\n                msg = f\"Call to the command {self.command} did not succeed\"\n                logger.exception(msg)\n                logger.exception(e)\n\n                task_console.log(msg, style=defaults.error_style)\n                task_console.log(e, style=defaults.error_style)\n                logfire.error(\"Shell task failed\", error=str(e)[:256])\n\n                attempt_log.status = defaults.FAIL\n\n        attempt_log.end_time = str(datetime.now())\n        return attempt_log\n</code></pre> <p>Step 2: Run all task tests</p> <p>Run: <code>uv run pytest tests/runnable/test_tasks.py -v</code> Expected: All tests PASS</p> <p>Step 3: Commit</p> <pre><code>git add runnable/tasks.py\ngit commit -m \"feat: add telemetry spans to ShellTaskType\"\n</code></pre>"},{"location":"plans/2025-01-30-telemetry-implementation/#task_9_export_telemetry_in_package_init","title":"Task 9: Export Telemetry in Package init","text":"<p>Files: - Modify: <code>runnable/__init__.py</code></p> <p>Step 1: Check current init.py exports</p> <p>Run: <code>cat runnable/__init__.py | head -50</code> Review what's currently exported.</p> <p>Step 2: Add telemetry exports</p> <p>Add to <code>runnable/__init__.py</code>:</p> <pre><code>from runnable.telemetry import (\n    set_stream_queue,\n    get_stream_queue,\n    truncate_value,\n)\n\n# Conditionally export StreamingSpanProcessor\ntry:\n    from runnable.telemetry import StreamingSpanProcessor, OTEL_AVAILABLE\nexcept ImportError:\n    StreamingSpanProcessor = None\n    OTEL_AVAILABLE = False\n</code></pre> <p>Step 3: Verify import works</p> <p>Run: <code>uv run python -c \"from runnable import set_stream_queue, get_stream_queue; print('OK')\"</code> Expected: <code>OK</code></p> <p>Step 4: Commit</p> <pre><code>git add runnable/__init__.py\ngit commit -m \"feat: export telemetry helpers from runnable package\"\n</code></pre>"},{"location":"plans/2025-01-30-telemetry-implementation/#task_10_run_full_test_suite","title":"Task 10: Run Full Test Suite","text":"<p>Files: None (verification only)</p> <p>Step 1: Run all tests</p> <p>Run: <code>uv run pytest tests/ -v --tb=short</code> Expected: All tests PASS</p> <p>Step 2: Run a simple pipeline example to verify telemetry doesn't break anything</p> <p>Run: <code>uv run python examples/01-tasks/python_tasks.py</code> Expected: Pipeline executes successfully (telemetry no-ops since logfire not configured)</p> <p>Step 3: Install telemetry extras and verify</p> <p>Run: <code>uv sync --extra telemetry &amp;&amp; uv run python -c \"from runnable.telemetry import StreamingSpanProcessor; print('StreamingSpanProcessor available')\"</code> Expected: <code>StreamingSpanProcessor available</code></p>"},{"location":"plans/2025-01-30-telemetry-implementation/#task_11_local_telemetry_test","title":"Task 11: Local Telemetry Test","text":"<p>Files: - Create: <code>examples/telemetry-local/simple_telemetry_test.py</code></p> <p>Goal: Validate telemetry works end-to-end without FastAPI complexity.</p> <p>Step 1: Create the local test script</p> <p>Create <code>examples/telemetry-local/simple_telemetry_test.py</code>:</p> <pre><code>\"\"\"\nSimple local test to verify telemetry is working.\n\nRun with:\n    uv run python examples/telemetry-local/simple_telemetry_test.py\n\"\"\"\n\nimport logfire\n\nfrom runnable import Pipeline, PythonTask, pickled\n\n\n# Configure logfire to output to console\nlogfire.configure(\n    send_to_logfire=False,  # Don't send to cloud\n    console=logfire.ConsoleOptions(\n        colors=\"auto\",\n        span_style=\"indented\",\n        include_timestamps=True,\n        verbose=True,\n    ),\n)\n\n\ndef step_one(x: int) -&gt; int:\n    \"\"\"First step - doubles the input.\"\"\"\n    print(f\"Step one: received x={x}\")\n    result = x * 2\n    print(f\"Step one: returning {result}\")\n    return result\n\n\ndef step_two(doubled: int) -&gt; str:\n    \"\"\"Second step - formats the result.\"\"\"\n    print(f\"Step two: received doubled={doubled}\")\n    result = f\"Final result: {doubled}\"\n    print(f\"Step two: returning '{result}'\")\n    return result\n\n\ndef main():\n    \"\"\"Run a simple pipeline and observe telemetry output.\"\"\"\n    print(\"=\" * 60)\n    print(\"Running pipeline with telemetry enabled\")\n    print(\"You should see spans for pipeline and each task\")\n    print(\"=\" * 60)\n    print()\n\n    pipeline = Pipeline(\n        steps=[\n            PythonTask(\n                function=step_one,\n                name=\"step_one\",\n                returns=[pickled(\"doubled\")],\n            ),\n            PythonTask(\n                function=step_two,\n                name=\"step_two\",\n                returns=[pickled(\"final_result\")],\n            ),\n        ]\n    )\n\n    # Execute with initial parameter\n    result = pipeline.execute(parameters_file=None)\n\n    print()\n    print(\"=\" * 60)\n    print(\"Pipeline completed!\")\n    print(f\"Result: {result}\")\n    print(\"=\" * 60)\n\n    return result\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Step 2: Create a parameters file for testing</p> <p>Create <code>examples/telemetry-local/params.yaml</code>:</p> <pre><code>x: 5\n</code></pre> <p>Step 3: Run the local test</p> <p>Run: <code>uv run python examples/telemetry-local/simple_telemetry_test.py</code></p> <p>Expected output should show: - Console output with colored/indented spans - <code>pipeline:*</code> span wrapping the execution - <code>task:step_one</code> span with inputs/outputs - <code>task:step_two</code> span with inputs/outputs - Timestamps and durations</p> <p>Step 4: Verify spans are visible</p> <p>If telemetry is working correctly, you should see output like: <pre><code>pipeline:unnamed\n\u251c\u2500\u2500 Pipeline execution started\n\u251c\u2500\u2500 task:step_one\n\u2502   \u251c\u2500\u2500 Task started inputs={\"x\": 5}\n\u2502   \u2514\u2500\u2500 Task completed outputs={\"doubled\": 10} status=success\n\u251c\u2500\u2500 task:step_two\n\u2502   \u251c\u2500\u2500 Task started inputs={\"doubled\": 10}\n\u2502   \u2514\u2500\u2500 Task completed outputs={\"final_result\": \"...\"} status=success\n\u2514\u2500\u2500 Pipeline completed status=success\n</code></pre></p> <p>Step 5: Commit</p> <pre><code>git add examples/telemetry-local/\ngit commit -m \"feat: add local telemetry test example\"\n</code></pre>"},{"location":"plans/2025-01-30-telemetry-implementation/#task_12_create_fastapi_example_with_logfire_integration","title":"Task 12: Create FastAPI Example with Logfire Integration","text":"<p>Files: - Create: <code>examples/fastapi-telemetry/README.md</code> - Create: <code>examples/fastapi-telemetry/main.py</code> - Create: <code>examples/fastapi-telemetry/pipelines.py</code></p> <p>Architecture: - <code>logfire.instrument_fastapi(app)</code> - instruments HTTP requests automatically - Our pipeline/task spans become child spans of HTTP request via OTEL context propagation - <code>StreamingSpanProcessor</code> - pushes span events to queue for real-time SSE streaming</p> <p>Step 1: Create example directory and README</p> <p>Create <code>examples/fastapi-telemetry/README.md</code>:</p> <pre><code># FastAPI Telemetry Streaming Example\n\nThis example demonstrates how to integrate runnable with FastAPI using logfire's\nbuilt-in FastAPI instrumentation plus real-time SSE streaming.\n\n## Prerequisites\n\n```bash\n# Install runnable with telemetry support\nuv add runnable[telemetry]\n\n# Install FastAPI with logfire integration\nuv add 'logfire[fastapi]' uvicorn\n</code></pre>"},{"location":"plans/2025-01-30-telemetry-implementation/#running_the_example","title":"Running the Example","text":"<pre><code># Start the FastAPI server\nuv run uvicorn examples.fastapi-telemetry.main:app --reload\n\n# In another terminal, trigger a workflow\ncurl -X POST http://localhost:8000/run-workflow \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"pipeline_name\": \"example\", \"user_parameters\": {\"x\": 10}}'\n</code></pre>"},{"location":"plans/2025-01-30-telemetry-implementation/#how_it_works","title":"How It Works","text":"<ol> <li><code>logfire.instrument_fastapi(app)</code> instruments all HTTP requests</li> <li>FastAPI receives a workflow request - this creates a parent span</li> <li>A <code>Queue</code> is set up for streaming spans via <code>set_stream_queue()</code></li> <li>The pipeline runs in a thread pool (runnable is synchronous)</li> <li>Pipeline/task spans are automatically children of the HTTP request span</li> <li><code>StreamingSpanProcessor</code> pushes span events to the queue</li> <li>FastAPI streams the events back to the client via SSE</li> </ol>"},{"location":"plans/2025-01-30-telemetry-implementation/#span_hierarchy","title":"Span Hierarchy","text":"<p><pre><code>HTTP POST /run-workflow (from logfire.instrument_fastapi)\n\u2514\u2500\u2500 pipeline:example\n    \u251c\u2500\u2500 task:compute\n    \u2514\u2500\u2500 task:finalize\n</code></pre> <pre><code>**Step 2: Create pipelines.py**\n\nCreate `examples/fastapi-telemetry/pipelines.py`:\n\n```python\n\"\"\"Example pipelines for FastAPI integration.\"\"\"\n\nfrom runnable import Pipeline, PythonTask, pickled\n\n\ndef compute(x: int) -&gt; int:\n    \"\"\"Simple compute function.\"\"\"\n    import time\n    time.sleep(1)  # Simulate work\n    return x * 2\n\n\ndef finalize(result: int) -&gt; str:\n    \"\"\"Finalize the result.\"\"\"\n    import time\n    time.sleep(0.5)\n    return f\"Result: {result}\"\n\n\ndef build_example_pipeline() -&gt; Pipeline:\n    \"\"\"Build an example pipeline.\"\"\"\n    return Pipeline(\n        steps=[\n            PythonTask(\n                function=compute,\n                name=\"compute\",\n                returns=[pickled(\"result\")],\n            ),\n            PythonTask(\n                function=finalize,\n                name=\"finalize\",\n                returns=[pickled(\"final\")],\n            ),\n        ]\n    )\n\n\n# Pipeline registry\nPIPELINE_REGISTRY = {\n    \"example\": {\n        \"builder\": build_example_pipeline,\n        \"parameters_file\": None,\n    },\n}\n</code></pre></p> <p>Step 3: Create main.py</p> <p>Create <code>examples/fastapi-telemetry/main.py</code>:</p> <pre><code>\"\"\"FastAPI integration example with telemetry streaming using logfire.\"\"\"\n\nimport asyncio\nimport json\nimport os\nfrom concurrent.futures import ThreadPoolExecutor\nfrom queue import Queue, Empty\n\nimport logfire\nfrom fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse\nfrom pydantic import BaseModel\n\nfrom runnable import Pipeline, set_stream_queue\nfrom runnable.telemetry import StreamingSpanProcessor\n\n# Import pipeline registry\nfrom .pipelines import PIPELINE_REGISTRY\n\n# Configure logfire with console output and StreamingSpanProcessor\nlogfire.configure(\n    send_to_logfire=False,  # Set True to send to logfire cloud\n    console=logfire.ConsoleOptions(\n        colors=\"auto\",\n        span_style=\"indented\",\n        verbose=True,\n    ),\n)\n\n# Create FastAPI app\napp = FastAPI(title=\"Runnable Telemetry Demo\")\n\n# Instrument FastAPI with logfire - this creates spans for HTTP requests\nlogfire.instrument_fastapi(app)\n\nexecutor = ThreadPoolExecutor(max_workers=4)\n\n\nclass WorkflowRequest(BaseModel):\n    pipeline_name: str\n    user_parameters: dict = {}\n\n\ndef run_pipeline(pipeline: Pipeline, parameters_file: str | None, user_params: dict):\n    \"\"\"Run pipeline in thread pool with user parameters as env vars.\"\"\"\n    # Set user parameters as environment variables\n    for key, value in user_params.items():\n        env_key = f\"RUNNABLE_PRM_{key}\"\n        if isinstance(value, (dict, list)):\n            os.environ[env_key] = json.dumps(value)\n        else:\n            os.environ[env_key] = str(value)\n\n    try:\n        return pipeline.execute(parameters_file=parameters_file)\n    finally:\n        # Clean up env vars\n        for key in user_params:\n            env_key = f\"RUNNABLE_PRM_{key}\"\n            if env_key in os.environ:\n                del os.environ[env_key]\n\n\n@app.post(\"/run-workflow\")\nasync def run_workflow(request: WorkflowRequest):\n    \"\"\"\n    Run a workflow with SSE streaming of telemetry.\n\n    The HTTP request span (from logfire.instrument_fastapi) is the parent.\n    Pipeline and task spans are automatically children of this request.\n    \"\"\"\n    if request.pipeline_name not in PIPELINE_REGISTRY:\n        return {\"error\": f\"Unknown pipeline: {request.pipeline_name}\"}\n\n    span_queue = Queue()\n    set_stream_queue(span_queue)\n\n    pipeline_def = PIPELINE_REGISTRY[request.pipeline_name]\n    pipeline = pipeline_def[\"builder\"]()\n    parameters_file = pipeline_def[\"parameters_file\"]\n\n    async def event_stream():\n        loop = asyncio.get_event_loop()\n\n        future = loop.run_in_executor(\n            executor,\n            run_pipeline,\n            pipeline,\n            parameters_file,\n            request.user_parameters,\n        )\n\n        while not future.done():\n            try:\n                span_data = span_queue.get_nowait()\n                yield f\"data: {json.dumps(span_data)}\\n\\n\"\n            except Empty:\n                await asyncio.sleep(0.05)\n\n        # Drain remaining spans\n        while True:\n            try:\n                span_data = span_queue.get_nowait()\n                yield f\"data: {json.dumps(span_data)}\\n\\n\"\n            except Empty:\n                break\n\n        try:\n            result = future.result()\n            yield f\"data: {json.dumps({'type': 'complete', 'status': 'success'})}\\n\\n\"\n        except Exception as e:\n            yield f\"data: {json.dumps({'type': 'complete', 'status': 'error', 'error': str(e)})}\\n\\n\"\n        finally:\n            set_stream_queue(None)\n\n    return StreamingResponse(\n        event_stream(),\n        media_type=\"text/event-stream\",\n        headers={\"Cache-Control\": \"no-cache\", \"X-Accel-Buffering\": \"no\"},\n    )\n\n\n@app.get(\"/pipelines\")\nasync def list_pipelines():\n    \"\"\"List available pipelines.\"\"\"\n    return {\"pipelines\": list(PIPELINE_REGISTRY.keys())}\n\n\nif __name__ == \"__main__\":\n    import uvicorn\n\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n</code></pre> <p>Step 4: Commit</p> <pre><code>git add examples/fastapi-telemetry/\ngit commit -m \"feat: add FastAPI telemetry streaming example\"\n</code></pre>"},{"location":"plans/2025-01-30-telemetry-implementation/#task_13_final_integration_test_and_commit","title":"Task 13: Final Integration Test and Commit","text":"<p>Step 1: Run full test suite</p> <p>Run: <code>uv run pytest tests/ -v</code> Expected: All tests PASS</p> <p>Step 2: Run pre-commit checks</p> <p>Run: <code>uv run pre-commit run --all-files</code> Expected: All checks PASS</p> <p>Step 3: Create final commit with all changes</p> <p>If any files weren't committed individually:</p> <pre><code>git add -A\ngit status\n# Review changes, then:\ngit commit -m \"feat: complete telemetry implementation phase 1\"\n</code></pre>"},{"location":"plans/2025-01-30-telemetry-implementation/#summary","title":"Summary","text":"<p>After completing all tasks, the following will be implemented:</p> <ol> <li><code>logfire-api</code> dependency added to core</li> <li><code>runnable/telemetry.py</code> module with:</li> <li><code>truncate_value()</code> helper</li> <li><code>set_stream_queue()</code> / <code>get_stream_queue()</code> for SSE streaming</li> <li><code>StreamingSpanProcessor</code> for dual output (collector + SSE)</li> <li><code>runnable/context.py</code> instrumented:</li> <li><code>PipelineContext.execute()</code> emits pipeline span</li> <li><code>JobContext.execute()</code> emits job span</li> <li><code>runnable/tasks.py</code> instrumented:</li> <li><code>PythonTaskType.execute_command()</code> emits task span</li> <li><code>NotebookTaskType.execute_command()</code> emits task span</li> <li><code>ShellTaskType.execute_command()</code> emits task span</li> <li>Local telemetry test in <code>examples/telemetry-local/</code> - validates telemetry works with console output</li> <li>FastAPI example in <code>examples/fastapi-telemetry/</code> - uses <code>logfire.instrument_fastapi()</code> + SSE streaming</li> <li>Tests for all telemetry functionality</li> </ol>"},{"location":"plans/2025-01-30-telemetry-implementation/#task_order","title":"Task Order","text":"Task Description Status 1 Add logfire-api dependency Pending 2 Create telemetry module with helpers Pending 3 Add StreamingSpanProcessor Pending 4 Add telemetry to PipelineContext.execute() Pending 5 Add telemetry to JobContext.execute() Pending 6 Add telemetry to PythonTaskType Pending 7 Add telemetry to NotebookTaskType Pending 8 Add telemetry to ShellTaskType Pending 9 Export telemetry in package init Pending 10 Run full test suite Pending 11 Local telemetry test Pending 12 FastAPI example with logfire integration Pending 13 Final integration test Pending"},{"location":"plans/2025-12-15-attempt-number-cleanup/","title":"Attempt Number Cleanup Implementation Plan","text":"<p>For Claude: REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.</p> <p>Goal: Clean up attempt number logic by extracting calculation into proper methods and removing deprecated step_attempt_number property</p> <p>Architecture: Replace the property-based approach with explicit attempt number calculation based on existing step logs in the run log store</p> <p>Tech Stack: Python, pytest for testing</p>"},{"location":"plans/2025-12-15-attempt-number-cleanup/#task_1_extract_attempt_number_calculation_method","title":"Task 1: Extract Attempt Number Calculation Method","text":"<p>Files: - Modify: <code>extensions/pipeline_executor/__init__.py:224-238</code> - Test: <code>tests/extensions/test_pipeline_executor.py</code></p> <p>Step 1: Write the failing test</p> <pre><code>def test_calculate_attempt_number_first_attempt(mock_context, generic_executor):\n    \"\"\"Test calculating attempt number for first execution\"\"\"\n    from runnable import exceptions\n\n    mock_context.run_log_store.get_step_log.side_effect = exceptions.StepLogNotFoundError(\"Not found\")\n    node = MagicMock()\n    node._get_step_log_name.return_value = \"test_step\"\n\n    attempt_num = generic_executor._calculate_attempt_number(node, None)\n\n    assert attempt_num == 1\n\ndef test_calculate_attempt_number_with_existing_attempts(mock_context, generic_executor):\n    \"\"\"Test calculating attempt number when previous attempts exist\"\"\"\n    from runnable.datastore import StepLog, StepAttempt\n\n    # Mock existing step log with 2 attempts\n    mock_step_log = StepLog(\n        name=\"test_step\",\n        internal_name=\"test_step\",\n        attempts=[\n            StepAttempt(attempt_number=1, status=\"FAILED\"),\n            StepAttempt(attempt_number=2, status=\"FAILED\")\n        ]\n    )\n    mock_context.run_log_store.get_step_log.return_value = mock_step_log\n\n    node = MagicMock()\n    node._get_step_log_name.return_value = \"test_step\"\n\n    attempt_num = generic_executor._calculate_attempt_number(node, None)\n\n    assert attempt_num == 3\n</code></pre> <p>Step 2: Run test to verify it fails</p> <p>Run: <code>pytest tests/extensions/test_pipeline_executor.py::test_calculate_attempt_number_first_attempt -v</code> Expected: FAIL with \"AttributeError: '_calculate_attempt_number' method not found\"</p> <p>Step 3: Implement the minimal method</p> <p>Add to <code>GenericPipelineExecutor</code> class in <code>extensions/pipeline_executor/__init__.py</code>:</p> <pre><code>def _calculate_attempt_number(self, node: BaseNode, map_variable: dict = None) -&gt; int:\n    \"\"\"\n    Calculate the attempt number for a node based on existing attempts in the run log.\n\n    Args:\n        node: The node to calculate attempt number for\n        map_variable: Optional map variable if node is in a map state\n\n    Returns:\n        int: The attempt number (starting from 1)\n    \"\"\"\n    step_log_name = node._get_step_log_name(map_variable)\n\n    try:\n        existing_step_log = self._context.run_log_store.get_step_log(\n            step_log_name, self._context.run_id\n        )\n        # If step log exists, increment attempt number based on existing attempts\n        return len(existing_step_log.attempts) + 1\n    except exceptions.StepLogNotFoundError:\n        # This is the first attempt, use attempt number 1\n        return 1\n</code></pre> <p>Step 4: Run test to verify it passes</p> <p>Run: <code>pytest tests/extensions/test_pipeline_executor.py::test_calculate_attempt_number_first_attempt -v</code> Expected: PASS</p> <p>Step 5: Commit</p> <pre><code>git add tests/extensions/test_pipeline_executor.py extensions/pipeline_executor/__init__.py\ngit commit -m \"feat: add _calculate_attempt_number method to pipeline executor\"\n</code></pre>"},{"location":"plans/2025-12-15-attempt-number-cleanup/#task_2_update_execute_node_method_to_use_new_logic","title":"Task 2: Update execute_node Method to Use New Logic","text":"<p>Files: - Modify: <code>extensions/pipeline_executor/__init__.py:224-246</code></p> <p>Step 1: Write test for execute_node using new logic</p> <p>Add to existing test file:</p> <pre><code>def test_execute_node_uses_calculated_attempt_number(mock_context, generic_executor):\n    \"\"\"Test that execute_node uses the calculated attempt number\"\"\"\n    import os\n    from runnable import defaults\n\n    node = MagicMock()\n    node._get_step_log_name.return_value = \"test_step\"\n    node.execute.return_value = MagicMock()\n\n    # Mock the attempt calculation method\n    generic_executor._calculate_attempt_number = MagicMock(return_value=3)\n\n    generic_executor.execute_node(node)\n\n    # Verify attempt number was calculated and used\n    generic_executor._calculate_attempt_number.assert_called_once_with(node, None)\n    node.execute.assert_called_once_with(\n        map_variable=None,\n        attempt_number=3,\n        mock=False\n    )\n    # Verify environment variable was set\n    assert os.environ[defaults.ATTEMPT_NUMBER] == \"3\"\n</code></pre> <p>Step 2: Run test to verify it fails</p> <p>Run: <code>pytest tests/extensions/test_pipeline_executor.py::test_execute_node_uses_calculated_attempt_number -v</code> Expected: FAIL with assertion error on environment variable or method calls</p> <p>Step 3: Update execute_node method</p> <p>Replace the inline logic (lines 224-246) in <code>execute_node</code> method:</p> <pre><code>def execute_node(self, node: BaseNode, map_variable: dict = None, mock: bool = False):\n    \"\"\"\n    Executes a given node.\n\n    Args:\n        node (Node): The node to execute\n        map_variable (dict, optional): If the node is of a map state, map_variable is the value of the iterable.\n                    Defaults to None.\n    \"\"\"\n    # Calculate attempt number based on existing attempts in run log\n    current_attempt_number = self._calculate_attempt_number(node, map_variable)\n\n    # Set the environment variable for this attempt\n    os.environ[defaults.ATTEMPT_NUMBER] = str(current_attempt_number)\n\n    logger.info(\n        f\"Trying to execute node: {node.internal_name}, attempt : {current_attempt_number}\"\n    )\n\n    self._context_node = node\n    self._context_attempt_number = current_attempt_number\n\n    # ... rest of method unchanged ...\n\n    step_log = node.execute(\n        map_variable=map_variable,\n        attempt_number=current_attempt_number,\n        mock=mock,\n    )\n</code></pre> <p>Step 4: Run test to verify it passes</p> <p>Run: <code>pytest tests/extensions/test_pipeline_executor.py::test_execute_node_uses_calculated_attempt_number -v</code> Expected: PASS</p> <p>Step 5: Commit</p> <pre><code>git add extensions/pipeline_executor/__init__.py\ngit commit -m \"refactor: use extracted attempt calculation logic in execute_node\"\n</code></pre>"},{"location":"plans/2025-12-15-attempt-number-cleanup/#task_3_remove_step_attempt_number_property_from_baseexecutor","title":"Task 3: Remove step_attempt_number Property from BaseExecutor","text":"<p>Files: - Modify: <code>runnable/executor.py:67-85</code> - Test: <code>tests/runnable/test_executor.py</code></p> <p>Step 1: Check current usage of the property</p> <p>Run: <code>grep -r \"step_attempt_number\" --include=\"*.py\" .</code> Expected: Find all references to remove or update</p> <p>Step 2: Write test for property removal</p> <p>Add to test file (if not already covered):</p> <pre><code>def test_base_executor_no_step_attempt_number():\n    \"\"\"Test that BaseExecutor no longer has step_attempt_number property\"\"\"\n    from runnable.executor import BaseExecutor\n\n    # This should not exist anymore\n    assert not hasattr(BaseExecutor, 'step_attempt_number')\n</code></pre> <p>Step 3: Run test to verify current state</p> <p>Run: <code>pytest tests/runnable/test_executor.py::test_base_executor_no_step_attempt_number -v</code> Expected: FAIL because property still exists</p> <p>Step 4: Remove the property and TODO</p> <p>Remove lines 67-85 from <code>runnable/executor.py</code>:</p> <pre><code># Remove this entire section:\n# TODO: we might have to remove this in favor of a correct implementation in the pipeline executor\n@property\ndef step_attempt_number(self) -&gt; int:\n    \"\"\"\n    The attempt number of the current step.\n    Orchestrators should use this step to submit multiple attempts of the job.\n\n    Returns:\n        int: The attempt number of the current step.\n    \"\"\"\n    run_id = BaseExecutor._get_run_id_from_environment()\n\n    # TODO: Maybe this should be part of another class so it is mockable\n    runnable_object = FailureMode()\n\n    return runnable_object.step_attempt_number or 1\n</code></pre> <p>Step 5: Run test to verify it passes</p> <p>Run: <code>pytest tests/runnable/test_executor.py::test_base_executor_no_step_attempt_number -v</code> Expected: PASS</p> <p>Step 6: Commit</p> <pre><code>git add runnable/executor.py tests/runnable/test_executor.py\ngit commit -m \"refactor: remove deprecated step_attempt_number property from BaseExecutor\"\n</code></pre>"},{"location":"plans/2025-12-15-attempt-number-cleanup/#task_4_update_job_executor_to_remove_step_attempt_number_usage","title":"Task 4: Update Job Executor to Remove step_attempt_number Usage","text":"<p>Files: - Modify: <code>extensions/job_executor/__init__.py:112-125</code></p> <p>Step 1: Check the current implementation</p> <p>Review the step_attempt_number property in job executor to understand its usage</p> <p>Step 2: Write test for job executor without property</p> <pre><code>def test_job_executor_no_step_attempt_number():\n    \"\"\"Test that job executor doesn't use step_attempt_number property\"\"\"\n    from extensions.job_executor import BaseJobExecutor\n\n    # Should not have this property anymore\n    assert not hasattr(BaseJobExecutor, 'step_attempt_number')\n</code></pre> <p>Step 3: Run test to verify current state</p> <p>Run: <code>pytest tests/extensions/test_job_executor.py::test_job_executor_no_step_attempt_number -v</code> Expected: FAIL because property still exists</p> <p>Step 4: Remove the property from job executor</p> <p>Remove the step_attempt_number property from <code>BaseJobExecutor</code> class:</p> <pre><code># Remove this property entirely from BaseJobExecutor\n</code></pre> <p>Step 5: Update any references in job executor</p> <p>If job executor needs attempt numbers, update it to receive them as parameters rather than using the property.</p> <p>Step 6: Run test to verify it passes</p> <p>Run: <code>pytest tests/extensions/test_job_executor.py::test_job_executor_no_step_attempt_number -v</code> Expected: PASS</p> <p>Step 7: Commit</p> <pre><code>git add extensions/job_executor/__init__.py tests/extensions/test_job_executor.py\ngit commit -m \"refactor: remove step_attempt_number property from job executor\"\n</code></pre>"},{"location":"plans/2025-12-15-attempt-number-cleanup/#task_5_integration_testing_and_cleanup","title":"Task 5: Integration Testing and Cleanup","text":"<p>Files: - Test: Run full test suite - Modify: Remove any remaining TODOs</p> <p>Step 1: Remove resolved TODOs</p> <p>Remove the completed TODO comments from <code>extensions/pipeline_executor/__init__.py</code>:</p> <pre><code># Remove these lines:\n# TODO: Move this to a different function,\n# TODO: Remove the property step_attempt_number and use this logic\n</code></pre> <p>Step 2: Run full test suite</p> <p>Run: <code>pytest tests/ -v</code> Expected: All tests pass</p> <p>Step 3: Test with example pipeline</p> <p>Run: <code>uv run examples/01-tasks/python_tasks.py</code> Expected: Pipeline executes successfully with correct attempt numbers in logs</p> <p>Step 4: Verify attempt number environment variable</p> <p>Add a simple test to verify the attempt number environment variable is set correctly during execution.</p> <p>Step 5: Final commit</p> <pre><code>git add .\ngit commit -m \"cleanup: remove resolved TODOs for attempt number refactoring\"\n</code></pre>"},{"location":"plans/2025-12-15-attempt-number-cleanup/#task_6_documentation_update","title":"Task 6: Documentation Update","text":"<p>Files: - Create: <code>docs/architecture/attempt-number-handling.md</code></p> <p>Step 1: Document the new approach</p> <p>Create documentation explaining how attempt numbers are now calculated:</p> <pre><code># Attempt Number Handling\n\n## Overview\n\nAttempt numbers are calculated dynamically based on existing attempts in the run log store.\n\n## Implementation\n\nThe `GenericPipelineExecutor` calculates attempt numbers by:\n1. Querying the run log store for existing attempts\n2. Counting the number of previous attempts\n3. Setting the next attempt number as count + 1\n\nThis approach ensures accurate attempt tracking without maintaining state in executor properties.\n</code></pre> <p>Step 2: Commit documentation</p> <pre><code>git add docs/architecture/attempt-number-handling.md\ngit commit -m \"docs: add attempt number handling documentation\"\n</code></pre>"},{"location":"plans/2025-12-19-argo-configmap-memoization-design/","title":"Argo ConfigMap Memoization Design","text":""},{"location":"plans/2025-12-19-argo-configmap-memoization-design/#overview","title":"Overview","text":"<p>Extend the existing Argo executor's memoization schema to include ConfigMap cache support, completing compliance with Argo Workflows' native memoization specification while enabling persistent caching for workflow resubmission.</p>"},{"location":"plans/2025-12-19-argo-configmap-memoization-design/#current_state","title":"Current State","text":"<p>The existing <code>Memoize</code> class only supports cache keys: <pre><code>class Memoize(BaseModelWIthConfig):\n    key: str\n</code></pre></p> <p>Generated workflows produce incomplete memoization configuration: <pre><code>memoize:\n  key: \"{{workflow.parameters.run_id}}\"\n  # Missing required cache configuration\n</code></pre></p>"},{"location":"plans/2025-12-19-argo-configmap-memoization-design/#requirements","title":"Requirements","text":"<ol> <li>Argo Compliance: Match Argo Workflows' official memoization specification</li> <li>Persistent Caching: ConfigMaps persist across workflow runs for resubmit functionality</li> <li>Automatic Management: Argo controller creates ConfigMaps as needed</li> <li>Backward Compatibility: Existing workflows continue working unchanged</li> <li>Workflow Isolation: Each workflow uses its own ConfigMap for cache isolation</li> </ol>"},{"location":"plans/2025-12-19-argo-configmap-memoization-design/#architecture","title":"Architecture","text":""},{"location":"plans/2025-12-19-argo-configmap-memoization-design/#enhanced_schema","title":"Enhanced Schema","text":"<pre><code>class ConfigMapCache(BaseModelWIthConfig):\n    name: str\n\nclass Cache(BaseModelWIthConfig):\n    config_map: ConfigMapCache\n\nclass Memoize(BaseModelWIthConfig):\n    key: str\n    cache: Cache\n</code></pre>"},{"location":"plans/2025-12-19-argo-configmap-memoization-design/#generated_yaml_structure","title":"Generated YAML Structure","text":"<pre><code>memoize:\n  key: \"{{workflow.parameters.run_id}}\"\n  cache:\n    configMap:\n      name: \"runnable-x7k9m2\"\n</code></pre>"},{"location":"plans/2025-12-19-argo-configmap-memoization-design/#configmap_naming_strategy","title":"ConfigMap Naming Strategy","text":"<ul> <li>Pattern: <code>runnable-&lt;6-random-chars&gt;</code> per workflow</li> <li>Generation: Once per <code>ArgoExecutor</code> instance</li> <li>Sharing: Same ConfigMap used across all templates in workflow</li> <li>Persistence: ConfigMaps remain after workflow completion for resubmit</li> </ul>"},{"location":"plans/2025-12-19-argo-configmap-memoization-design/#implementation_details","title":"Implementation Details","text":""},{"location":"plans/2025-12-19-argo-configmap-memoization-design/#argoexecutor_enhancement","title":"ArgoExecutor Enhancement","text":"<pre><code>class ArgoExecutor(GenericPipelineExecutor):\n    def __init__(self, ...):\n        super().__init__(...)\n        self._cache_name = self._generate_cache_name()\n\n    def _generate_cache_name(self) -&gt; str:\n        import secrets\n        import string\n        chars = string.ascii_lowercase + string.digits\n        return f\"runnable-{''.join(secrets.choice(chars) for _ in range(6))}\"\n\n    @property\n    def cache_name(self) -&gt; str:\n        return self._cache_name\n</code></pre>"},{"location":"plans/2025-12-19-argo-configmap-memoization-design/#template_generation_updates","title":"Template Generation Updates","text":"<p>Both <code>_create_container_template()</code> and <code>_create_nested_template()</code> methods will use:</p> <pre><code>memoize=Memoize(\n    key=\"{{workflow.parameters.run_id}}\",\n    cache=Cache(config_map=ConfigMapCache(name=self.cache_name))\n)\n</code></pre>"},{"location":"plans/2025-12-19-argo-configmap-memoization-design/#step_isolation","title":"Step Isolation","text":"<ul> <li>Single ConfigMap per workflow: <code>runnable-x7k9m2</code></li> <li>Argo's internal memoization logic handles step differentiation</li> <li>Cache keys combined with template names provide automatic isolation</li> <li>No manual step-level cache management required</li> </ul>"},{"location":"plans/2025-12-19-argo-configmap-memoization-design/#technical_considerations","title":"Technical Considerations","text":""},{"location":"plans/2025-12-19-argo-configmap-memoization-design/#error_handling","title":"Error Handling","text":"<ul> <li>ConfigMap Creation: Argo controller handles creation failures gracefully</li> <li>Size Limits: 1MB ConfigMap limit managed by Argo (entry eviction/overflow)</li> <li>Permissions: Requires <code>configmaps</code> create/read permissions for Argo service account</li> </ul>"},{"location":"plans/2025-12-19-argo-configmap-memoization-design/#backward_compatibility","title":"Backward Compatibility","text":"<ul> <li>Existing <code>Memoize(key=\"...\")</code> usage remains valid</li> <li>No breaking changes to current API</li> <li>New cache functionality enabled automatically for all workflows</li> </ul>"},{"location":"plans/2025-12-19-argo-configmap-memoization-design/#cache_lifecycle","title":"Cache Lifecycle","text":"<ul> <li>Creation: Argo controller creates ConfigMaps on first memoization access</li> <li>Persistence: ConfigMaps persist indefinitely for resubmit functionality</li> <li>Cleanup: Manual cleanup outside of workflow execution (if desired)</li> </ul>"},{"location":"plans/2025-12-19-argo-configmap-memoization-design/#benefits","title":"Benefits","text":"<ol> <li>Full Argo Compliance: Complete implementation of Argo's memoization specification</li> <li>True Memoization: Persistent cache enables cross-workflow-run optimization</li> <li>Seamless Integration: No configuration changes required for existing workflows</li> <li>Resubmit Support: Cached results available for workflow resubmission scenarios</li> <li>Workflow Isolation: Per-workflow ConfigMaps prevent cache collision</li> </ol>"},{"location":"plans/2025-12-19-argo-configmap-memoization-design/#testing_strategy","title":"Testing Strategy","text":"<ol> <li>Schema Tests: Verify YAML serialization matches Argo specification</li> <li>Name Generation: Test ConfigMap name uniqueness and format</li> <li>Argo Validation: Ensure <code>argo lint</code> passes for generated workflows</li> <li>Integration Tests: Verify memoization functionality in actual workflow runs</li> <li>Resubmit Tests: Confirm cached results used in resubmitted workflows</li> </ol>"},{"location":"plans/2025-12-19-argo-configmap-memoization-design/#migration_path","title":"Migration Path","text":"<p>No migration required - enhancement is additive: - Current workflows automatically get enhanced memoization - No configuration changes needed - Backward compatibility maintained</p>"},{"location":"plans/2025-12-19-argo-configmap-memoization-design/#technical_notes","title":"Technical Notes","text":"<ul> <li>ConfigMap names provide sufficient uniqueness for concurrent workflows</li> <li>Argo service account requires ConfigMap permissions (typically already present)</li> <li>Cache persistence aligns with resubmit workflow design patterns</li> <li>Generated cache names are deterministic per executor instance but unique across instances</li> </ul>"},{"location":"plans/2025-12-19-configmap-memoization-implementation/","title":"ConfigMap Memoization Implementation Plan","text":"<p>For Claude: REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.</p> <p>Goal: Add ConfigMap cache support to Argo memoization schema for persistent caching across workflow resubmissions</p> <p>Architecture: Extend existing Memoize class with ConfigMapCache and Cache models, add cache name generation to ArgoExecutor, update template creation methods to include cache configuration</p> <p>Tech Stack: Python, Pydantic, Argo Workflows YAML generation, practical testing with example pipelines</p>"},{"location":"plans/2025-12-19-configmap-memoization-implementation/#task_1_extend_memoization_schema_classes","title":"Task 1: Extend Memoization Schema Classes","text":"<p>Files: - Modify: <code>extensions/pipeline_executor/argo.py:114-116</code></p> <p>Step 1: Implement the new schema classes</p> <pre><code># extensions/pipeline_executor/argo.py (after line 116)\nclass ConfigMapCache(BaseModelWIthConfig):\n    name: str\n\n\nclass Cache(BaseModelWIthConfig):\n    config_map: ConfigMapCache\n\n\n# Modify existing Memoize class (line 114-116)\nclass Memoize(BaseModelWIthConfig):\n    key: str\n    cache: Optional[Cache] = Field(default=None)\n</code></pre> <p>Step 2: Test schema with simple verification</p> <p>Run: <code>uv run python -c \" from extensions.pipeline_executor.argo import Memoize, Cache, ConfigMapCache cache = Cache(config_map=ConfigMapCache(name='test-123')) memoize = Memoize(key='test-key', cache=cache) print('Schema works:', memoize.model_dump()) \"</code> Expected: Should print schema dict with cache structure</p> <p>Step 3: Commit</p> <pre><code>git add extensions/pipeline_executor/argo.py\ngit commit -m \"feat: add ConfigMap cache support to Memoize schema\"\n</code></pre>"},{"location":"plans/2025-12-19-configmap-memoization-implementation/#task_2_add_cache_name_generation_to_argoexecutor","title":"Task 2: Add Cache Name Generation to ArgoExecutor","text":"<p>Files: - Modify: <code>extensions/pipeline_executor/argo.py</code> (ArgoExecutor class)</p> <p>Step 1: Add cache name generation methods to ArgoExecutor</p> <pre><code># extensions/pipeline_executor/argo.py (in ArgoExecutor __init__ method)\ndef __init__(self, config: dict):\n    # Add after existing __init__ logic\n    super().__init__(config)\n    # ... existing initialization ...\n    self._cache_name = self._generate_cache_name()\n\ndef _generate_cache_name(self) -&gt; str:\n    \"\"\"Generate a unique ConfigMap name for this workflow's cache.\"\"\"\n    import secrets\n    import string\n    chars = string.ascii_lowercase + string.digits\n    suffix = ''.join(secrets.choice(chars) for _ in range(6))\n    return f\"runnable-{suffix}\"\n\n@property\ndef cache_name(self) -&gt; str:\n    \"\"\"Get the ConfigMap name for this workflow's memoization cache.\"\"\"\n    return self._cache_name\n</code></pre> <p>Step 2: Test cache name generation</p> <p>Run: `uv run python -c \" from extensions.pipeline_executor.argo import ArgoExecutor import re</p>"},{"location":"plans/2025-12-19-configmap-memoization-implementation/#test_with_minimal_config","title":"Test with minimal config","text":"<p>config = {'pvc_for_runnable': 'test', 'defaults': {}} executor = ArgoExecutor(config) name = executor.cache_name print('Cache name:', name) assert re.match(r'^runnable-[a-z0-9]{6}$', name), f'Invalid format: {name}' print('Format validation passed') \"` Expected: Should print valid cache name and pass format validation</p> <p>Step 3: Commit</p> <pre><code>git add extensions/pipeline_executor/argo.py\ngit commit -m \"feat: add ConfigMap cache name generation to ArgoExecutor\"\n</code></pre>"},{"location":"plans/2025-12-19-configmap-memoization-implementation/#task_3_update_template_creation_methods_with_cache","title":"Task 3: Update Template Creation Methods with Cache","text":"<p>Files: - Modify: <code>extensions/pipeline_executor/argo.py:601</code> (_create_container_template method) - Modify: <code>extensions/pipeline_executor/argo.py:676</code> (_create_nested_template method)</p> <p>Step 1: Update _create_container_template to include cache</p> <pre><code># extensions/pipeline_executor/argo.py (in _create_container_template method around line 601)\n\n# Find the existing memoize line:\n# memoize=Memoize(key=\"{{workflow.parameters.run_id}}\"),\n\n# Replace with:\nmemoize=Memoize(\n    key=\"{{workflow.parameters.run_id}}\",\n    cache=Cache(config_map=ConfigMapCache(name=self.cache_name))\n),\n</code></pre> <p>Step 2: Update _create_nested_template to include cache</p> <pre><code># extensions/pipeline_executor/argo.py (in _create_nested_template method around line 676)\n\n# Find the existing memoize line:\n# memoize=Memoize(key=\"{{workflow.parameters.run_id}}\"),\n\n# Replace with:\nmemoize=Memoize(\n    key=\"{{workflow.parameters.run_id}}\",\n    cache=Cache(config_map=ConfigMapCache(name=self.cache_name))\n),\n</code></pre> <p>Step 3: Commit</p> <pre><code>git add extensions/pipeline_executor/argo.py\ngit commit -m \"feat: add cache configuration to template memoization\"\n</code></pre>"},{"location":"plans/2025-12-19-configmap-memoization-implementation/#task_4_test_with_example_pipeline_generation","title":"Task 4: Test with Example Pipeline Generation","text":"<p>Files: - Test: Generate and validate YAML from example pipelines</p> <p>Step 1: Generate YAML from python_tasks example</p> <p>Run: <code>RUNNABLE_CONFIGURATION_FILE=examples/configs/argo-config.yaml uv run python examples/01-tasks/python_tasks.py</code> Expected: Should generate argo-pipeline.yaml without errors</p> <p>Step 2: Check cache configuration in generated YAML</p> <p>Run: <code>grep -A 5 -B 1 \"memoize:\" argo-pipeline.yaml</code> Expected: Should show memoize blocks with cache.configMap.name fields</p> <p>Step 3: Validate with Argo linting</p> <p>Run: <code>argo lint argo-pipeline.yaml</code> Expected: PASS - workflow validates successfully with Argo</p> <p>Step 4: Test multiple examples</p> <p>Run: <code>RUNNABLE_CONFIGURATION_FILE=examples/configs/argo-config.yaml uv run python examples/02-sequential/traversal.py &amp;&amp; argo lint argo-pipeline.yaml</code> Expected: PASS - sequential example generates valid YAML</p> <p>Run: <code>RUNNABLE_CONFIGURATION_FILE=examples/configs/argo-config.yaml uv run python examples/06-parallel/parallel.py &amp;&amp; argo lint argo-pipeline.yaml</code> Expected: PASS - parallel example generates valid YAML</p> <p>Step 5: Commit</p> <pre><code>git add argo-pipeline.yaml\ngit commit -m \"test: verify example pipelines generate valid YAML with cache\"\n</code></pre>"},{"location":"plans/2025-12-19-configmap-memoization-implementation/#task_5_verify_yaml_structure_matches_argo_spec","title":"Task 5: Verify YAML Structure Matches Argo Spec","text":"<p>Files: - Test: Detailed verification of generated YAML structure</p> <p>Step 1: Check exact YAML structure</p> <p>Run: `uv run python -c \" import yaml with open('argo-pipeline.yaml', 'r') as f:     workflow = yaml.safe_load(f)</p>"},{"location":"plans/2025-12-19-configmap-memoization-implementation/#check_that_all_templates_have_proper_memoize_structure","title":"Check that all templates have proper memoize structure","text":"<p>templates = workflow['spec']['templates'] for template in templates:     if 'memoize' in template:         memoize = template['memoize']         print(f\\\"Template '{template['name']}' memoize:\\\")         print(f\\\"  key: {memoize.get('key')}\\\")         if 'cache' in memoize:             cache = memoize['cache']             if 'configMap' in cache:                 print(f\\\"  configMap name: {cache['configMap']['name']}\\\")             else:                 print('  ERROR: No configMap in cache')         else:             print('  ERROR: No cache in memoize')         print() \"` Expected: All templates should show cache.configMap.name structure</p> <p>Step 2: Verify ConfigMap naming pattern</p> <p>Run: `uv run python -c \" import yaml, re with open('argo-pipeline.yaml', 'r') as f:     workflow = yaml.safe_load(f)</p> <p>cache_names = set() for template in workflow['spec']['templates']:     if 'memoize' in template and 'cache' in template['memoize']:         cache_name = template['memoize']['cache']['configMap']['name']         cache_names.add(cache_name)</p> <p>print(f'Found {len(cache_names)} unique cache names:') for name in cache_names:     print(f'  {name}')     assert re.match(r'^runnable-[a-z0-9]{6}$', name), f'Invalid format: {name}'</p> <p>print('All cache names follow runnable-xxxxxx pattern') assert len(cache_names) == 1, 'All templates should use same cache name' print('All templates share same cache name \u2713') \"` Expected: Single valid cache name used across all templates</p> <p>Step 3: Commit verification</p> <pre><code>git add -A\ngit commit -m \"test: verify YAML structure matches Argo memoization spec\"\n</code></pre>"},{"location":"plans/2025-12-19-configmap-memoization-implementation/#task_6_update_documentation","title":"Task 6: Update Documentation","text":"<p>Files: - Modify: <code>docs/plans/2025-01-17-argo-retry-design.md:23-24</code></p> <p>Step 1: Update existing retry design documentation</p> <pre><code># docs/plans/2025-01-17-argo-retry-design.md (lines 23-24)\n\n# Change from:\nmemoize:\n  key: \"{{workflow.parameters.run_id}}\"\n\n# To:\nmemoize:\n  key: \"{{workflow.parameters.run_id}}\"\n  cache:\n    configMap:\n      name: \"runnable-abc123\"  # Generated per workflow\n</code></pre> <p>Step 2: Commit documentation updates</p> <pre><code>git add docs/plans/2025-01-17-argo-retry-design.md\ngit commit -m \"docs: update retry design with ConfigMap memoization\"\n</code></pre>"},{"location":"plans/2025-12-19-configmap-memoization-implementation/#task_7_final_integration_testing","title":"Task 7: Final Integration Testing","text":"<p>Files: - Test: Complete validation across multiple pipeline types</p> <p>Step 1: Test all example pipeline types</p> <p>Run: <code>for example in examples/01-tasks/python_tasks.py examples/03-parameters/passing_parameters_python.py examples/04-catalog/catalog_python.py; do   echo \"Testing $example\"   RUNNABLE_CONFIGURATION_FILE=examples/configs/argo-config.yaml uv run python $example   argo lint argo-pipeline.yaml   echo \"\u2713 $example passed\" done</code> Expected: All example pipelines generate valid Argo YAML</p> <p>Step 2: Verify backward compatibility</p> <p>Run: `uv run python -c \"</p>"},{"location":"plans/2025-12-19-configmap-memoization-implementation/#test_that_old-style_memoize_still_works","title":"Test that old-style memoize still works","text":"<p>from extensions.pipeline_executor.argo import Memoize old_memoize = Memoize(key='test-key') print('Old style still works:', old_memoize.model_dump())</p>"},{"location":"plans/2025-12-19-configmap-memoization-implementation/#test_that_new_style_works","title":"Test that new style works","text":"<p>from extensions.pipeline_executor.argo import Cache, ConfigMapCache new_memoize = Memoize(     key='test-key',     cache=Cache(config_map=ConfigMapCache(name='runnable-abc123')) ) print('New style works:', new_memoize.model_dump()) \"` Expected: Both old and new memoization styles work</p> <p>Step 3: Final commit</p> <pre><code>git add -A\ngit commit -m \"feat: complete ConfigMap memoization implementation\n\n- Add ConfigMapCache, Cache schema classes\n- Generate unique cache names per workflow\n- Update template creation with cache config\n- Validate against Argo specification\n- Maintain backward compatibility\n\nEnables persistent memoization across workflow resubmissions.\"\n</code></pre>"},{"location":"plans/2025-12-19-configmap-memoization-implementation/#summary","title":"Summary","text":"<p>This streamlined plan implements ConfigMap memoization by:</p> <ol> <li>Schema Extension: Add ConfigMapCache and Cache models to support Argo's native format</li> <li>Cache Generation: Generate unique <code>runnable-xxxxxx</code> names per workflow</li> <li>Template Updates: Update both container and nested template creation methods</li> <li>Real-world Testing: Generate and lint YAML from actual example pipelines using proper runnable commands</li> <li>Validation: Verify structure matches Argo specification exactly</li> </ol> <p>Testing focuses on practical pipeline generation and Argo linting using the correct runnable execution pattern with environment variables.</p>"},{"location":"plans/2026-01-01-async-execution-design/","title":"Async Execution Design","text":""},{"location":"plans/2026-01-01-async-execution-design/#overview","title":"Overview","text":"<p>Add native async support to runnable for agentic workflows that need to execute async Python functions and stream events in real-time.</p>"},{"location":"plans/2026-01-01-async-execution-design/#problem","title":"Problem","text":"<p>The current runnable execution is entirely synchronous. For agentic workflows: - User functions may need to perform async IO (HTTP calls, database queries) - Real-time streaming requires async execution to emit events as tasks complete - FastAPI integration currently uses thread pool executor as a workaround</p>"},{"location":"plans/2026-01-01-async-execution-design/#solution","title":"Solution","text":"<p>Hybrid sync/async architecture with explicit method separation and shared helpers to minimize code duplication.</p>"},{"location":"plans/2026-01-01-async-execution-design/#design_principles","title":"Design Principles","text":"<ol> <li>Explicit sync/async methods - Clear naming convention (<code>execute_graph</code> vs <code>execute_graph_async</code>)</li> <li>Shared helpers - Extract common logic into sync helper methods called by both paths</li> <li>No breaking changes - Existing sync code continues to work unchanged</li> <li>Async boundary at execution - Run log store, catalog, and parameter handling remain sync</li> <li>Base class defaults - Async methods in base classes raise <code>NotImplementedError</code> for non-local executors</li> </ol>"},{"location":"plans/2026-01-01-async-execution-design/#architecture_pattern","title":"Architecture Pattern","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Shared Helpers (sync)                        \u2502\n\u2502  _prepare_node_for_execution(), _finalize_node_execution()      \u2502\n\u2502  _get_next_node(), fan_out(), fan_in(), _sync_catalog()         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u25b2                              \u25b2\n                \u2502                              \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502     Sync Path                 \u2502  \u2502     Async Path            \u2502\n\u2502  execute_graph()              \u2502  \u2502  execute_graph_async()    \u2502\n\u2502  execute_from_graph()         \u2502  \u2502  execute_from_graph_async()\u2502\n\u2502  execute_as_graph()           \u2502  \u2502  execute_as_graph_async() \u2502\n\u2502  _execute_node()              \u2502  \u2502  _execute_node_async()    \u2502\n\u2502  execute_command()            \u2502  \u2502  execute_command_async()  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"plans/2026-01-01-async-execution-design/#components","title":"Components","text":""},{"location":"plans/2026-01-01-async-execution-design/#1_basepipelineexecutor_updated","title":"1. BasePipelineExecutor (Updated)","text":"<p>Location: <code>runnable/executor.py</code></p> <p>Add async method stubs that raise <code>NotImplementedError</code> by default. Only interactive executors (local, async-local) implement these.</p> <pre><code>class BasePipelineExecutor(BaseExecutor):\n    \"\"\"\n    Base executor with both sync and async method signatures.\n\n    Async methods raise NotImplementedError by default - only implemented\n    by interactive executors that support async execution.\n    \"\"\"\n\n    service_type: str = \"pipeline_executor\"\n    overrides: dict[str, Any] = {}\n\n    _context_node: Optional[BaseNode] = PrivateAttr(default=None)\n\n    # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n    # Sync Path - Abstract methods (existing)\n    # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n    @abstractmethod\n    def execute_graph(self, dag: Graph, map_variable: MapVariableType = None):\n        \"\"\"Sync graph traversal - implemented by all executors.\"\"\"\n        ...\n\n    @abstractmethod\n    def execute_from_graph(self, node: BaseNode, map_variable: MapVariableType = None):\n        \"\"\"Sync node execution entry point.\"\"\"\n        ...\n\n    @abstractmethod\n    def trigger_node_execution(self, node: BaseNode, map_variable: MapVariableType = None):\n        \"\"\"Sync trigger for node execution.\"\"\"\n        ...\n\n    @abstractmethod\n    def _execute_node(\n        self, node: BaseNode, map_variable: MapVariableType = None, mock: bool = False\n    ):\n        \"\"\"Sync node execution wrapper.\"\"\"\n        ...\n\n    # ... other existing abstract methods ...\n\n    # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n    # Async Path - Default implementations that raise NotImplementedError\n    # Only interactive executors (local, async-local) override these\n    # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n    async def execute_graph_async(self, dag: Graph, map_variable: MapVariableType = None):\n        \"\"\"\n        Async graph traversal.\n\n        Only implemented by interactive executors that support async execution.\n        Transpilers (Argo, etc.) do not implement this.\n\n        Raises:\n            NotImplementedError: If executor does not support async execution.\n        \"\"\"\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} does not support async execution. \"\n            f\"Use a local executor or async-local executor for async pipelines.\"\n        )\n\n    async def execute_from_graph_async(\n        self, node: BaseNode, map_variable: MapVariableType = None\n    ):\n        \"\"\"\n        Async node execution entry point.\n\n        Raises:\n            NotImplementedError: If executor does not support async execution.\n        \"\"\"\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} does not support async execution.\"\n        )\n\n    async def trigger_node_execution_async(\n        self, node: BaseNode, map_variable: MapVariableType = None\n    ):\n        \"\"\"\n        Async trigger for node execution.\n\n        Raises:\n            NotImplementedError: If executor does not support async execution.\n        \"\"\"\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} does not support async execution.\"\n        )\n\n    async def _execute_node_async(\n        self, node: BaseNode, map_variable: MapVariableType = None, mock: bool = False\n    ):\n        \"\"\"\n        Async node execution wrapper.\n\n        Raises:\n            NotImplementedError: If executor does not support async execution.\n        \"\"\"\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} does not support async execution.\"\n        )\n</code></pre>"},{"location":"plans/2026-01-01-async-execution-design/#2_basetasktype_updated","title":"2. BaseTaskType (Updated)","text":"<p>Location: <code>runnable/tasks.py</code></p> <p>Add async method stub that raises <code>NotImplementedError</code> by default. Only async task types implement this.</p> <pre><code>class BaseTaskType(BaseModel):\n    \"\"\"\n    Base task type with both sync and async execution methods.\n\n    Async method raises NotImplementedError by default - only implemented\n    by task types that support async execution (AsyncPythonTaskType).\n    \"\"\"\n\n    task_type: str = Field(serialization_alias=\"command_type\")\n    secrets: List[str] = Field(default_factory=list)\n    returns: List[TaskReturns] = Field(default_factory=list, alias=\"returns\")\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n    # Shared Helpers (sync) - used by both sync and async paths\n    # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n    @property\n    def _context(self):\n        current_context = context.get_run_context()\n        if current_context is None:\n            raise RuntimeError(\"No run context available\")\n        return current_context\n\n    def _safe_serialize_params(self, params: Dict[str, Parameter]) -&gt; Dict[str, Any]:\n        \"\"\"Safely serialize parameters for logging/events.\"\"\"\n        result = {}\n        for k, v in params.items():\n            try:\n                result[k] = truncate_value(v.get_value())\n            except Exception:\n                result[k] = \"&lt;unserializable&gt;\"\n        return result\n\n    @contextlib.contextmanager\n    def expose_secrets(self):\n        \"\"\"Context manager to expose secrets - shared by sync/async.\"\"\"\n        self.set_secrets_as_env_variables()\n        try:\n            yield\n        finally:\n            self.delete_secrets_from_env_variables()\n\n    @contextlib.contextmanager\n    def execution_context(self, map_variable: MapVariableType = None):\n        \"\"\"\n        Context manager for task execution setup/teardown.\n\n        Handles parameter loading and storing - shared by sync/async.\n        \"\"\"\n        params = self._context.run_log_store.get_parameters(self._context.run_id)\n        yield params\n        # Store updated parameters after execution\n        diff_parameters = self._diff_parameters(\n            self._context.run_log_store.get_parameters(self._context.run_id),\n            params\n        )\n        if diff_parameters:\n            self._context.run_log_store.set_parameters(\n                self._context.run_id, diff_parameters\n            )\n\n    def _process_returns(\n        self,\n        user_set_parameters: Any,\n        attempt_log: StepAttempt,\n        params: Dict[str, Parameter],\n        map_variable: MapVariableType = None,\n    ):\n        \"\"\"\n        Process task returns and update attempt log - shared by sync/async.\n        \"\"\"\n        if not self.returns:\n            return\n\n        if not isinstance(user_set_parameters, tuple):\n            user_set_parameters = (user_set_parameters,)\n\n        if len(user_set_parameters) != len(self.returns):\n            raise ValueError(\n                \"Returns task signature does not match the function returns\"\n            )\n\n        output_parameters: Dict[str, Parameter] = {}\n        metrics: Dict[str, Parameter] = {}\n\n        for i, task_return in enumerate(self.returns):\n            output_parameter = task_return_to_parameter(\n                task_return=task_return,\n                value=user_set_parameters[i],\n            )\n\n            if task_return.kind == \"metric\":\n                metrics[task_return.name] = output_parameter\n\n            param_name = task_return.name\n            if map_variable:\n                for _, v in map_variable.items():\n                    param_name = f\"{v}_{param_name}\"\n\n            output_parameters[param_name] = output_parameter\n\n        attempt_log.output_parameters = output_parameters\n        attempt_log.user_defined_metrics = metrics\n        params.update(output_parameters)\n\n    # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n    # Sync Path\n    # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n    def execute_command(\n        self,\n        map_variable: MapVariableType = None,\n    ) -&gt; StepAttempt:\n        \"\"\"\n        Sync command execution - implemented by sync task types.\n\n        Raises:\n            NotImplementedError: Base class, must be overridden.\n        \"\"\"\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} must implement execute_command()\"\n        )\n\n    # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n    # Async Path\n    # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n    async def execute_command_async(\n        self,\n        map_variable: MapVariableType = None,\n        event_callback: Optional[Callable[[dict], None]] = None,\n    ) -&gt; StepAttempt:\n        \"\"\"\n        Async command execution.\n\n        Only implemented by task types that support async execution\n        (AsyncPythonTaskType). Sync task types (PythonTaskType,\n        NotebookTaskType, ShellTaskType) raise NotImplementedError.\n\n        Args:\n            map_variable: If the command is part of map node.\n            event_callback: Optional callback for streaming events.\n\n        Raises:\n            NotImplementedError: If task type does not support async execution.\n        \"\"\"\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} does not support async execution. \"\n            f\"Use AsyncPythonTask for async functions.\"\n        )\n</code></pre>"},{"location":"plans/2026-01-01-async-execution-design/#3_genericpipelineexecutor_updated","title":"3. GenericPipelineExecutor (Updated)","text":"<p>Location: <code>extensions/pipeline_executor/__init__.py</code></p> <p>Implement shared helpers and both sync/async paths:</p> <pre><code>class GenericPipelineExecutor(BasePipelineExecutor):\n    \"\"\"\n    Base executor implementation with both sync and async execution paths.\n\n    Shared helpers contain the actual logic, called by both sync and async methods.\n    \"\"\"\n\n    service_name: str = \"\"\n    service_type: str = \"pipeline_executor\"\n\n    # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n    # Shared Helpers (sync) - contain the actual logic\n    # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n    def _prepare_node_for_execution(\n        self, node: BaseNode, map_variable: MapVariableType = None\n    ) -&gt; Optional[StepLog]:\n        \"\"\"\n        Setup before node execution - shared by sync/async paths.\n\n        Returns None if node should be skipped (retry logic).\n        \"\"\"\n        if self._should_skip_step_in_retry(node, map_variable):\n            logger.info(f\"Skipping execution of '{node.internal_name}' due to retry logic\")\n            console.print(\n                f\":fast_forward: Skipping node {node.internal_name} - already successful\",\n                style=\"bold yellow\",\n            )\n            return None\n\n        # Handle step log creation for retry vs normal runs\n        if self._context.is_retry:\n            try:\n                step_log = self._context.run_log_store.get_step_log(\n                    node._get_step_log_name(map_variable), self._context.run_id\n                )\n            except exceptions.StepLogNotFoundError:\n                step_log = self._context.run_log_store.create_step_log(\n                    node.name, node._get_step_log_name(map_variable)\n                )\n        else:\n            step_log = self._context.run_log_store.create_step_log(\n                node.name, node._get_step_log_name(map_variable)\n            )\n\n        step_log.step_type = node.node_type\n        step_log.status = defaults.PROCESSING\n        self._context.run_log_store.add_step_log(step_log, self._context.run_id)\n\n        return step_log\n\n    def _finalize_graph_execution(\n        self, node: BaseNode, dag: Graph, map_variable: MapVariableType = None\n    ):\n        \"\"\"Finalize after graph traversal - shared by sync/async paths.\"\"\"\n        run_log = self._context.run_log_store.get_branch_log(\n            node._get_branch_log_name(map_variable), self._context.run_id\n        )\n\n        branch = \"graph\"\n        if node.internal_branch_name:\n            branch = node.internal_branch_name\n\n        logger.info(f\"Finished execution of {branch} with status {run_log.status}\")\n\n        if dag == self._context.dag:\n            run_log = cast(RunLog, run_log)\n            console.print(\"Completed Execution, Summary:\", style=\"bold color(208)\")\n            console.print(run_log.get_summary(), style=defaults.info_style)\n\n    # Existing shared helpers: _get_status_and_next_node_name, _sync_catalog,\n    # _calculate_attempt_number, _should_skip_step_in_retry, etc.\n\n    # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n    # Sync Path - existing methods (LocalExecutor, Argo transpiler)\n    # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n    def execute_graph(self, dag: Graph, map_variable: MapVariableType = None):\n        \"\"\"Sync graph traversal - existing implementation.\"\"\"\n        current_node = dag.start_at\n        previous_node = None\n\n        while True:\n            working_on = dag.get_node_by_name(current_node)\n\n            if previous_node == current_node:\n                raise Exception(\"Potentially running in an infinite loop\")\n            previous_node = current_node\n\n            self.execute_from_graph(working_on, map_variable=map_variable)\n            status, next_node_name = self._get_status_and_next_node_name(\n                current_node=working_on, dag=dag, map_variable=map_variable\n            )\n\n            if working_on.node_type in [\"success\", \"fail\"]:\n                break\n            current_node = next_node_name\n\n        self._finalize_graph_execution(working_on, dag, map_variable)\n\n    def execute_from_graph(self, node: BaseNode, map_variable: MapVariableType = None):\n        \"\"\"Sync node execution entry point - existing implementation.\"\"\"\n        step_log = self._prepare_node_for_execution(node, map_variable)\n        if step_log is None:\n            return  # Skipped\n\n        if node.node_type in [\"success\", \"fail\"]:\n            self._execute_node(node, map_variable=map_variable)\n            return\n\n        if node.is_composite:\n            node.execute_as_graph(map_variable=map_variable)\n            return\n\n        task_name = node._resolve_map_placeholders(node.internal_name, map_variable)\n        console.print(\n            f\":runner: Executing the node {task_name} ... \", style=\"bold color(208)\"\n        )\n        self.trigger_node_execution(node=node, map_variable=map_variable)\n\n    # _execute_node - existing implementation\n    # trigger_node_execution - existing implementation\n\n    # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n    # Async Path - new methods, override base class NotImplementedError\n    # Only LocalExecutor and AsyncLocalExecutor implement these\n    # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n    async def execute_graph_async(self, dag: Graph, map_variable: MapVariableType = None):\n        \"\"\"\n        Async graph traversal.\n\n        Default implementation raises NotImplementedError.\n        Override in LocalExecutor/AsyncLocalExecutor.\n        \"\"\"\n        # Call parent which raises NotImplementedError\n        await super().execute_graph_async(dag, map_variable)\n\n    async def execute_from_graph_async(\n        self, node: BaseNode, map_variable: MapVariableType = None\n    ):\n        \"\"\"\n        Async node execution entry point.\n\n        Default implementation raises NotImplementedError.\n        Override in LocalExecutor/AsyncLocalExecutor.\n        \"\"\"\n        await super().execute_from_graph_async(node, map_variable)\n\n    # ... other async methods delegate to parent NotImplementedError ...\n</code></pre>"},{"location":"plans/2026-01-01-async-execution-design/#4_localexecutor_updated","title":"4. LocalExecutor (Updated)","text":"<p>Location: <code>extensions/pipeline_executor/local.py</code></p> <p>Implement async methods for the local executor:</p> <pre><code>class LocalExecutor(GenericPipelineExecutor):\n    \"\"\"\n    Local executor with both sync and async execution support.\n    \"\"\"\n\n    service_name: str = \"local\"\n\n    # Sync methods - existing implementation (unchanged)\n\n    # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n    # Async Path - implement async methods\n    # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n    async def execute_graph_async(self, dag: Graph, map_variable: MapVariableType = None):\n        \"\"\"Async graph traversal.\"\"\"\n        current_node = dag.start_at\n        previous_node = None\n\n        while True:\n            working_on = dag.get_node_by_name(current_node)\n\n            if previous_node == current_node:\n                raise Exception(\"Potentially running in an infinite loop\")\n            previous_node = current_node\n\n            await self.execute_from_graph_async(working_on, map_variable=map_variable)\n            # Sync helper - no await needed\n            status, next_node_name = self._get_status_and_next_node_name(\n                current_node=working_on, dag=dag, map_variable=map_variable\n            )\n\n            if working_on.node_type in [\"success\", \"fail\"]:\n                break\n            current_node = next_node_name\n\n        # Sync helper - no await needed\n        self._finalize_graph_execution(working_on, dag, map_variable)\n\n    async def execute_from_graph_async(\n        self, node: BaseNode, map_variable: MapVariableType = None\n    ):\n        \"\"\"Async node execution entry point.\"\"\"\n        # Sync helper - no await needed\n        step_log = self._prepare_node_for_execution(node, map_variable)\n        if step_log is None:\n            return  # Skipped\n\n        if node.node_type in [\"success\", \"fail\"]:\n            await self._execute_node_async(node, map_variable=map_variable)\n            return\n\n        if node.is_composite:\n            await node.execute_as_graph_async(map_variable=map_variable)\n            return\n\n        task_name = node._resolve_map_placeholders(node.internal_name, map_variable)\n        console.print(\n            f\":runner: Executing the node {task_name} ... \", style=\"bold color(208)\"\n        )\n        await self.trigger_node_execution_async(node=node, map_variable=map_variable)\n\n    async def trigger_node_execution_async(\n        self, node: BaseNode, map_variable: MapVariableType = None\n    ):\n        \"\"\"Async trigger for node execution.\"\"\"\n        await self._execute_node_async(node=node, map_variable=map_variable)\n\n    async def _execute_node_async(\n        self, node: BaseNode, map_variable: MapVariableType = None, mock: bool = False\n    ):\n        \"\"\"Async node execution wrapper.\"\"\"\n        current_attempt_number = self._calculate_attempt_number(node, map_variable)\n        os.environ[defaults.ATTEMPT_NUMBER] = str(current_attempt_number)\n\n        self._context_node = node\n\n        # Sync - catalog get\n        data_catalogs_get = self._sync_catalog(stage=\"get\")\n\n        # ASYNC - execute the node\n        step_log = await node.execute_async(\n            map_variable=map_variable,\n            attempt_number=current_attempt_number,\n            mock=mock,\n        )\n\n        # Sync - catalog put and finalization\n        allow_file_not_found_exc = step_log.status != defaults.SUCCESS\n        data_catalogs_put = self._sync_catalog(\n            stage=\"put\", allow_file_no_found_exc=allow_file_not_found_exc\n        )\n        step_log.add_data_catalogs(data_catalogs_put or [])\n        step_log.add_data_catalogs(data_catalogs_get or [])\n\n        console.print(f\"Summary of the step: {step_log.internal_name}\")\n        console.print(step_log.get_summary(), style=defaults.info_style)\n\n        self.add_task_log_to_catalog(\n            name=self._context_node.internal_name, map_variable=map_variable\n        )\n        self._context_node = None\n\n        self._context.run_log_store.add_step_log(step_log, self._context.run_id)\n</code></pre>"},{"location":"plans/2026-01-01-async-execution-design/#5_argoexecutor_no_async_support","title":"5. ArgoExecutor (No async support)","text":"<p>Location: <code>extensions/pipeline_executor/argo.py</code></p> <p>Transpilers do NOT implement async methods - they inherit the <code>NotImplementedError</code> from base class:</p> <pre><code>class ArgoExecutor(GenericPipelineExecutor):\n    \"\"\"\n    Argo executor - transpiles DAG to Argo workflow spec.\n\n    Does NOT support async execution - async methods raise NotImplementedError.\n    This is intentional: transpilers don't execute, they generate specs.\n    \"\"\"\n\n    service_name: str = \"argo\"\n\n    # Only sync methods implemented\n    # execute_graph - transpiles to Argo spec\n    # execute_from_graph - not used by transpilers\n\n    # Async methods: inherited NotImplementedError from BasePipelineExecutor\n    # execute_graph_async - raises NotImplementedError\n    # execute_from_graph_async - raises NotImplementedError\n    # etc.\n</code></pre>"},{"location":"plans/2026-01-01-async-execution-design/#6_basenode_updated","title":"6. BaseNode (Updated)","text":"<p>Location: <code>runnable/nodes.py</code></p> <p>Add async execute method with default fallback to sync:</p> <pre><code>class BaseNode(ABC, BaseModel):\n    \"\"\"Base node with sync and async execution methods.\"\"\"\n\n    # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n    # Sync Path\n    # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n    @abstractmethod\n    def execute(\n        self,\n        map_variable: MapVariableType = None,\n        attempt_number: int = 1,\n        mock: bool = False,\n    ) -&gt; StepLog:\n        \"\"\"Sync execution - implemented by subclasses.\"\"\"\n        ...\n\n    # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n    # Async Path\n    # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n    async def execute_async(\n        self,\n        map_variable: MapVariableType = None,\n        attempt_number: int = 1,\n        mock: bool = False,\n    ) -&gt; StepLog:\n        \"\"\"\n        Async execution - default delegates to sync execute().\n\n        Override in subclasses that support true async execution (TaskNode).\n        Terminal nodes (SuccessNode, FailNode) use this default.\n        \"\"\"\n        return self.execute(\n            map_variable=map_variable,\n            attempt_number=attempt_number,\n            mock=mock,\n        )\n</code></pre>"},{"location":"plans/2026-01-01-async-execution-design/#7_compositenode_updated","title":"7. CompositeNode (Updated)","text":"<p>Location: <code>runnable/nodes.py</code></p> <p>Add async variant for composite node execution:</p> <pre><code>class CompositeNode(TraversalNode):\n    \"\"\"Base class for nodes that contain sub-graphs.\"\"\"\n\n    is_composite: bool = Field(default=True, exclude=True)\n\n    # fan_out() and fan_in() stay sync - they only do run log operations\n\n    @abstractmethod\n    def execute_as_graph(self, map_variable: MapVariableType = None):\n        \"\"\"Sync execution of sub-graph - override in subclasses.\"\"\"\n        ...\n\n    async def execute_as_graph_async(self, map_variable: MapVariableType = None):\n        \"\"\"\n        Async execution of sub-graph.\n\n        Default raises NotImplementedError - override in subclasses\n        that support async execution.\n        \"\"\"\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} must implement execute_as_graph_async() \"\n            f\"for async execution support.\"\n        )\n</code></pre>"},{"location":"plans/2026-01-01-async-execution-design/#8_parallelnode_updated","title":"8. ParallelNode (Updated)","text":"<p>Location: <code>extensions/nodes/parallel.py</code></p> <p>Add async variant:</p> <pre><code>class ParallelNode(CompositeNode):\n    \"\"\"Parallel execution node with sync and async paths.\"\"\"\n\n    # fan_out() stays sync - just creates branch logs\n    # fan_in() stays sync - just collates branch status\n\n    def execute_as_graph(self, map_variable: MapVariableType = None):\n        \"\"\"Sync parallel execution - existing implementation.\"\"\"\n        self.fan_out(map_variable=map_variable)\n\n        for _, branch in self.branches.items():\n            self._context.pipeline_executor.execute_graph(\n                branch, map_variable=map_variable\n            )\n\n        self.fan_in(map_variable=map_variable)\n\n    async def execute_as_graph_async(self, map_variable: MapVariableType = None):\n        \"\"\"Async parallel execution.\"\"\"\n        self.fan_out(map_variable=map_variable)  # sync helper\n\n        for _, branch in self.branches.items():\n            await self._context.pipeline_executor.execute_graph_async(\n                branch, map_variable=map_variable\n            )\n\n        self.fan_in(map_variable=map_variable)  # sync helper\n</code></pre>"},{"location":"plans/2026-01-01-async-execution-design/#9_tasknode_updated","title":"9. TaskNode (Updated)","text":"<p>Location: <code>runnable/nodes.py</code></p> <p>Add async execution that checks task type support:</p> <pre><code>class TaskNode(TraversalNode):\n    \"\"\"Task node with sync and async execution paths.\"\"\"\n\n    def execute(\n        self,\n        map_variable: MapVariableType = None,\n        attempt_number: int = 1,\n        mock: bool = False,\n    ) -&gt; StepLog:\n        \"\"\"Sync task execution - existing implementation.\"\"\"\n        step_log = self._create_step_log(map_variable)\n\n        task = self._get_task()\n        attempt_log = task.execute_command(map_variable=map_variable)\n\n        step_log.attempts.append(attempt_log)\n        step_log.status = attempt_log.status\n        return step_log\n\n    async def execute_async(\n        self,\n        map_variable: MapVariableType = None,\n        attempt_number: int = 1,\n        mock: bool = False,\n    ) -&gt; StepLog:\n        \"\"\"Async task execution.\"\"\"\n        step_log = self._create_step_log(map_variable)\n\n        task = self._get_task()\n\n        # Try async first, fall back to sync\n        try:\n            attempt_log = await task.execute_command_async(map_variable=map_variable)\n        except NotImplementedError:\n            # Task doesn't support async, fall back to sync\n            attempt_log = task.execute_command(map_variable=map_variable)\n\n        step_log.attempts.append(attempt_log)\n        step_log.status = attempt_log.status\n        return step_log\n</code></pre>"},{"location":"plans/2026-01-01-async-execution-design/#10_asyncpythontasktype","title":"10. AsyncPythonTaskType","text":"<p>Location: <code>runnable/tasks.py</code></p> <p>New task type that implements async execution:</p> <pre><code>class AsyncPythonTaskType(BaseTaskType):\n    \"\"\"\n    Task type for executing async Python functions.\n\n    Supports both regular async functions and AsyncGenerator functions\n    for streaming use cases.\n    \"\"\"\n\n    task_type: str = Field(default=\"async-python\", serialization_alias=\"command_type\")\n    command: str\n\n    # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n    # Sync Path - raises error (async functions can't run sync)\n    # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n    def execute_command(self, map_variable: MapVariableType = None) -&gt; StepAttempt:\n        \"\"\"Sync execution - not supported for async tasks.\"\"\"\n        raise RuntimeError(\n            \"AsyncPythonTaskType requires async execution. \"\n            \"Use AsyncPipeline or ensure executor calls execute_command_async().\"\n        )\n\n    # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n    # Async Path - the real implementation\n    # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n    async def execute_command_async(\n        self,\n        map_variable: MapVariableType = None,\n        event_callback: Optional[Callable[[dict], None]] = None,\n    ) -&gt; StepAttempt:\n        \"\"\"Execute an async Python function.\"\"\"\n        attempt_log = StepAttempt(\n            status=defaults.FAIL,\n            start_time=str(datetime.now()),\n        )\n\n        with self.execution_context(map_variable=map_variable) as params:\n            with self.expose_secrets():\n                module, func = utils.get_module_and_attr_names(self.command)\n                sys.path.insert(0, os.getcwd())\n                imported_module = importlib.import_module(module)\n                f = getattr(imported_module, func)\n\n                filtered_parameters = parameters.filter_arguments_for_func(\n                    f, params.copy(), map_variable\n                )\n\n                # Call the async function\n                result = await f(**filtered_parameters)\n\n                # Handle AsyncGenerator (streaming)\n                if inspect.isasyncgen(result):\n                    accumulated_chunks: List[str] = []\n                    async for chunk in result:\n                        accumulated_chunks.append(str(chunk))\n                        if event_callback:\n                            event_callback({\n                                \"type\": \"task_chunk\",\n                                \"name\": self.command,\n                                \"chunk\": str(chunk),\n                            })\n                    user_set_parameters = \"\".join(accumulated_chunks)\n                else:\n                    user_set_parameters = result\n\n                # Shared helper for return processing\n                self._process_returns(user_set_parameters, attempt_log, params, map_variable)\n                attempt_log.status = defaults.SUCCESS\n\n        attempt_log.end_time = str(datetime.now())\n        return attempt_log\n</code></pre>"},{"location":"plans/2026-01-01-async-execution-design/#11_pipelinecontext_updated","title":"11. PipelineContext (Updated)","text":"<p>Location: <code>runnable/context.py</code></p> <p>Add async execute method:</p> <pre><code>class PipelineContext(BaseModel):\n    \"\"\"Pipeline context with sync and async execution.\"\"\"\n\n    def execute(self):\n        \"\"\"Sync pipeline execution - existing implementation.\"\"\"\n        assert self.dag is not None\n\n        if self.pipeline_executor._should_setup_run_log_at_traversal:\n            self.pipeline_executor._set_up_run_log(exists_ok=False)\n\n        self.pipeline_executor.execute_graph(dag=self.dag)\n        self._handle_completion()\n\n    async def execute_async(self):\n        \"\"\"Async pipeline execution.\"\"\"\n        assert self.dag is not None\n\n        if self.pipeline_executor._should_setup_run_log_at_traversal:\n            self.pipeline_executor._set_up_run_log(exists_ok=False)\n\n        await self.pipeline_executor.execute_graph_async(dag=self.dag)\n        self._handle_completion()  # Sync helper\n\n    def _handle_completion(self):\n        \"\"\"Handle post-execution - shared by sync/async.\"\"\"\n        run_log = self.run_log_store.get_run_log_by_id(\n            run_id=self.run_id, full=False\n        )\n\n        if run_log.status == defaults.SUCCESS:\n            console.print(\"Pipeline executed successfully!\", style=defaults.success_style)\n        else:\n            console.print(\"Pipeline execution failed.\", style=defaults.error_style)\n            raise exceptions.ExecutionFailedError(self.run_id)\n</code></pre>"},{"location":"plans/2026-01-01-async-execution-design/#12_sdk_classes","title":"12. SDK Classes","text":"<p>Location: <code>runnable/sdk.py</code></p> <pre><code>class Pipeline(BaseModel):\n    \"\"\"Pipeline with sync execution (existing behavior unchanged).\"\"\"\n\n    def execute(self, ...):\n        \"\"\"Sync execution - existing implementation.\"\"\"\n        context = PipelineContext(...)\n        context.execute()\n\n\nclass AsyncPipeline(BaseModel):\n    \"\"\"Pipeline with async execution.\"\"\"\n\n    async def execute(self, ...):\n        \"\"\"Async execution.\"\"\"\n        context = PipelineContext(\n            pipeline_executor={\"type\": \"local\"},  # or \"async-local\"\n            ...\n        )\n        await context.execute_async()\n\n    async def execute_stream(self, ...) -&gt; AsyncGenerator[PipelineEvent, None]:\n        \"\"\"Streaming execution with events.\"\"\"\n        # Implementation for SSE streaming\n        ...\n</code></pre>"},{"location":"plans/2026-01-01-async-execution-design/#method_naming_convention","title":"Method Naming Convention","text":"Sync Method Async Method Location <code>execute_graph()</code> <code>execute_graph_async()</code> BasePipelineExecutor <code>execute_from_graph()</code> <code>execute_from_graph_async()</code> BasePipelineExecutor <code>trigger_node_execution()</code> <code>trigger_node_execution_async()</code> BasePipelineExecutor <code>_execute_node()</code> <code>_execute_node_async()</code> BasePipelineExecutor <code>execute_as_graph()</code> <code>execute_as_graph_async()</code> CompositeNode subclasses <code>execute()</code> <code>execute_async()</code> BaseNode, TaskNode <code>execute_command()</code> <code>execute_command_async()</code> BaseTaskType subclasses <code>execute()</code> <code>execute_async()</code> PipelineContext"},{"location":"plans/2026-01-01-async-execution-design/#error_handling_for_unsupported_async","title":"Error Handling for Unsupported Async","text":"Component Async Method Called On Error ArgoExecutor <code>execute_graph_async()</code> <code>NotImplementedError: ArgoExecutor does not support async execution</code> NotebookTaskType <code>execute_command_async()</code> <code>NotImplementedError: NotebookTaskType does not support async execution</code> ShellTaskType <code>execute_command_async()</code> <code>NotImplementedError: ShellTaskType does not support async execution</code>"},{"location":"plans/2026-01-01-async-execution-design/#shared_helpers_sync","title":"Shared Helpers (Sync)","text":"<p>These methods contain the actual logic and are called by both sync and async paths:</p> Helper Method Purpose Location <code>_prepare_node_for_execution()</code> Setup before node execution GenericPipelineExecutor <code>_finalize_graph_execution()</code> Cleanup after graph traversal GenericPipelineExecutor <code>_get_status_and_next_node_name()</code> Determine next node GenericPipelineExecutor <code>_sync_catalog()</code> Catalog get/put operations GenericPipelineExecutor <code>_calculate_attempt_number()</code> Calculate retry attempt GenericPipelineExecutor <code>fan_out()</code> Create branch logs CompositeNode <code>fan_in()</code> Collate branch status CompositeNode <code>_handle_completion()</code> Post-execution handling PipelineContext <code>_process_returns()</code> Handle task returns BaseTaskType <code>execution_context()</code> Parameter load/store BaseTaskType <code>expose_secrets()</code> Secret management BaseTaskType"},{"location":"plans/2026-01-01-async-execution-design/#plugin_registration","title":"Plugin Registration","text":"<p>In <code>pyproject.toml</code>:</p> <pre><code>[project.entry-points.'tasks']\nasync-python = \"runnable.tasks:AsyncPythonTaskType\"\n\n# No new executor needed - LocalExecutor now supports async\n# Optionally register async-local as alias\n[project.entry-points.'pipeline_executor']\nasync-local = \"extensions.pipeline_executor.local:LocalExecutor\"\n</code></pre>"},{"location":"plans/2026-01-01-async-execution-design/#streaming_events","title":"Streaming Events","text":"<p>AsyncGenerator approach - events flow out to client, data flows internally:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                          LocalExecutor                          \u2502\n\u2502                                                                 \u2502\n\u2502  Task 1 (generate_text)          Task 2 (process_result)       \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u2502\n\u2502  \u2502 yield \"Hello\"       \u2502         \u2502                     \u2502       \u2502\n\u2502  \u2502 yield \" world\"      \u2502         \u2502 text = \"Hello world\"\u2502       \u2502\n\u2502  \u2502 yield \"!\"           \u2502         \u2502 return {\"len\": 11}  \u2502       \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502\n\u2502           \u2502                               \u25b2                     \u2502\n\u2502           \u2502 accumulate internally         \u2502                     \u2502\n\u2502           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502                     \u2502\n\u2502                    \"Hello world\"    (via params/run_log_store) \u2502\n\u2502                                                                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502 yield events\n           \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502   FastAPI SSE   \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Event Types:</p> Type Description <code>pipeline_started</code> Pipeline execution began <code>task_started</code> Task began execution <code>task_chunk</code> Chunk yielded from async generator task <code>task_completed</code> Task completed (includes outputs) <code>task_error</code> Task failed <code>pipeline_completed</code> Pipeline finished"},{"location":"plans/2026-01-01-async-execution-design/#usage_examples","title":"Usage Examples","text":""},{"location":"plans/2026-01-01-async-execution-design/#sync_execution_unchanged","title":"Sync execution (unchanged)","text":"<pre><code>from runnable import Pipeline, PythonTask\n\ndef my_func(x: int) -&gt; int:\n    return x * 2\n\npipeline = Pipeline(\n    steps=[PythonTask(function=my_func, name=\"step1\", returns=[\"result\"])]\n)\npipeline.execute()  # Sync - works exactly as before\n</code></pre>"},{"location":"plans/2026-01-01-async-execution-design/#async_execution","title":"Async execution","text":"<pre><code>from runnable import AsyncPipeline, AsyncPythonTask\n\nasync def fetch_data(url: str) -&gt; dict:\n    async with aiohttp.ClientSession() as session:\n        async with session.get(url) as response:\n            return await response.json()\n\npipeline = AsyncPipeline(\n    steps=[AsyncPythonTask(function=fetch_data, name=\"fetch\", returns=[\"data\"])]\n)\nawait pipeline.execute()  # Async\n</code></pre>"},{"location":"plans/2026-01-01-async-execution-design/#streaming_execution","title":"Streaming execution","text":"<pre><code>@app.post(\"/generate\")\nasync def generate(request: Request):\n    pipeline = AsyncPipeline(steps=[...])\n\n    async def event_stream():\n        async for event in pipeline.execute_stream():\n            yield f\"data: {event.model_dump_json()}\\n\\n\"\n\n    return StreamingResponse(event_stream(), media_type=\"text/event-stream\")\n</code></pre>"},{"location":"plans/2026-01-01-async-execution-design/#out_of_scope","title":"Out of Scope","text":"<ul> <li>Async notebook/shell tasks (inherently sync - raise NotImplementedError)</li> <li>Async container/argo executors (transpilers don't execute - raise NotImplementedError)</li> <li>Changes to run log store or catalog (stay sync)</li> <li>Async parameter handling</li> </ul>"},{"location":"plans/2026-01-01-async-execution-design/#testing_strategy","title":"Testing Strategy","text":"<ol> <li>Unit tests for shared helpers (called by both paths)</li> <li>Unit tests for sync path (ensure no regression)</li> <li>Unit tests for async path (new functionality)</li> <li>Unit tests for NotImplementedError on unsupported executors/tasks</li> <li>Integration tests with composite nodes (parallel, map)</li> <li>Integration tests for streaming</li> <li>Test error handling and retry logic in both paths</li> </ol>"},{"location":"plans/2026-01-01-async-execution-plan/","title":"Async Execution Implementation Plan","text":"<p>For Claude: REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.</p> <p>Goal: Add native async execution support to runnable for agentic workflows with async Python functions.</p> <p>Architecture: Hybrid sync/async pattern with explicit method separation (<code>execute_graph</code> vs <code>execute_graph_async</code>) and shared helpers to minimize code duplication. Base classes provide <code>NotImplementedError</code> defaults for async methods - only interactive executors implement them.</p> <p>Key Design Decisions: - Async methods in base classes raise <code>NotImplementedError</code> by default - Only <code>LocalExecutor</code> implements async methods (no separate <code>AsyncLocalExecutor</code>) - Shared helpers extract common logic called by both sync and async paths - Composite nodes (<code>ParallelNode</code>, <code>MapNode</code>) get <code>execute_as_graph_async()</code> methods - <code>PipelineContext</code> gets <code>execute_async()</code> method (no separate <code>AsyncPipelineContext</code>)</p> <p>Tech Stack: Python asyncio, inspect (isasyncgen), pydantic, stevedore plugins, pytest-asyncio</p>"},{"location":"plans/2026-01-01-async-execution-plan/#task_1_add_pytest-asyncio_dependency","title":"Task 1: Add pytest-asyncio dependency","text":"<p>Files: - Modify: <code>pyproject.toml</code></p> <p>Step 1: Add pytest-asyncio to dev dependencies</p> <p>In <code>pyproject.toml</code>, find the <code>[tool.uv]</code> section with dev-dependencies and add pytest-asyncio:</p> <pre><code>[tool.uv]\ndev-dependencies = [\n    # ... existing deps ...\n    \"pytest-asyncio&gt;=0.23.0\",\n]\n</code></pre> <p>Step 2: Sync dependencies</p> <p>Run: <code>uv sync --all-extras --dev</code> Expected: Successfully installed pytest-asyncio</p> <p>Step 3: Commit</p> <pre><code>git add pyproject.toml uv.lock\ngit commit -m \"chore: add pytest-asyncio for async tests\"\n</code></pre>"},{"location":"plans/2026-01-01-async-execution-plan/#task_2_add_async_methods_to_basepipelineexecutor","title":"Task 2: Add async methods to BasePipelineExecutor","text":"<p>Files: - Modify: <code>runnable/executor.py</code></p> <p>Step 1: Add async method stubs that raise NotImplementedError</p> <p>Add after the existing abstract methods in <code>BasePipelineExecutor</code>:</p> <pre><code>    # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n    # Async Path - Default implementations that raise NotImplementedError\n    # Only interactive executors (local) override these\n    # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n    async def execute_graph_async(\n        self, dag: \"Graph\", map_variable: MapVariableType = None\n    ):\n        \"\"\"\n        Async graph traversal.\n\n        Only implemented by interactive executors that support async execution.\n        Transpilers (Argo, etc.) do not implement this.\n\n        Raises:\n            NotImplementedError: If executor does not support async execution.\n        \"\"\"\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} does not support async execution. \"\n            f\"Use a local executor for async pipelines.\"\n        )\n\n    async def execute_from_graph_async(\n        self, node: \"BaseNode\", map_variable: MapVariableType = None\n    ):\n        \"\"\"\n        Async node execution entry point.\n\n        Raises:\n            NotImplementedError: If executor does not support async execution.\n        \"\"\"\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} does not support async execution.\"\n        )\n\n    async def trigger_node_execution_async(\n        self, node: \"BaseNode\", map_variable: MapVariableType = None\n    ):\n        \"\"\"\n        Async trigger for node execution.\n\n        Raises:\n            NotImplementedError: If executor does not support async execution.\n        \"\"\"\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} does not support async execution.\"\n        )\n\n    async def _execute_node_async(\n        self,\n        node: \"BaseNode\",\n        map_variable: MapVariableType = None,\n        mock: bool = False,\n    ):\n        \"\"\"\n        Async node execution wrapper.\n\n        Raises:\n            NotImplementedError: If executor does not support async execution.\n        \"\"\"\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} does not support async execution.\"\n        )\n</code></pre> <p>Step 2: Run tests to verify no regression</p> <p>Run: <code>uv run pytest tests/runnable/test_executor.py -v</code> Expected: All existing tests PASS</p> <p>Step 3: Commit</p> <pre><code>git add runnable/executor.py\ngit commit -m \"feat: add async method stubs to BasePipelineExecutor\"\n</code></pre>"},{"location":"plans/2026-01-01-async-execution-plan/#task_3_add_async_method_stub_to_basetasktype","title":"Task 3: Add async method stub to BaseTaskType","text":"<p>Files: - Modify: <code>runnable/tasks.py</code></p> <p>Step 1: Add imports at top of file</p> <pre><code>import inspect\nfrom typing import Callable, Optional\n</code></pre> <p>Step 2: Add async method stub to BaseTaskType</p> <p>Add after <code>execute_command</code> method:</p> <pre><code>    async def execute_command_async(\n        self,\n        map_variable: MapVariableType = None,\n        event_callback: Optional[Callable[[dict], None]] = None,\n    ) -&gt; StepAttempt:\n        \"\"\"\n        Async command execution.\n\n        Only implemented by task types that support async execution\n        (AsyncPythonTaskType). Sync task types (PythonTaskType,\n        NotebookTaskType, ShellTaskType) raise NotImplementedError.\n\n        Args:\n            map_variable: If the command is part of map node.\n            event_callback: Optional callback for streaming events.\n\n        Raises:\n            NotImplementedError: If task type does not support async execution.\n        \"\"\"\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} does not support async execution. \"\n            f\"Use AsyncPythonTask for async functions.\"\n        )\n</code></pre> <p>Step 3: Run tests to verify no regression</p> <p>Run: <code>uv run pytest tests/runnable/test_tasks.py -v</code> Expected: All existing tests PASS</p> <p>Step 4: Commit</p> <pre><code>git add runnable/tasks.py\ngit commit -m \"feat: add execute_command_async stub to BaseTaskType\"\n</code></pre>"},{"location":"plans/2026-01-01-async-execution-plan/#task_4_add_shared_helpers_to_genericpipelineexecutor","title":"Task 4: Add shared helpers to GenericPipelineExecutor","text":"<p>Files: - Modify: <code>extensions/pipeline_executor/__init__.py</code></p> <p>Step 1: Extract shared helper for node preparation</p> <p>Add new method <code>_prepare_node_for_execution</code> before <code>execute_from_graph</code>:</p> <pre><code>    def _prepare_node_for_execution(\n        self, node: BaseNode, map_variable: MapVariableType = None\n    ) -&gt; Optional[StepLog]:\n        \"\"\"\n        Setup before node execution - shared by sync/async paths.\n\n        Returns None if node should be skipped (retry logic).\n        \"\"\"\n        if self._should_skip_step_in_retry(node, map_variable):\n            logger.info(f\"Skipping execution of '{node.internal_name}' due to retry logic\")\n            console.print(\n                f\":fast_forward: Skipping node {node.internal_name} - already successful\",\n                style=\"bold yellow\",\n            )\n            return None\n\n        # Handle step log creation for retry vs normal runs\n        if self._context.is_retry:\n            try:\n                step_log = self._context.run_log_store.get_step_log(\n                    node._get_step_log_name(map_variable), self._context.run_id\n                )\n                logger.info(f\"Reusing existing step log for retry: {node.internal_name}\")\n            except exceptions.StepLogNotFoundError:\n                step_log = self._context.run_log_store.create_step_log(\n                    node.name, node._get_step_log_name(map_variable)\n                )\n                logger.info(f\"Creating new step log for retry: {node.internal_name}\")\n        else:\n            step_log = self._context.run_log_store.create_step_log(\n                node.name, node._get_step_log_name(map_variable)\n            )\n\n        step_log.step_type = node.node_type\n        step_log.status = defaults.PROCESSING\n        self._context.run_log_store.add_step_log(step_log, self._context.run_id)\n\n        return step_log\n\n    def _finalize_graph_execution(\n        self, node: BaseNode, dag: Graph, map_variable: MapVariableType = None\n    ):\n        \"\"\"Finalize after graph traversal - shared by sync/async paths.\"\"\"\n        run_log = self._context.run_log_store.get_branch_log(\n            node._get_branch_log_name(map_variable), self._context.run_id\n        )\n\n        branch = \"graph\"\n        if node.internal_branch_name:\n            branch = node.internal_branch_name\n\n        logger.info(f\"Finished execution of {branch} with status {run_log.status}\")\n\n        if dag == self._context.dag:\n            run_log = cast(RunLog, run_log)\n            console.print(\"Completed Execution, Summary:\", style=\"bold color(208)\")\n            console.print(run_log.get_summary(), style=defaults.info_style)\n</code></pre> <p>Step 2: Refactor execute_from_graph to use shared helper</p> <p>Update <code>execute_from_graph</code> to use <code>_prepare_node_for_execution</code>:</p> <pre><code>    def execute_from_graph(self, node: BaseNode, map_variable: MapVariableType = None):\n        \"\"\"Sync node execution entry point.\"\"\"\n        step_log = self._prepare_node_for_execution(node, map_variable)\n        if step_log is None:\n            return  # Skipped\n\n        logger.info(f\"Executing node: {node.get_summary()}\")\n\n        if node.node_type in [\"success\", \"fail\"]:\n            self._execute_node(node, map_variable=map_variable)\n            return\n\n        if node.is_composite:\n            node.execute_as_graph(map_variable=map_variable)\n            return\n\n        task_name = node._resolve_map_placeholders(node.internal_name, map_variable)\n        console.print(\n            f\":runner: Executing the node {task_name} ... \", style=\"bold color(208)\"\n        )\n        self.trigger_node_execution(node=node, map_variable=map_variable)\n</code></pre> <p>Step 3: Run tests to verify no regression</p> <p>Run: <code>uv run pytest tests/extensions/pipeline_executor/ -v</code> Expected: All existing tests PASS</p> <p>Step 4: Commit</p> <pre><code>git add extensions/pipeline_executor/__init__.py\ngit commit -m \"refactor: extract shared helpers in GenericPipelineExecutor\"\n</code></pre>"},{"location":"plans/2026-01-01-async-execution-plan/#task_5_add_async_methods_to_localexecutor","title":"Task 5: Add async methods to LocalExecutor","text":"<p>Files: - Modify: <code>extensions/pipeline_executor/local.py</code></p> <p>Step 1: Add async method implementations</p> <p>Add after existing sync methods:</p> <pre><code>    # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n    # Async Path - implement async methods\n    # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n    async def execute_graph_async(self, dag: Graph, map_variable: MapVariableType = None):\n        \"\"\"Async graph traversal.\"\"\"\n        current_node = dag.start_at\n        previous_node = None\n        logger.info(f\"Running async execution with {current_node}\")\n\n        branch_task_name: str = \"\"\n        if dag.internal_branch_name:\n            branch_task_name = BaseNode._resolve_map_placeholders(\n                dag.internal_branch_name or \"Graph\",\n                map_variable,\n            )\n            console.print(\n                f\":runner: Executing the branch {branch_task_name} ... \",\n                style=\"bold color(208)\",\n            )\n\n        while True:\n            working_on = dag.get_node_by_name(current_node)\n            task_name = working_on._resolve_map_placeholders(\n                working_on.internal_name, map_variable\n            )\n\n            if previous_node == current_node:\n                raise Exception(\"Potentially running in an infinite loop\")\n            previous_node = current_node\n\n            try:\n                await self.execute_from_graph_async(working_on, map_variable=map_variable)\n                # Sync helper - no await needed\n                status, next_node_name = self._get_status_and_next_node_name(\n                    current_node=working_on, dag=dag, map_variable=map_variable\n                )\n\n                if status == defaults.SUCCESS:\n                    console.print(f\":white_check_mark: Node {task_name} succeeded\")\n                else:\n                    console.print(f\":x: Node {task_name} failed\")\n            except Exception as e:\n                console.print(\":x: Error during execution\", style=\"bold red\")\n                console.print(e, style=defaults.error_style)\n                logger.exception(e)\n                raise\n\n            console.rule(style=\"[dark orange]\")\n\n            if working_on.node_type in [\"success\", \"fail\"]:\n                break\n            current_node = next_node_name\n\n        # Sync helper - no await needed\n        self._finalize_graph_execution(working_on, dag, map_variable)\n\n    async def execute_from_graph_async(\n        self, node: BaseNode, map_variable: MapVariableType = None\n    ):\n        \"\"\"Async node execution entry point.\"\"\"\n        # Sync helper - no await needed\n        step_log = self._prepare_node_for_execution(node, map_variable)\n        if step_log is None:\n            return  # Skipped\n\n        logger.info(f\"Executing node: {node.get_summary()}\")\n\n        if node.node_type in [\"success\", \"fail\"]:\n            await self._execute_node_async(node, map_variable=map_variable)\n            return\n\n        if node.is_composite:\n            await node.execute_as_graph_async(map_variable=map_variable)\n            return\n\n        task_name = node._resolve_map_placeholders(node.internal_name, map_variable)\n        console.print(\n            f\":runner: Executing the node {task_name} ... \", style=\"bold color(208)\"\n        )\n        await self.trigger_node_execution_async(node=node, map_variable=map_variable)\n\n    async def trigger_node_execution_async(\n        self, node: BaseNode, map_variable: MapVariableType = None\n    ):\n        \"\"\"Async trigger for node execution.\"\"\"\n        await self._execute_node_async(node=node, map_variable=map_variable)\n\n    async def _execute_node_async(\n        self, node: BaseNode, map_variable: MapVariableType = None, mock: bool = False\n    ):\n        \"\"\"Async node execution wrapper.\"\"\"\n        current_attempt_number = self._calculate_attempt_number(node, map_variable)\n        os.environ[defaults.ATTEMPT_NUMBER] = str(current_attempt_number)\n\n        logger.info(\n            f\"Trying to execute node: {node.internal_name}, attempt: {current_attempt_number}\"\n        )\n\n        self._context_node = node\n\n        # Sync - catalog get\n        data_catalogs_get: Optional[List[DataCatalog]] = self._sync_catalog(stage=\"get\")\n        logger.debug(f\"data_catalogs_get: {data_catalogs_get}\")\n\n        # ASYNC - execute the node\n        step_log = await node.execute_async(\n            map_variable=map_variable,\n            attempt_number=current_attempt_number,\n            mock=mock,\n        )\n\n        # Sync - catalog put and finalization\n        allow_file_not_found_exc = step_log.status != defaults.SUCCESS\n        data_catalogs_put: Optional[List[DataCatalog]] = self._sync_catalog(\n            stage=\"put\", allow_file_no_found_exc=allow_file_not_found_exc\n        )\n        logger.debug(f\"data_catalogs_put: {data_catalogs_put}\")\n        step_log.add_data_catalogs(data_catalogs_put or [])\n        step_log.add_data_catalogs(data_catalogs_get or [])\n\n        console.print(f\"Summary of the step: {step_log.internal_name}\")\n        console.print(step_log.get_summary(), style=defaults.info_style)\n\n        self.add_task_log_to_catalog(\n            name=self._context_node.internal_name, map_variable=map_variable\n        )\n        self._context_node = None\n\n        self._context.run_log_store.add_step_log(step_log, self._context.run_id)\n</code></pre> <p>Step 2: Add necessary imports</p> <p>Add at top of file: <pre><code>from typing import List, Optional\nfrom runnable.datastore import DataCatalog\n</code></pre></p> <p>Step 3: Run tests to verify no regression</p> <p>Run: <code>uv run pytest tests/extensions/pipeline_executor/test_local_executor.py -v</code> Expected: All existing tests PASS</p> <p>Step 4: Commit</p> <pre><code>git add extensions/pipeline_executor/local.py\ngit commit -m \"feat: add async execution methods to LocalExecutor\"\n</code></pre>"},{"location":"plans/2026-01-01-async-execution-plan/#task_6_add_async_methods_to_basenode","title":"Task 6: Add async methods to BaseNode","text":"<p>Files: - Modify: <code>runnable/nodes.py</code></p> <p>Step 1: Add execute_async method to BaseNode</p> <p>Add after the <code>execute</code> method:</p> <pre><code>    async def execute_async(\n        self,\n        map_variable: MapVariableType = None,\n        attempt_number: int = 1,\n        mock: bool = False,\n    ) -&gt; StepLog:\n        \"\"\"\n        Async execution - default delegates to sync execute().\n\n        Override in subclasses that support true async execution (TaskNode).\n        Terminal nodes (SuccessNode, FailNode) use this default.\n        \"\"\"\n        return self.execute(\n            map_variable=map_variable,\n            attempt_number=attempt_number,\n            mock=mock,\n        )\n</code></pre> <p>Step 2: Run tests to verify no regression</p> <p>Run: <code>uv run pytest tests/runnable/test_nodes.py -v</code> Expected: All existing tests PASS</p> <p>Step 3: Commit</p> <pre><code>git add runnable/nodes.py\ngit commit -m \"feat: add execute_async method to BaseNode\"\n</code></pre>"},{"location":"plans/2026-01-01-async-execution-plan/#task_7_add_execute_as_graph_async_to_compositenode","title":"Task 7: Add execute_as_graph_async to CompositeNode","text":"<p>Files: - Modify: <code>runnable/nodes.py</code></p> <p>Step 1: Add execute_as_graph_async to CompositeNode</p> <p>Find <code>CompositeNode</code> class and add:</p> <pre><code>    async def execute_as_graph_async(self, map_variable: MapVariableType = None):\n        \"\"\"\n        Async execution of sub-graph.\n\n        Default raises NotImplementedError - override in subclasses\n        that support async execution (ParallelNode, MapNode, DagNode).\n        \"\"\"\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} must implement execute_as_graph_async() \"\n            f\"for async execution support.\"\n        )\n</code></pre> <p>Step 2: Commit</p> <pre><code>git add runnable/nodes.py\ngit commit -m \"feat: add execute_as_graph_async stub to CompositeNode\"\n</code></pre>"},{"location":"plans/2026-01-01-async-execution-plan/#task_8_add_async_methods_to_tasknode","title":"Task 8: Add async methods to TaskNode","text":"<p>Files: - Modify: <code>runnable/nodes.py</code></p> <p>Step 1: Add execute_async to TaskNode</p> <p>Find <code>TaskNode</code> class and override <code>execute_async</code>:</p> <pre><code>    async def execute_async(\n        self,\n        map_variable: MapVariableType = None,\n        attempt_number: int = 1,\n        mock: bool = False,\n    ) -&gt; StepLog:\n        \"\"\"Async task execution.\"\"\"\n        step_log = self._context.run_log_store.get_step_log(\n            self._get_step_log_name(map_variable), self._context.run_id\n        )\n\n        task = self._get_task()\n\n        # Try async first, fall back to sync\n        try:\n            attempt_log = await task.execute_command_async(map_variable=map_variable)\n        except NotImplementedError:\n            # Task doesn't support async, fall back to sync\n            attempt_log = task.execute_command(map_variable=map_variable)\n\n        step_log.attempts.append(attempt_log)\n        step_log.status = attempt_log.status\n        return step_log\n</code></pre> <p>Step 2: Commit</p> <pre><code>git add runnable/nodes.py\ngit commit -m \"feat: add execute_async to TaskNode with fallback to sync\"\n</code></pre>"},{"location":"plans/2026-01-01-async-execution-plan/#task_9_add_async_methods_to_parallelnode","title":"Task 9: Add async methods to ParallelNode","text":"<p>Files: - Modify: <code>extensions/nodes/parallel.py</code></p> <p>Step 1: Add execute_as_graph_async method</p> <p>Add after <code>execute_as_graph</code>:</p> <pre><code>    async def execute_as_graph_async(self, map_variable: MapVariableType = None):\n        \"\"\"Async parallel execution.\"\"\"\n        self.fan_out(map_variable=map_variable)  # sync - just creates branch logs\n\n        for _, branch in self.branches.items():\n            await self._context.pipeline_executor.execute_graph_async(\n                branch, map_variable=map_variable\n            )\n\n        self.fan_in(map_variable=map_variable)  # sync - just collates status\n</code></pre> <p>Step 2: Run tests to verify no regression</p> <p>Run: <code>uv run pytest tests/extensions/nodes/test_parallel.py -v</code> Expected: All existing tests PASS</p> <p>Step 3: Commit</p> <pre><code>git add extensions/nodes/parallel.py\ngit commit -m \"feat: add execute_as_graph_async to ParallelNode\"\n</code></pre>"},{"location":"plans/2026-01-01-async-execution-plan/#task_10_add_async_methods_to_mapnode","title":"Task 10: Add async methods to MapNode","text":"<p>Files: - Modify: <code>extensions/nodes/map.py</code></p> <p>Step 1: Add execute_as_graph_async method</p> <p>Add after <code>execute_as_graph</code>:</p> <pre><code>    async def execute_as_graph_async(self, map_variable: MapVariableType = None):\n        \"\"\"Async map execution.\"\"\"\n        self.fan_out(map_variable=map_variable)  # sync\n\n        iterate_on = self._context.run_log_store.get_parameters(self._context.run_id)[\n            self.iterate_on\n        ].get_value()\n\n        for iter_variable in iterate_on:\n            effective_map_variable = map_variable or {}\n            effective_map_variable[self.iterate_as] = iter_variable\n\n            await self._context.pipeline_executor.execute_graph_async(\n                self.branch, map_variable=effective_map_variable\n            )\n\n        self.fan_in(map_variable=map_variable)  # sync\n</code></pre> <p>Step 2: Commit</p> <pre><code>git add extensions/nodes/map.py\ngit commit -m \"feat: add execute_as_graph_async to MapNode\"\n</code></pre>"},{"location":"plans/2026-01-01-async-execution-plan/#task_11_add_execute_async_to_pipelinecontext","title":"Task 11: Add execute_async to PipelineContext","text":"<p>Files: - Modify: <code>runnable/context.py</code></p> <p>Step 1: Add shared helper for completion handling</p> <p>Add to <code>PipelineContext</code>:</p> <pre><code>    def _handle_completion(self):\n        \"\"\"Handle post-execution - shared by sync/async.\"\"\"\n        run_log = self.run_log_store.get_run_log_by_id(\n            run_id=self.run_id, full=False\n        )\n\n        if run_log.status == defaults.SUCCESS:\n            console.print(\"Pipeline executed successfully!\", style=defaults.success_style)\n            logfire.info(\"Pipeline completed\", status=\"success\")\n        else:\n            console.print(\"Pipeline execution failed.\", style=defaults.error_style)\n            logfire.error(\"Pipeline failed\", status=\"failed\")\n            raise exceptions.ExecutionFailedError(self.run_id)\n</code></pre> <p>Step 2: Add execute_async method</p> <p>Add after <code>execute</code> method:</p> <pre><code>    async def execute_async(self):\n        \"\"\"Async pipeline execution.\"\"\"\n        assert self.dag is not None\n\n        pipeline_name = getattr(self.dag, \"name\", \"unnamed\")\n\n        with logfire.span(\n            \"pipeline:{pipeline_name}\",\n            pipeline_name=pipeline_name,\n            run_id=self.run_id,\n            executor=self.pipeline_executor.__class__.__name__,\n        ):\n            logfire.info(\"Async pipeline execution started\")\n\n            console.print(\"Working with context:\")\n            console.print(run_context)\n            console.rule(style=\"[dark orange]\")\n\n            if self.pipeline_executor._should_setup_run_log_at_traversal:\n                self.pipeline_executor._set_up_run_log(exists_ok=False)\n\n            try:\n                await self.pipeline_executor.execute_graph_async(dag=self.dag)\n                self._handle_completion()\n\n            except Exception as e:\n                console.print(e, style=defaults.error_style)\n                logfire.error(\"Pipeline failed with exception\", error=str(e)[:256])\n                raise\n\n            if self.pipeline_executor._should_setup_run_log_at_traversal:\n                return run_context.run_log_store.get_run_log_by_id(\n                    run_id=run_context.run_id\n                )\n</code></pre> <p>Step 3: Refactor execute to use shared helper</p> <p>Update <code>execute</code> method to use <code>_handle_completion()</code>.</p> <p>Step 4: Commit</p> <pre><code>git add runnable/context.py\ngit commit -m \"feat: add execute_async to PipelineContext\"\n</code></pre>"},{"location":"plans/2026-01-01-async-execution-plan/#task_12_create_asyncpythontasktype_-_test","title":"Task 12: Create AsyncPythonTaskType - Test","text":"<p>Files: - Create: <code>tests/runnable/test_async_tasks.py</code></p> <p>Step 1: Write the failing test</p> <pre><code>import pytest\n\nfrom runnable.tasks import AsyncPythonTaskType\n\n\ndef test_async_python_task_type_initialization():\n    \"\"\"Test AsyncPythonTaskType can be instantiated with command.\"\"\"\n    task = AsyncPythonTaskType(command=\"examples.common.functions.hello\")\n    assert task.task_type == \"async-python\"\n    assert task.command == \"examples.common.functions.hello\"\n\n\ndef test_async_python_task_type_sync_raises():\n    \"\"\"Test AsyncPythonTaskType.execute_command raises RuntimeError.\"\"\"\n    task = AsyncPythonTaskType(command=\"test.func\")\n    with pytest.raises(RuntimeError, match=\"requires async execution\"):\n        task.execute_command()\n</code></pre> <p>Step 2: Run test to verify it fails</p> <p>Run: <code>uv run pytest tests/runnable/test_async_tasks.py -v</code> Expected: FAIL with \"cannot import name 'AsyncPythonTaskType'\"</p> <p>Step 3: Commit test</p> <pre><code>git add tests/runnable/test_async_tasks.py\ngit commit -m \"test: add failing tests for AsyncPythonTaskType\"\n</code></pre>"},{"location":"plans/2026-01-01-async-execution-plan/#task_13_create_asyncpythontasktype_-_implementation","title":"Task 13: Create AsyncPythonTaskType - Implementation","text":"<p>Files: - Modify: <code>runnable/tasks.py</code></p> <p>Step 1: Add AsyncPythonTaskType class</p> <p>Add after <code>ShellTaskType</code>:</p> <pre><code>class AsyncPythonTaskType(BaseTaskType):\n    \"\"\"\n    Task type for executing async Python functions.\n\n    Supports both regular async functions and AsyncGenerator functions\n    for streaming use cases. Sync execution raises RuntimeError.\n    \"\"\"\n\n    task_type: str = Field(default=\"async-python\", serialization_alias=\"command_type\")\n    command: str\n\n    def execute_command(self, map_variable: MapVariableType = None) -&gt; StepAttempt:\n        \"\"\"Sync execution - not supported for async tasks.\"\"\"\n        raise RuntimeError(\n            \"AsyncPythonTaskType requires async execution. \"\n            \"Use AsyncPipeline or ensure executor calls execute_command_async().\"\n        )\n\n    async def execute_command_async(\n        self,\n        map_variable: MapVariableType = None,\n        event_callback: Optional[Callable[[dict], None]] = None,\n    ) -&gt; StepAttempt:\n        \"\"\"Execute an async Python function.\"\"\"\n        attempt_log = StepAttempt(\n            status=defaults.FAIL,\n            start_time=str(datetime.now()),\n        )\n\n        def emit_event(event: dict):\n            if event_callback:\n                event_callback(event)\n\n        with logfire.span(\n            \"task:{task_name}\",\n            task_name=self.command,\n            task_type=self.task_type,\n        ):\n            with self.execution_context(map_variable=map_variable) as params:\n                with self.expose_secrets():\n                    logfire.info(\n                        \"Task started\",\n                        inputs=self._safe_serialize_params(params),\n                    )\n                    emit_event({\n                        \"type\": \"task_started\",\n                        \"name\": self.command,\n                        \"inputs\": self._safe_serialize_params(params),\n                    })\n\n                    module, func = utils.get_module_and_attr_names(self.command)\n                    sys.path.insert(0, os.getcwd())\n                    imported_module = importlib.import_module(module)\n                    f = getattr(imported_module, func)\n\n                    try:\n                        filtered_parameters = parameters.filter_arguments_for_func(\n                            f, params.copy(), map_variable\n                        )\n                        logger.info(\n                            f\"Calling async {func} from {module} with {filtered_parameters}\"\n                        )\n\n                        with redirect_output(console=task_console) as (buffer, stderr_buffer):\n                            result = await f(**filtered_parameters)\n\n                            # Handle AsyncGenerator (streaming)\n                            if inspect.isasyncgen(result):\n                                accumulated_chunks: List[str] = []\n                                async for chunk in result:\n                                    accumulated_chunks.append(str(chunk))\n                                    emit_event({\n                                        \"type\": \"task_chunk\",\n                                        \"name\": self.command,\n                                        \"chunk\": str(chunk),\n                                    })\n                                user_set_parameters = \"\".join(accumulated_chunks)\n                            else:\n                                user_set_parameters = result\n\n                        # Process returns (reuse existing logic)\n                        if self.returns:\n                            if not isinstance(user_set_parameters, tuple):\n                                user_set_parameters = (user_set_parameters,)\n\n                            if len(user_set_parameters) != len(self.returns):\n                                raise ValueError(\n                                    \"Returns task signature does not match function returns\"\n                                )\n\n                            output_parameters: Dict[str, Parameter] = {}\n                            metrics: Dict[str, Parameter] = {}\n\n                            for i, task_return in enumerate(self.returns):\n                                output_parameter = task_return_to_parameter(\n                                    task_return=task_return,\n                                    value=user_set_parameters[i],\n                                )\n\n                                if task_return.kind == \"metric\":\n                                    metrics[task_return.name] = output_parameter\n\n                                param_name = task_return.name\n                                if map_variable:\n                                    for _, v in map_variable.items():\n                                        param_name = f\"{v}_{param_name}\"\n\n                                output_parameters[param_name] = output_parameter\n\n                            attempt_log.output_parameters = output_parameters\n                            attempt_log.user_defined_metrics = metrics\n                            params.update(output_parameters)\n\n                            logfire.info(\n                                \"Task completed\",\n                                outputs=self._safe_serialize_params(output_parameters),\n                                status=\"success\",\n                            )\n                            emit_event({\n                                \"type\": \"task_completed\",\n                                \"name\": self.command,\n                                \"outputs\": self._safe_serialize_params(output_parameters),\n                            })\n                        else:\n                            logfire.info(\"Task completed\", status=\"success\")\n                            emit_event({\"type\": \"task_completed\", \"name\": self.command})\n\n                        attempt_log.status = defaults.SUCCESS\n\n                    except Exception as e:\n                        msg = f\"Call to async function {self.command} did not succeed.\\n\"\n                        attempt_log.message = msg\n                        task_console.print_exception(show_locals=False)\n                        task_console.log(e, style=defaults.error_style)\n                        logfire.error(\"Task failed\", error=str(e)[:256])\n                        emit_event({\n                            \"type\": \"task_error\",\n                            \"name\": self.command,\n                            \"error\": str(e)[:256],\n                        })\n\n        attempt_log.end_time = str(datetime.now())\n        return attempt_log\n</code></pre> <p>Step 2: Run tests</p> <p>Run: <code>uv run pytest tests/runnable/test_async_tasks.py -v</code> Expected: PASS</p> <p>Step 3: Commit</p> <pre><code>git add runnable/tasks.py\ngit commit -m \"feat: add AsyncPythonTaskType for async function execution\"\n</code></pre>"},{"location":"plans/2026-01-01-async-execution-plan/#task_14_register_asyncpythontasktype_plugin","title":"Task 14: Register AsyncPythonTaskType plugin","text":"<p>Files: - Modify: <code>pyproject.toml</code></p> <p>Step 1: Add entry point</p> <pre><code>[project.entry-points.'tasks']\n\"async-python\" = \"runnable.tasks:AsyncPythonTaskType\"\n</code></pre> <p>Step 2: Sync and verify</p> <p>Run: <code>uv sync --all-extras --dev</code></p> <p>Verify: <pre><code>uv run python -c \"from stevedore import driver; mgr = driver.DriverManager(namespace='tasks', name='async-python', invoke_on_load=True, invoke_kwds={'command': 'test'}); print(mgr.driver)\"\n</code></pre></p> <p>Step 3: Commit</p> <pre><code>git add pyproject.toml\ngit commit -m \"feat: register async-python task plugin\"\n</code></pre>"},{"location":"plans/2026-01-01-async-execution-plan/#task_15_create_asyncpythontask_sdk_class","title":"Task 15: Create AsyncPythonTask SDK class","text":"<p>Files: - Modify: <code>runnable/sdk.py</code></p> <p>Step 1: Add imports</p> <pre><code>import asyncio\n</code></pre> <p>Step 2: Add AsyncPythonTask class</p> <pre><code>class AsyncPythonTask(BaseTraversal):\n    \"\"\"SDK class for async Python tasks.\"\"\"\n\n    function: Callable = Field(exclude=True)\n    returns: List[Union[str, TaskReturns]] = Field(default_factory=list)\n    catalog: Optional[Catalog] = Field(default=None)\n    overrides: Dict[str, Any] = Field(default_factory=dict)\n    secrets: List[str] = Field(default_factory=list)\n\n    _task_type: str = PrivateAttr(default=\"async-python\")\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    @field_validator(\"function\")\n    @classmethod\n    def validate_async_function(cls, func: Callable) -&gt; Callable:\n        \"\"\"Validate that the function is an async function.\"\"\"\n        if not asyncio.iscoroutinefunction(func):\n            raise ValueError(\n                f\"function must be an async function (defined with 'async def'), \"\n                f\"got {type(func).__name__}\"\n            )\n        return func\n\n    @field_validator(\"returns\", mode=\"before\")\n    @classmethod\n    def serialize_returns(cls, returns: List[Union[str, TaskReturns]]) -&gt; List[TaskReturns]:\n        task_returns = []\n        for ret in returns:\n            if isinstance(ret, str):\n                task_returns.append(TaskReturns(name=ret, kind=\"json\"))\n            else:\n                task_returns.append(ret)\n        return task_returns\n\n    def create_node(self) -&gt; TaskNode:\n        if self.function:\n            module = self.function.__module__\n            name = self.function.__qualname__\n            command = f\"{module}.{name}\"\n        else:\n            raise ValueError(\"AsyncPythonTask requires a function\")\n\n        node_dict = {\n            \"command\": command,\n            \"command_type\": self._task_type,\n            \"returns\": [r.model_dump() for r in self.returns],\n            \"catalog\": self.catalog.model_dump() if self.catalog else None,\n            \"overrides\": self.overrides,\n            \"secrets\": self.secrets,\n        }\n\n        return TaskNode.parse_from_config(self.model_dump() | node_dict)\n</code></pre> <p>Step 3: Update StepType union</p> <p>Add <code>AsyncPythonTask</code> to <code>StepType</code>.</p> <p>Step 4: Commit</p> <pre><code>git add runnable/sdk.py\ngit commit -m \"feat: add AsyncPythonTask SDK class\"\n</code></pre>"},{"location":"plans/2026-01-01-async-execution-plan/#task_16_create_asyncpipeline_sdk_class","title":"Task 16: Create AsyncPipeline SDK class","text":"<p>Files: - Modify: <code>runnable/sdk.py</code></p> <p>Step 1: Add AsyncPipeline class</p> <pre><code>class AsyncPipeline(BaseModel):\n    \"\"\"Pipeline with async execution.\"\"\"\n\n    steps: List[StepType] = Field(default_factory=list)\n    name: str = Field(default=\"\")\n    description: str = Field(default=\"\")\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    @model_validator(mode=\"after\")\n    def validate_steps(self) -&gt; Self:\n        if not self.steps:\n            raise ValueError(\"AsyncPipeline must have at least one step\")\n\n        for i, step in enumerate(self.steps[:-1]):\n            if not step.next_node:\n                step.next_node = self.steps[i + 1].name\n\n        last_step = self.steps[-1]\n        if not last_step.terminate_with_success and not last_step.terminate_with_failure:\n            last_step.terminate_with_success = True\n\n        return self\n\n    def return_dag(self) -&gt; graph.Graph:\n        dag = graph.Graph(\n            start_at=self.steps[0].name,\n            name=self.name,\n            description=self.description,\n        )\n\n        for step in self.steps:\n            dag.add_node(step.create_node())\n\n        dag.add_terminal_nodes()\n        dag.check_graph()\n\n        return dag\n\n    async def execute(\n        self,\n        configuration_file: str = \"\",\n        run_id: str = \"\",\n        tag: str = \"\",\n        parameters_file: str = \"\",\n    ):\n        \"\"\"Execute the pipeline asynchronously.\"\"\"\n        from runnable.context import PipelineContext\n\n        dag = self.return_dag()\n\n        services = defaults.DEFAULT_SERVICES.copy()\n        services[\"pipeline_executor\"] = {\"type\": \"local\"}\n\n        context = PipelineContext(\n            pipeline_executor=services[\"pipeline_executor\"],\n            catalog=services[\"catalog\"],\n            secrets=services[\"secrets\"],\n            pickler=services[\"pickler\"],\n            run_log_store=services[\"run_log_store\"],\n            run_id=run_id,\n            tag=tag,\n            parameters_file=parameters_file,\n            configuration_file=configuration_file,\n            pipeline_definition_file=f\"{self.__class__.__module__}.{self.__class__.__qualname__}\",\n        )\n\n        context.__dict__[\"_dag\"] = dag\n\n        await context.execute_async()\n</code></pre> <p>Step 2: Commit</p> <pre><code>git add runnable/sdk.py\ngit commit -m \"feat: add AsyncPipeline SDK class\"\n</code></pre>"},{"location":"plans/2026-01-01-async-execution-plan/#task_17_export_async_classes","title":"Task 17: Export async classes","text":"<p>Files: - Modify: <code>runnable/__init__.py</code></p> <p>Step 1: Add exports</p> <pre><code>from runnable.sdk import (\n    # ... existing ...\n    AsyncPythonTask,\n    AsyncPipeline,\n)\n</code></pre> <p>Step 2: Verify</p> <pre><code>uv run python -c \"from runnable import AsyncPipeline, AsyncPythonTask; print('OK')\"\n</code></pre> <p>Step 3: Commit</p> <pre><code>git add runnable/__init__.py\ngit commit -m \"feat: export async classes from runnable package\"\n</code></pre>"},{"location":"plans/2026-01-01-async-execution-plan/#task_18_integration_test","title":"Task 18: Integration test","text":"<p>Files: - Create: <code>tests/integration/test_async_pipeline.py</code></p> <p>Step 1: Write integration test</p> <pre><code>import pytest\nimport asyncio\n\nfrom runnable import AsyncPipeline, AsyncPythonTask\n\n\nasync def compute(x: int = 10) -&gt; int:\n    await asyncio.sleep(0.01)\n    return x * 2\n\n\nasync def finalize(result: int) -&gt; str:\n    await asyncio.sleep(0.01)\n    return f\"Final: {result}\"\n\n\n@pytest.mark.asyncio\nasync def test_simple_async_pipeline_execution():\n    pipeline = AsyncPipeline(\n        steps=[\n            AsyncPythonTask(function=compute, name=\"compute\", returns=[\"result\"]),\n            AsyncPythonTask(function=finalize, name=\"finalize\", returns=[\"final\"]),\n        ]\n    )\n\n    await pipeline.execute()\n</code></pre> <p>Step 2: Run test</p> <p>Run: <code>uv run pytest tests/integration/test_async_pipeline.py -v</code> Expected: PASS</p> <p>Step 3: Commit</p> <pre><code>git add tests/integration/test_async_pipeline.py\ngit commit -m \"test: add integration test for async pipeline\"\n</code></pre>"},{"location":"plans/2026-01-01-async-execution-plan/#task_19_final_verification","title":"Task 19: Final verification","text":"<p>Step 1: Run all async tests</p> <pre><code>uv run pytest tests/runnable/test_async_tasks.py tests/integration/test_async_pipeline.py -v\n</code></pre> <p>Step 2: Run full test suite</p> <pre><code>uv run pytest --ignore=tests/integration -v\n</code></pre> <p>Step 3: Final commit</p> <pre><code>git add -A\ngit commit -m \"chore: async execution feature complete\"\n</code></pre>"},{"location":"plans/2026-01-01-async-execution-plan/#summary","title":"Summary","text":"<p>This plan implements:</p> <ol> <li> <p>Base class async stubs - <code>BasePipelineExecutor</code> and <code>BaseTaskType</code> get async methods that raise <code>NotImplementedError</code></p> </li> <li> <p>Shared helpers - <code>GenericPipelineExecutor</code> extracts common logic into helpers called by both sync/async paths</p> </li> <li> <p>LocalExecutor async methods - <code>execute_graph_async</code>, <code>execute_from_graph_async</code>, <code>trigger_node_execution_async</code>, <code>_execute_node_async</code></p> </li> <li> <p>Node async methods - <code>BaseNode.execute_async</code>, <code>CompositeNode.execute_as_graph_async</code>, <code>TaskNode.execute_async</code> (with fallback)</p> </li> <li> <p>Composite node async - <code>ParallelNode.execute_as_graph_async</code>, <code>MapNode.execute_as_graph_async</code></p> </li> <li> <p>PipelineContext.execute_async - Async pipeline execution entry point</p> </li> <li> <p>AsyncPythonTaskType - Task type for async functions with streaming support</p> </li> <li> <p>SDK classes - <code>AsyncPythonTask</code>, <code>AsyncPipeline</code></p> </li> </ol> <p>Key architectural decisions: - No separate <code>AsyncLocalExecutor</code> - <code>LocalExecutor</code> implements both sync and async - No separate <code>AsyncPipelineContext</code> - <code>PipelineContext</code> has both <code>execute()</code> and <code>execute_async()</code> - Shared helpers minimize code duplication between sync/async paths - Clear <code>NotImplementedError</code> messages when async called on unsupported components</p>"},{"location":"plans/2026-01-02-contextvars-run-context/","title":"Contextvars Run Context Implementation Plan","text":"<p>For Claude: REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.</p> <p>Goal: Replace global run_context with contextvars to fix concurrency issues when multiple pipelines run simultaneously (e.g., FastAPI async endpoints).</p> <p>Architecture: Replace the global <code>run_context</code> variable in <code>context.py</code> with Python's <code>contextvars</code> module for proper request-level isolation. Update all references throughout the codebase to use context-aware getter functions. This ensures each execution context (async task, thread, etc.) maintains its own isolated run context.</p> <p>Tech Stack: Python contextvars, Pydantic, existing runnable framework</p>"},{"location":"plans/2026-01-02-contextvars-run-context/#task_1_write_tests_for_context_isolation","title":"Task 1: Write Tests for Context Isolation","text":"<p>Files: - Create: <code>tests/runnable/test_context_isolation.py</code></p> <p>Step 1: Write the failing test</p> <pre><code>import asyncio\nimport pytest\nfrom runnable.context import PipelineContext, get_run_context, set_run_context\n\n\ndef test_context_isolation_sync():\n    \"\"\"Test that different contexts don't interfere in sync execution.\"\"\"\n    # Create first context\n    context1 = PipelineContext(\n        pipeline_definition_file=\"test1.py\",\n        run_id=\"test-run-1\",\n        catalog={\"type\": \"file-system\"},\n        secrets={\"type\": \"env-secrets\"},\n        pickler={\"type\": \"pickle\"},\n        run_log_store={\"type\": \"memory\"},\n        pipeline_executor={\"type\": \"local\"}\n    )\n\n    # Create second context\n    context2 = PipelineContext(\n        pipeline_definition_file=\"test2.py\",\n        run_id=\"test-run-2\",\n        catalog={\"type\": \"file-system\"},\n        secrets={\"type\": \"env-secrets\"},\n        pickler={\"type\": \"pickle\"},\n        run_log_store={\"type\": \"memory\"},\n        pipeline_executor={\"type\": \"local\"}\n    )\n\n    # Set context1, verify it's returned\n    set_run_context(context1)\n    assert get_run_context().run_id == \"test-run-1\"\n\n    # Set context2, verify it overwrites (current behavior)\n    set_run_context(context2)\n    assert get_run_context().run_id == \"test-run-2\"\n\n\n@pytest.mark.asyncio\nasync def test_context_isolation_async():\n    \"\"\"Test that async tasks maintain separate contexts.\"\"\"\n    results = []\n\n    async def async_task(context_id: str):\n        context = PipelineContext(\n            pipeline_definition_file=f\"test{context_id}.py\",\n            run_id=f\"test-run-{context_id}\",\n            catalog={\"type\": \"file-system\"},\n            secrets={\"type\": \"env-secrets\"},\n            pickler={\"type\": \"pickle\"},\n            run_log_store={\"type\": \"memory\"},\n            pipeline_executor={\"type\": \"local\"}\n        )\n\n        # Each task should maintain its own context\n        current_context = get_run_context()\n        assert current_context is not None\n        assert current_context.run_id == f\"test-run-{context_id}\"\n\n        # Simulate some async work\n        await asyncio.sleep(0.1)\n\n        # Context should still be correct after await\n        current_context = get_run_context()\n        assert current_context.run_id == f\"test-run-{context_id}\"\n        results.append(context_id)\n\n    # Run multiple async tasks concurrently\n    await asyncio.gather(\n        async_task(\"1\"),\n        async_task(\"2\"),\n        async_task(\"3\")\n    )\n\n    assert len(results) == 3\n    assert set(results) == {\"1\", \"2\", \"3\"}\n</code></pre> <p>Step 2: Run test to verify it fails</p> <p>Run: <code>uv run pytest tests/runnable/test_context_isolation.py -v</code> Expected: FAIL with \"ImportError: cannot import name 'get_run_context' from 'runnable.context'\"</p> <p>Step 3: Commit failing test</p> <pre><code>git add tests/runnable/test_context_isolation.py\ngit commit -m \"test: add context isolation tests (failing)\"\n</code></pre>"},{"location":"plans/2026-01-02-contextvars-run-context/#task_2_implement_contextvars_in_contextpy","title":"Task 2: Implement Contextvars in context.py","text":"<p>Files: - Modify: <code>runnable/context.py:1-10</code> (imports) - Modify: <code>runnable/context.py:526-527</code> (global variable) - Modify: <code>runnable/context.py:276-278</code> (model_post_init)</p> <p>Step 1: Add contextvars import</p> <pre><code>import contextvars\nimport hashlib\nimport importlib\nimport json\nimport logging\nimport os\nimport sys\nfrom datetime import datetime\nfrom enum import Enum\nfrom functools import cached_property, partial\nfrom typing import Annotated, Any, Callable, Dict, Optional, TYPE_CHECKING\n</code></pre> <p>Step 2: Replace global variable with contextvars</p> <p>Replace: <pre><code>run_context: PipelineContext | JobContext = None  # type: ignore\n</code></pre></p> <p>With: <pre><code># Context variable for thread/async-safe run context storage\nif TYPE_CHECKING:\n    from typing import Union\n    RunnableContextType = Union['PipelineContext', 'JobContext']\nelse:\n    RunnableContextType = Any\n\n_run_context_var: contextvars.ContextVar[Optional[RunnableContextType]] = contextvars.ContextVar(\n    'run_context',\n    default=None\n)\n\ndef get_run_context() -&gt; Optional[RunnableContextType]:\n    \"\"\"Get the current run context for this execution context.\"\"\"\n    return _run_context_var.get()\n\ndef set_run_context(context: RunnableContextType) -&gt; None:\n    \"\"\"Set the run context for this execution context.\"\"\"\n    _run_context_var.set(context)\n\n# Backward compatibility property (deprecated)\n@property\ndef run_context() -&gt; Optional[RunnableContextType]:\n    \"\"\"Deprecated: Use get_run_context() instead.\"\"\"\n    return get_run_context()\n</code></pre></p> <p>Step 3: Update model_post_init</p> <p>Replace: <pre><code>    def model_post_init(self, __context: Any) -&gt; None:\n        os.environ[defaults.ENV_RUN_ID] = self.run_id\n\n        if self.configuration_file:\n            os.environ[defaults.RUNNABLE_CONFIGURATION_FILE] = self.configuration_file\n        if self.tag:\n            os.environ[defaults.RUNNABLE_RUN_TAG] = self.tag\n\n        global run_context\n        if not run_context:\n            run_context = self  # type: ignore\n</code></pre></p> <p>With: <pre><code>    def model_post_init(self, __context: Any) -&gt; None:\n        os.environ[defaults.ENV_RUN_ID] = self.run_id\n\n        if self.configuration_file:\n            os.environ[defaults.RUNNABLE_CONFIGURATION_FILE] = self.configuration_file\n        if self.tag:\n            os.environ[defaults.RUNNABLE_RUN_TAG] = self.tag\n\n        # Set the context using contextvars for proper isolation\n        set_run_context(self)\n</code></pre></p> <p>Step 4: Run tests to verify contextvars basics work</p> <p>Run: <code>uv run pytest tests/runnable/test_context_isolation.py::test_context_isolation_sync -v</code> Expected: PASS</p> <p>Step 5: Commit contextvars implementation</p> <pre><code>git add runnable/context.py\ngit commit -m \"feat: implement contextvars for run_context isolation\"\n</code></pre>"},{"location":"plans/2026-01-02-contextvars-run-context/#task_3_update_taskspy_to_use_get_run_context","title":"Task 3: Update tasks.py to use get_run_context()","text":"<p>Files: - Modify: <code>runnable/tasks.py:141</code> (_context property) - Modify: <code>runnable/tasks.py:146</code> (set_secrets_as_env_variables) - Modify: <code>runnable/tasks.py:570</code> (notebook task) - Modify: <code>runnable/tasks.py:716</code> (shell task)</p> <p>Step 1: Write failing test for tasks context usage</p> <p>Add to <code>tests/runnable/test_context_isolation.py</code>:</p> <pre><code>def test_task_uses_correct_context():\n    \"\"\"Test that tasks access the correct context.\"\"\"\n    from runnable.tasks import BaseTaskType\n    from runnable.context import PipelineContext, set_run_context\n\n    # Create a mock task\n    class MockTask(BaseTaskType):\n        command: str = \"test\"\n        secrets: list = []\n\n    task = MockTask()\n\n    # Create and set context\n    context = PipelineContext(\n        pipeline_definition_file=\"test.py\",\n        run_id=\"task-test-run\",\n        catalog={\"type\": \"file-system\"},\n        secrets={\"type\": \"env-secrets\"},\n        pickler={\"type\": \"pickle\"},\n        run_log_store={\"type\": \"memory\"},\n        pipeline_executor={\"type\": \"local\"}\n    )\n\n    # Task should access the correct context\n    task_context = task._context\n    assert task_context is not None\n    assert task_context.run_id == \"task-test-run\"\n</code></pre> <p>Step 2: Run test to verify it fails</p> <p>Run: <code>uv run pytest tests/runnable/test_context_isolation.py::test_task_uses_correct_context -v</code> Expected: FAIL (tasks still use old context access)</p> <p>Step 3: Update tasks.py context access</p> <p>Replace: <pre><code>    @property\n    def _context(self):\n        return context.run_context\n</code></pre></p> <p>With: <pre><code>    @property\n    def _context(self):\n        current_context = context.get_run_context()\n        if current_context is None:\n            raise RuntimeError(\"No run context available in current execution context\")\n        return current_context\n</code></pre></p> <p>Step 4: Update secrets handling</p> <p>Replace: <pre><code>    def set_secrets_as_env_variables(self):\n        # Preparing the environment for the task execution\n        for key in self.secrets:\n            secret_value = context.run_context.secrets.get(key)\n            os.environ[key] = secret_value\n</code></pre></p> <p>With: <pre><code>    def set_secrets_as_env_variables(self):\n        # Preparing the environment for the task execution\n        current_context = context.get_run_context()\n        if current_context is None:\n            raise RuntimeError(\"No run context available for secrets\")\n\n        for key in self.secrets:\n            secret_value = current_context.secrets.get(key)\n            os.environ[key] = secret_value\n</code></pre></p> <p>Step 5: Update notebook task context usage</p> <p>Find line around 570 and replace: <pre><code>                context.run_context.catalog.put(name=notebook_output_path)\n</code></pre></p> <p>With: <pre><code>                current_context = context.get_run_context()\n                if current_context is None:\n                    raise RuntimeError(\"No run context available for catalog operations\")\n                current_context.catalog.put(name=notebook_output_path)\n</code></pre></p> <p>Step 6: Update shell task secrets</p> <p>Find line around 716 and replace: <pre><code>        if self.secrets:\n            for key in self.secrets:\n                secret_value = context.run_context.secrets.get(key)\n                subprocess_env[key] = secret_value\n</code></pre></p> <p>With: <pre><code>        if self.secrets:\n            current_context = context.get_run_context()\n            if current_context is None:\n                raise RuntimeError(\"No run context available for secrets\")\n\n            for key in self.secrets:\n                secret_value = current_context.secrets.get(key)\n                subprocess_env[key] = secret_value\n</code></pre></p> <p>Step 7: Run task tests</p> <p>Run: <code>uv run pytest tests/runnable/test_context_isolation.py::test_task_uses_correct_context -v</code> Expected: PASS</p> <p>Step 8: Commit tasks.py updates</p> <pre><code>git add runnable/tasks.py\ngit commit -m \"feat: update tasks.py to use contextvars run_context\"\n</code></pre>"},{"location":"plans/2026-01-02-contextvars-run-context/#task_4_update_executorpy_to_use_get_run_context","title":"Task 4: Update executor.py to use get_run_context()","text":"<p>Files: - Modify: <code>runnable/executor.py:45</code> (_context property)</p> <p>Step 1: Write test for executor context usage</p> <p>Add to <code>tests/runnable/test_context_isolation.py</code>:</p> <pre><code>def test_executor_uses_correct_context():\n    \"\"\"Test that executors access the correct context.\"\"\"\n    from runnable.executor import BaseJobExecutor\n    from runnable.context import PipelineContext, set_run_context\n\n    # Create mock executor\n    class MockExecutor(BaseJobExecutor):\n        def submit_job(self, job, catalog_settings=None):\n            pass\n\n    executor = MockExecutor()\n\n    # Create and set context\n    context = PipelineContext(\n        pipeline_definition_file=\"test.py\",\n        run_id=\"executor-test-run\",\n        catalog={\"type\": \"file-system\"},\n        secrets={\"type\": \"env-secrets\"},\n        pickler={\"type\": \"pickle\"},\n        run_log_store={\"type\": \"memory\"},\n        pipeline_executor={\"type\": \"local\"}\n    )\n\n    # Executor should access correct context\n    executor_context = executor._context\n    assert executor_context is not None\n    assert executor_context.run_id == \"executor-test-run\"\n</code></pre> <p>Step 2: Run test to verify it fails</p> <p>Run: <code>uv run pytest tests/runnable/test_context_isolation.py::test_executor_uses_correct_context -v</code> Expected: FAIL</p> <p>Step 3: Update executor.py</p> <p>Replace: <pre><code>    @property\n    def _context(self):\n        return context.run_context\n</code></pre></p> <p>With: <pre><code>    @property\n    def _context(self):\n        current_context = context.get_run_context()\n        if current_context is None:\n            raise RuntimeError(\"No run context available in current execution context\")\n        return current_context\n</code></pre></p> <p>Step 4: Run executor test</p> <p>Run: <code>uv run pytest tests/runnable/test_context_isolation.py::test_executor_uses_correct_context -v</code> Expected: PASS</p> <p>Step 5: Commit executor.py updates</p> <pre><code>git add runnable/executor.py\ngit commit -m \"feat: update executor.py to use contextvars run_context\"\n</code></pre>"},{"location":"plans/2026-01-02-contextvars-run-context/#task_5_update_datastorepy_to_use_get_run_context","title":"Task 5: Update datastore.py to use get_run_context()","text":"<p>Files: - Modify: <code>runnable/datastore.py:101,108,112-132,405,571</code> (all run_context references)</p> <p>Step 1: Write test for datastore context usage</p> <p>Add to <code>tests/runnable/test_context_isolation.py</code>:</p> <pre><code>def test_datastore_uses_correct_context():\n    \"\"\"Test that datastore components access the correct context.\"\"\"\n    from runnable.datastore import PickledParameter\n    from runnable.context import PipelineContext, set_run_context\n\n    # Create and set context\n    context = PipelineContext(\n        pipeline_definition_file=\"test.py\",\n        run_id=\"datastore-test-run\",\n        catalog={\"type\": \"file-system\"},\n        secrets={\"type\": \"env-secrets\"},\n        pickler={\"type\": \"pickle\"},\n        run_log_store={\"type\": \"memory\"},\n        pipeline_executor={\"type\": \"local\"}\n    )\n\n    # Create pickled parameter\n    param = PickledParameter(value=\"test_param\")\n\n    # Should access correct context for serialization settings\n    assert param.description.startswith(\"Pickled object\")\n</code></pre> <p>Step 2: Run test to verify it fails</p> <p>Run: <code>uv run pytest tests/runnable/test_context_isolation.py::test_datastore_uses_correct_context -v</code> Expected: FAIL</p> <p>Step 3: Update datastore.py context access patterns</p> <p>Update all <code>context.run_context</code> references to use <code>context.get_run_context()</code>:</p> <pre><code>    @property\n    def description(self) -&gt; str:\n        current_context = context.get_run_context()\n        if current_context and current_context.object_serialisation:\n            return f\"Pickled object stored in catalog as: {self.value}\"\n        return f\"Object parameter: {self.value}\"\n\n    @property\n    def file_name(self) -&gt; str:\n        current_context = context.get_run_context()\n        if current_context is None:\n            raise RuntimeError(\"No run context available\")\n        return f\"{self.value}{current_context.pickler.extension}\"\n\n    def get_value(self) -&gt; Any:\n        current_context = context.get_run_context()\n        if current_context is None:\n            raise RuntimeError(\"No run context available\")\n\n        # If there was no serialisation, return the object from the return objects\n        if not current_context.object_serialisation:\n            return current_context.return_objects[self.value]\n\n        # If the object was serialised, get it from the catalog\n        catalog_handler = current_context.catalog\n        catalog_handler.get(name=self.file_name)\n        obj = current_context.pickler.load(path=self.file_name)\n        os.remove(self.file_name)  # Remove after loading\n        return obj\n\n    def put_object(self, data: Any) -&gt; None:\n        current_context = context.get_run_context()\n        if current_context is None:\n            raise RuntimeError(\"No run context available\")\n\n        if not current_context.object_serialisation:\n            current_context.return_objects[self.value] = data\n            return\n\n        # If the object was serialised, put it in the catalog\n        current_context.pickler.dump(data=data, path=self.file_name)\n\n        catalog_handler = current_context.catalog\n        catalog_handler.put(name=self.file_name)\n        os.remove(self.file_name)  # Remove after loading\n</code></pre> <p>Step 4: Update other datastore context references</p> <p>Find and update remaining references around lines 405 and 571:</p> <pre><code>        summary: Dict[str, Any] = {}\n\n        current_context = context.get_run_context()\n        if current_context is None:\n            raise RuntimeError(\"No run context available\")\n\n        summary[\"Unique execution id\"] = self.run_id\n</code></pre> <p>And:</p> <pre><code>    @property\n    def _context(self):\n        current_context = context.get_run_context()\n        if current_context is None:\n            raise RuntimeError(\"No run context available\")\n        return current_context\n</code></pre> <p>Step 5: Run datastore test</p> <p>Run: <code>uv run pytest tests/runnable/test_context_isolation.py::test_datastore_uses_correct_context -v</code> Expected: PASS</p> <p>Step 6: Commit datastore.py updates</p> <pre><code>git add runnable/datastore.py\ngit commit -m \"feat: update datastore.py to use contextvars run_context\"\n</code></pre>"},{"location":"plans/2026-01-02-contextvars-run-context/#task_6_update_remaining_files","title":"Task 6: Update Remaining Files","text":"<p>Files: - Modify: All files found in Grep search for <code>run_context</code></p> <p>Step 1: Find all remaining run_context references</p> <p>Run: <code>grep -r \"context.run_context\" runnable/ extensions/ --exclude-dir=__pycache__ | grep -v \".pyc\"</code></p> <p>Step 2: Update each file systematically</p> <p>For each file found, replace <code>context.run_context</code> with: <pre><code>current_context = context.get_run_context()\nif current_context is None:\n    raise RuntimeError(\"No run context available\")\n# Then use current_context instead of context.run_context\n</code></pre></p> <p>Step 3: Update import statements where needed</p> <p>Add imports for the new functions: <pre><code>from runnable.context import get_run_context\n</code></pre></p> <p>Step 4: Run targeted tests for each updated file</p> <p>Run: <code>uv run pytest tests/runnable/ -v -k \"test_[filename]\"</code></p> <p>Step 5: Commit each file update</p> <pre><code>git add [filename]\ngit commit -m \"feat: update [filename] to use contextvars run_context\"\n</code></pre>"},{"location":"plans/2026-01-02-contextvars-run-context/#task_7_run_full_async_context_test","title":"Task 7: Run Full Async Context Test","text":"<p>Files: - Test: <code>tests/runnable/test_context_isolation.py</code></p> <p>Step 1: Run the async isolation test</p> <p>Run: <code>uv run pytest tests/runnable/test_context_isolation.py::test_context_isolation_async -v</code> Expected: PASS - multiple async tasks should maintain separate contexts</p> <p>Step 2: Run all context isolation tests</p> <p>Run: <code>uv run pytest tests/runnable/test_context_isolation.py -v</code> Expected: All tests PASS</p> <p>Step 3: Commit final test verification</p> <pre><code>git add tests/runnable/test_context_isolation.py\ngit commit -m \"test: verify context isolation works correctly\"\n</code></pre>"},{"location":"plans/2026-01-02-contextvars-run-context/#task_8_integration_testing","title":"Task 8: Integration Testing","text":"<p>Files: - Test: Run existing test suite to ensure no regressions</p> <p>Step 1: Run core runnable tests</p> <p>Run: <code>uv run pytest tests/runnable/ -v</code> Expected: All existing tests still PASS</p> <p>Step 2: Run task-specific tests</p> <p>Run: <code>uv run pytest tests/runnable/test_tasks.py -v</code> Expected: All task tests PASS with new context system</p> <p>Step 3: Run executor tests</p> <p>Run: <code>uv run pytest tests/runnable/test_executor.py -v</code> Expected: All executor tests PASS</p> <p>Step 4: Run integration tests</p> <p>Run: <code>uv run pytest tests/test_pipeline_examples.py -v -k \"python_tasks\"</code> Expected: Pipeline execution works with contextvars</p> <p>Step 5: Final commit</p> <pre><code>git add .\ngit commit -m \"feat: complete contextvars implementation for run_context\n\n- Replace global run_context with contextvars for thread/async safety\n- Fix concurrency issues when multiple pipelines run simultaneously\n- Maintain backward compatibility where possible\n- Add comprehensive context isolation tests\"\n</code></pre>"},{"location":"plans/2026-01-02-contextvars-run-context/#task_9_documentation_update","title":"Task 9: Documentation Update","text":"<p>Files: - Create: <code>docs/architecture/context-isolation.md</code></p> <p>Step 1: Document the context isolation architecture</p> <pre><code># Context Isolation in Runnable\n\n## Problem Solved\n\nPreviously, `run_context` was a global variable that caused issues when multiple pipelines ran concurrently (e.g., in FastAPI endpoints). All pipelines would share the same context, leading to:\n\n- Data leakage between pipelines\n- Incorrect run IDs in logs\n- Configuration mix-ups\n- Resource conflicts\n\n## Solution\n\nReplaced the global variable with Python's `contextvars` module, providing:\n\n- **Request isolation**: Each execution context maintains its own run context\n- **Async safety**: Contexts automatically propagate through async/await chains\n- **Thread safety**: Works correctly with thread pools and concurrent execution\n- **Explicit error handling**: Clear errors when no context is available\n\n## Usage\n\n```python\nfrom runnable.context import get_run_context, set_run_context\n\n# Get current context (returns None if no context)\ncurrent_context = get_run_context()\n\n# Set context (automatically isolated per request/task)\nset_run_context(my_context)\n\n# Context automatically propagates through async chains\nasync def my_async_function():\n    context = get_run_context()  # Same context as caller\n    await some_other_async_function()\n</code></pre>"},{"location":"plans/2026-01-02-contextvars-run-context/#migration_notes","title":"Migration Notes","text":"<ul> <li>Old global <code>context.run_context</code> still works but is deprecated</li> <li>New code should use <code>get_run_context()</code> instead</li> <li>Error handling is now explicit - functions raise <code>RuntimeError</code> if no context available</li> <li>No changes needed for FastAPI or async usage - isolation happens automatically <pre><code>**Step 2: Commit documentation**\n\n```bash\ngit add docs/architecture/context-isolation.md\ngit commit -m \"docs: add context isolation architecture documentation\"\n</code></pre></li> </ul>"},{"location":"plans/2026-01-02-contextvars-run-context/#verification_checklist","title":"Verification Checklist","text":"<ul> <li> All tests pass: <code>uv run pytest</code></li> <li> Context isolation works in async scenarios</li> <li> Multiple concurrent pipeline executions don't interfere</li> <li> Existing functionality preserved</li> <li> Error handling is clear and explicit</li> <li> Documentation explains the architecture</li> </ul>"},{"location":"plans/2026-01-02-contextvars-run-context/#key_files_modified","title":"Key Files Modified","text":"<ul> <li><code>runnable/context.py</code> - Core contextvars implementation</li> <li><code>runnable/tasks.py</code> - Task context access</li> <li><code>runnable/executor.py</code> - Executor context access</li> <li><code>runnable/datastore.py</code> - Datastore context access</li> <li>All other files with <code>context.run_context</code> references</li> <li><code>tests/runnable/test_context_isolation.py</code> - New isolation tests</li> </ul>"},{"location":"plans/2026-01-12-loop-node-design/","title":"Loop Node Design","text":""},{"location":"plans/2026-01-12-loop-node-design/#overview","title":"Overview","text":"<p>Add a loop node to runnable's composite node capabilities. The loop node iterates over a branch until a break condition is met or max iterations is reached.</p>"},{"location":"plans/2026-01-12-loop-node-design/#requirements","title":"Requirements","text":"<ul> <li>Similar to composite nodes (parallel, map, conditional)</li> <li>Iterate until:</li> <li><code>max_iterations</code> is reached, OR</li> <li>A boolean parameter (<code>break_on</code>) evaluates to <code>True</code></li> <li>Do-while style: branch executes at least once, condition checked after each iteration</li> <li>Expose iteration index as a plain environment variable</li> <li>Parameters update in place (last iteration wins)</li> <li>Full lineage tracking via separate branch logs per iteration</li> <li>Compatible with Argo workflows via recursive template pattern</li> </ul>"},{"location":"plans/2026-01-12-loop-node-design/#node_definition","title":"Node Definition","text":"<pre><code>class LoopNode(CompositeNode):\n    node_type: str = Field(default=\"loop\", serialization_alias=\"type\")\n\n    # The sub-graph to execute repeatedly\n    branch: Graph\n\n    # Maximum iterations (safety limit)\n    max_iterations: int\n\n    # Boolean parameter name - when True, loop exits\n    break_on: str\n\n    # Environment variable name for iteration index (no prefix)\n    index_as: str\n</code></pre>"},{"location":"plans/2026-01-12-loop-node-design/#sdk_interface","title":"SDK Interface","text":"<pre><code>from runnable import Pipeline, PythonTask, Loop\n\nprocess = PythonTask(\n    name=\"process\",\n    function=process_func,\n    returns=[pickled(\"result\"), json(\"should_stop\")]\n)\n\n# Branch must be a Pipeline (consistent with other composite nodes)\nprocess_pipeline = Pipeline(steps=[process])\n\nloop = Loop(\n    name=\"retry_loop\",\n    branch=process_pipeline,\n    max_iterations=5,\n    break_on=\"should_stop\",\n    index_as=\"attempt_num\"  # env var: attempt_num=0,1,2...\n)\n\npipeline = Pipeline(steps=[loop, ...])\n</code></pre>"},{"location":"plans/2026-01-12-loop-node-design/#termination_logic","title":"Termination Logic","text":"<p>The loop terminates when EITHER condition is met:</p> <ol> <li><code>parameters[break_on] == True</code></li> <li><code>current_iteration &gt;= max_iterations</code></li> </ol>"},{"location":"plans/2026-01-12-loop-node-design/#naming_convention","title":"Naming Convention","text":"<p>Following the existing dot-path convention:</p> Node Type Pattern Example Parallel <code>node.branch_name.step</code> <code>parallel_node.branch_a.task1</code> Map <code>node.iter_value.step</code> <code>map_node.chunk1.task1</code> Loop <code>node.iteration_index.step</code> <code>loop_node.0.task1</code> <p>For a loop node named <code>retry_loop</code> with a task <code>process_data</code>:</p> <ul> <li>Iteration 0: <code>retry_loop.0.process_data</code></li> <li>Iteration 1: <code>retry_loop.1.process_data</code></li> <li>Iteration 2: <code>retry_loop.2.process_data</code></li> </ul> <p>Branch log names:</p> <ul> <li><code>retry_loop.0</code></li> <li><code>retry_loop.1</code></li> <li><code>retry_loop.2</code></li> </ul>"},{"location":"plans/2026-01-12-loop-node-design/#parameter_flow","title":"Parameter Flow","text":"<ol> <li> <p>fan_out: Copy parent parameters to branch scope for iteration 0</p> </li> <li> <p>Each iteration:</p> </li> <li>Branch reads/writes parameters in branch scope</li> <li>Parameters persist in run_log_store between iterations</li> <li>Each iteration sees parameters left by previous iteration</li> <li> <p>On continue: copy params from <code>node.N</code> scope to <code>node.N+1</code> scope</p> </li> <li> <p>fan_in (final): Copy branch parameters back to parent scope</p> </li> </ol>"},{"location":"plans/2026-01-12-loop-node-design/#method_responsibilities","title":"Method Responsibilities","text":""},{"location":"plans/2026-01-12-loop-node-design/#fan_out","title":"fan_out()","text":"<ul> <li>Create branch log for iteration 0: <code>loop_node.0</code></li> <li>Copy parent parameters to iteration 0 branch scope</li> <li>Called once at the start</li> </ul>"},{"location":"plans/2026-01-12-loop-node-design/#execute_as_graph","title":"execute_as_graph()","text":"<p>Local execution loop:</p> <pre><code>def execute_as_graph(self, iter_variable=None):\n    self.fan_out(iter_variable)\n\n    iteration = 0\n    while True:\n        # Set iteration index env var\n        os.environ[self.index_as] = str(iteration)\n\n        # Execute branch\n        self._context.pipeline_executor.execute_graph(\n            self.branch,\n            iter_variable=build_iter_variable(iter_variable, iteration)\n        )\n\n        # Check termination\n        should_exit = self.fan_in(iter_variable, iteration)\n\n        if should_exit:\n            break\n\n        iteration += 1\n        # Create next branch log, copy params\n        self._create_next_iteration(iteration, iter_variable)\n</code></pre>"},{"location":"plans/2026-01-12-loop-node-design/#fan_in","title":"fan_in()","text":"<ul> <li>Check <code>parameters[break_on] == True</code></li> <li>Check <code>iteration &gt;= max_iterations - 1</code> (0-indexed)</li> <li>Compute <code>should_exit = break_condition OR max_reached</code></li> <li>If <code>should_exit</code>:</li> <li>Set step_log status based on branch status</li> <li>Roll back parameters to parent scope</li> <li>Return <code>should_exit</code> flag</li> <li>For Argo: write <code>should_exit</code> to <code>/tmp/output.txt</code></li> </ul>"},{"location":"plans/2026-01-12-loop-node-design/#iterableparametermodel_integration","title":"IterableParameterModel Integration","text":"<p>Use the existing <code>loop_variable</code> field in <code>IterableParameterModel</code>:</p> <pre><code>class IterableParameterModel(BaseModel):\n    map_variable: OrderedDict[str, MapVariableModel] | None\n    loop_variable: list[LoopIndexModel] | None  # Use this for loop iterations\n</code></pre> <p>Build iter_variable for nested loops:</p> <pre><code>def build_iter_variable(parent_iter_variable, iteration):\n    iter_var = parent_iter_variable.model_copy(deep=True) if parent_iter_variable else IterableParameterModel()\n    iter_var.loop_variable = iter_var.loop_variable or []\n    iter_var.loop_variable.append(LoopIndexModel(value=iteration))\n    return iter_var\n</code></pre>"},{"location":"plans/2026-01-12-loop-node-design/#argo_implementation","title":"Argo Implementation","text":""},{"location":"plans/2026-01-12-loop-node-design/#template_structure","title":"Template Structure","text":"<pre><code>loop-node-template:\n  tasks:\n    - fan-out (initialize)\n    - loop-body (depends: fan-out, inputs: loop_index=0)\n\nloop-body-template (inputs: loop_index):\n  tasks:\n    - branch (execute sub-graph, env: INDEX_AS=loop_index)\n    - fan-in (depends: branch)\n        \u2192 outputs: should_exit\n    - recurse (depends: fan-in, when: should_exit != \"true\")\n        \u2192 calls loop-body-template with loop_index + 1\n</code></pre>"},{"location":"plans/2026-01-12-loop-node-design/#fan_out_container","title":"fan_out Container","text":"<ul> <li>Initializes branch log for iteration 0</li> <li>Copies parent parameters to branch scope</li> </ul>"},{"location":"plans/2026-01-12-loop-node-design/#fan_in_container","title":"fan_in Container","text":"<ul> <li>Reads <code>break_on</code> parameter from run_log_store</li> <li>Checks if <code>loop_index &gt;= max_iterations - 1</code></li> <li>Writes <code>should_exit</code> (\"true\" or \"false\") to <code>/tmp/output.txt</code></li> <li>If exiting: rolls back parameters to parent scope</li> </ul>"},{"location":"plans/2026-01-12-loop-node-design/#recurse_task","title":"Recurse Task","text":"<ul> <li>Conditional on <code>should_exit != \"true\"</code></li> <li>Calls <code>loop-body-template</code> with <code>loop_index + 1</code></li> <li>Argo handles the recursion natively</li> </ul>"},{"location":"plans/2026-01-12-loop-node-design/#file_changes_required","title":"File Changes Required","text":""},{"location":"plans/2026-01-12-loop-node-design/#new_files","title":"New Files","text":"<ul> <li><code>extensions/nodes/loop.py</code> - LoopNode implementation</li> </ul>"},{"location":"plans/2026-01-12-loop-node-design/#modified_files","title":"Modified Files","text":"<ul> <li><code>runnable/sdk.py</code> - Add <code>Loop</code> class for SDK interface</li> <li><code>pyproject.toml</code> - Register loop node entry point</li> <li><code>extensions/pipeline_executor/argo.py</code> - Add loop node handling in <code>_gather_tasks_for_dag_template</code></li> <li><code>runnable/context.py</code> - Add fan command support for loop node (if needed)</li> </ul>"},{"location":"plans/2026-01-12-loop-node-design/#testing_strategy","title":"Testing Strategy","text":"<ol> <li>Unit tests (<code>tests/extensions/nodes/test_loop.py</code>):</li> <li>Test termination on break condition</li> <li>Test termination on max_iterations</li> <li>Test parameter flow between iterations</li> <li> <p>Test nested loops</p> </li> <li> <p>Integration tests:</p> </li> <li>Local execution with break condition</li> <li>Local execution hitting max_iterations</li> <li> <p>Loop with multi-step branch</p> </li> <li> <p>Argo tests:</p> </li> <li>Verify generated YAML structure</li> <li>Test recursive template generation</li> </ol>"},{"location":"plans/2026-01-12-loop-node-design/#example_use_cases","title":"Example Use Cases","text":""},{"location":"plans/2026-01-12-loop-node-design/#retry_pattern","title":"Retry Pattern","text":"<pre><code>def attempt_operation(attempt_num):\n    # attempt_num available as env var\n    result = call_external_api()\n    return {\"success\": result.ok, \"should_stop\": result.ok}\n\ntask = PythonTask(\n    name=\"call_api\",\n    function=attempt_operation,\n    returns=[json(\"success\"), json(\"should_stop\")]\n)\n\nretry_pipeline = Pipeline(steps=[task])\n\nretry_loop = Loop(\n    name=\"retry\",\n    branch=retry_pipeline,\n    max_iterations=3,\n    break_on=\"should_stop\",\n    index_as=\"attempt_num\"\n)\n</code></pre>"},{"location":"plans/2026-01-12-loop-node-design/#convergence_loop","title":"Convergence Loop","text":"<pre><code>def train_epoch(epoch):\n    # epoch available as env var\n    loss = train_model()\n    return {\"loss\": loss, \"converged\": loss &lt; 0.01}\n\ntraining = PythonTask(\n    name=\"train\",\n    function=train_epoch,\n    returns=[json(\"loss\"), json(\"converged\")]\n)\n\ntraining_pipeline = Pipeline(steps=[training])\n\ntraining_loop = Loop(\n    name=\"training\",\n    branch=training_pipeline,\n    max_iterations=100,\n    break_on=\"converged\",\n    index_as=\"epoch\"\n)\n</code></pre>"},{"location":"plans/2026-01-13-loop-node-local-implementation/","title":"Loop Node Local Implementation Plan","text":"<p>For Claude: REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.</p> <p>Goal: Implement loop node for local execution only, with proper placeholder resolution following existing map patterns.</p> <p>Architecture: Follow existing composite node patterns, refactor placeholder resolution to support both map and loop variables.</p> <p>Tech Stack: Python, Pydantic (local execution only, no Argo)</p>"},{"location":"plans/2026-01-13-loop-node-local-implementation/#task_1_add_loop_placeholder_constant","title":"Task 1: Add Loop Placeholder Constant","text":"<p>Files: - Modify: <code>runnable/defaults.py</code> - Test: <code>tests/runnable/test_defaults.py</code></p> <p>Step 1: Write the failing test</p> <pre><code>def test_loop_placeholder_constant():\n    \"\"\"Test LOOP_PLACEHOLDER constant exists.\"\"\"\n    from runnable.defaults import LOOP_PLACEHOLDER\n\n    assert LOOP_PLACEHOLDER == \"loop_variable_placeholder\"\n    assert isinstance(LOOP_PLACEHOLDER, str)\n</code></pre> <p>Step 2: Run test to verify it fails</p> <p>Run: <code>uv run pytest tests/runnable/test_defaults.py::test_loop_placeholder_constant -v</code> Expected: FAIL with \"cannot import name 'LOOP_PLACEHOLDER'\"</p> <p>Step 3: Write minimal implementation</p> <p>Add to <code>runnable/defaults.py</code>:</p> <pre><code># Add after MAP_PLACEHOLDER\nLOOP_PLACEHOLDER = \"loop_variable_placeholder\"\n</code></pre> <p>Step 4: Run test to verify it passes</p> <p>Run: <code>uv run pytest tests/runnable/test_defaults.py::test_loop_placeholder_constant -v</code> Expected: PASS</p> <p>Step 5: Commit</p> <pre><code>git add runnable/defaults.py tests/runnable/test_defaults.py\ngit commit -m \"feat: add LOOP_PLACEHOLDER constant\n\nAdd loop placeholder constant following MAP_PLACEHOLDER pattern\nfor iteration index replacement in loop node names\"\n</code></pre>"},{"location":"plans/2026-01-13-loop-node-local-implementation/#task_2_refactor_placeholder_resolution","title":"Task 2: Refactor Placeholder Resolution","text":"<p>Files: - Modify: <code>runnable/nodes.py</code> - Test: <code>tests/runnable/test_nodes.py</code></p> <p>Step 1: Write the failing test</p> <pre><code>def test_resolve_iter_placeholders_map_only():\n    \"\"\"Test resolving map placeholders works as before.\"\"\"\n    from runnable.nodes import BaseNode\n    from runnable.defaults import IterableParameterModel, MapVariableModel, MAP_PLACEHOLDER\n    from collections import OrderedDict\n\n    iter_var = IterableParameterModel()\n    iter_var.map_variable = OrderedDict({\n        \"chunk\": MapVariableModel(value=\"item_a\")\n    })\n\n    name = f\"step.{MAP_PLACEHOLDER}.task\"\n    result = BaseNode._resolve_iter_placeholders(name, iter_var)\n\n    assert result == \"step.item_a.task\"\n\n\ndef test_resolve_iter_placeholders_loop_only():\n    \"\"\"Test resolving loop placeholders.\"\"\"\n    from runnable.nodes import BaseNode\n    from runnable.defaults import IterableParameterModel, LoopIndexModel, LOOP_PLACEHOLDER\n\n    iter_var = IterableParameterModel()\n    iter_var.loop_variable = [LoopIndexModel(value=2)]\n\n    name = f\"loop.{LOOP_PLACEHOLDER}.task\"\n    result = BaseNode._resolve_iter_placeholders(name, iter_var)\n\n    assert result == \"loop.2.task\"\n\n\ndef test_resolve_iter_placeholders_nested():\n    \"\"\"Test resolving nested map and loop placeholders.\"\"\"\n    from runnable.nodes import BaseNode\n    from runnable.defaults import (\n        IterableParameterModel, MapVariableModel, LoopIndexModel,\n        MAP_PLACEHOLDER, LOOP_PLACEHOLDER\n    )\n    from collections import OrderedDict\n\n    iter_var = IterableParameterModel()\n    iter_var.map_variable = OrderedDict({\n        \"chunk\": MapVariableModel(value=\"item_a\")\n    })\n    iter_var.loop_variable = [\n        LoopIndexModel(value=1),  # outer loop\n        LoopIndexModel(value=3)   # inner loop\n    ]\n\n    name = f\"map.{MAP_PLACEHOLDER}.loop.{LOOP_PLACEHOLDER}.inner_loop.{LOOP_PLACEHOLDER}.task\"\n    result = BaseNode._resolve_iter_placeholders(name, iter_var)\n\n    assert result == \"map.item_a.loop.1.inner_loop.3.task\"\n</code></pre> <p>Step 2: Run test to verify it fails</p> <p>Run: <code>uv run pytest tests/runnable/test_nodes.py::test_resolve_iter_placeholders_map_only -v</code> Expected: FAIL - method doesn't exist</p> <p>Step 3: Write minimal implementation</p> <p>Replace <code>_resolve_map_placeholders</code> method in <code>runnable/nodes.py</code>:</p> <pre><code>@classmethod\ndef _resolve_iter_placeholders(\n    cls,\n    name: str,\n    iter_variable: Optional[IterableParameterModel] = None,\n) -&gt; str:\n    \"\"\"\n    Resolve iteration placeholders (map and loop) in node names.\n\n    Replaces MAP_PLACEHOLDER with map variable values and LOOP_PLACEHOLDER\n    with loop iteration indices in order.\n\n    Args:\n        name: The name containing placeholders\n        iter_variable: Iteration variables (map and loop)\n\n    Returns:\n        str: Name with placeholders resolved\n    \"\"\"\n    if not iter_variable:\n        return name\n\n    resolved_name = name\n\n    # Resolve map placeholders\n    if iter_variable.map_variable:\n        for _, value in iter_variable.map_variable.items():\n            resolved_name = resolved_name.replace(\n                defaults.MAP_PLACEHOLDER, str(value.value), 1\n            )\n\n    # Resolve loop placeholders\n    if iter_variable.loop_variable:\n        for loop_index in iter_variable.loop_variable:\n            resolved_name = resolved_name.replace(\n                defaults.LOOP_PLACEHOLDER, str(loop_index.value), 1\n            )\n\n    return resolved_name\n\n# Keep old method for backward compatibility\n@classmethod\ndef _resolve_map_placeholders(\n    cls,\n    name: str,\n    iter_variable: Optional[IterableParameterModel] = None,\n) -&gt; str:\n    \"\"\"Deprecated: Use _resolve_iter_placeholders instead.\"\"\"\n    return cls._resolve_iter_placeholders(name, iter_variable)\n</code></pre> <p>Step 4: Run test to verify it passes</p> <p>Run: <code>uv run pytest tests/runnable/test_nodes.py -k \"test_resolve_iter_placeholders\" -v</code> Expected: PASS</p> <p>Step 5: Commit</p> <pre><code>git add runnable/nodes.py tests/runnable/test_nodes.py\ngit commit -m \"feat: refactor placeholder resolution for map and loop\n\n- Rename _resolve_map_placeholders to _resolve_iter_placeholders\n- Add support for LOOP_PLACEHOLDER resolution\n- Handle nested map and loop placeholders correctly\n- Keep old method for backward compatibility\"\n</code></pre>"},{"location":"plans/2026-01-13-loop-node-local-implementation/#task_3_core_loopnode_class","title":"Task 3: Core LoopNode Class","text":"<p>Files: - Create: <code>extensions/nodes/loop.py</code> - Test: <code>tests/extensions/nodes/test_loop.py</code></p> <p>Step 1: Write the failing test</p> <pre><code>import pytest\nfrom extensions.nodes.loop import LoopNode\nfrom runnable.graph import Graph, create_graph\n\n\ndef test_loop_node_creation():\n    \"\"\"Test basic LoopNode creation and attributes.\"\"\"\n    branch_config = {\n        \"start_at\": \"dummy_step\",\n        \"steps\": {\n            \"dummy_step\": {\n                \"type\": \"success\"\n            }\n        }\n    }\n    branch = create_graph(branch_config, internal_branch_name=\"test.branch\")\n\n    loop = LoopNode(\n        name=\"test_loop\",\n        branch=branch,\n        max_iterations=5,\n        break_on=\"should_stop\",\n        index_as=\"iteration\"\n    )\n\n    assert loop.name == \"test_loop\"\n    assert loop.node_type == \"loop\"\n    assert loop.max_iterations == 5\n    assert loop.break_on == \"should_stop\"\n    assert loop.index_as == \"iteration\"\n    assert loop.branch == branch\n\n\ndef test_loop_node_branch_name_generation():\n    \"\"\"Test loop node generates correct branch names using placeholders.\"\"\"\n    from runnable.defaults import IterableParameterModel, LoopIndexModel, LOOP_PLACEHOLDER\n\n    loop = LoopNode(\n        name=\"test_loop\",\n        branch=Graph(),  # minimal mock\n        max_iterations=3,\n        break_on=\"done\",\n        index_as=\"idx\"\n    )\n    loop.internal_name = \"test_loop\"\n\n    # Should use LOOP_PLACEHOLDER in branch name template\n    base_template = f\"test_loop.{LOOP_PLACEHOLDER}\"\n\n    # Mock iter_variable for iteration 2\n    iter_var = IterableParameterModel()\n    iter_var.loop_variable = [LoopIndexModel(value=2)]\n\n    result = loop._get_iteration_branch_name(iter_var)\n    assert result == \"test_loop.2\"\n</code></pre> <p>Step 2: Run test to verify it fails</p> <p>Run: <code>uv run pytest tests/extensions/nodes/test_loop.py::test_loop_node_creation -v</code> Expected: FAIL with \"No module named 'extensions.nodes.loop'\"</p> <p>Step 3: Write minimal implementation</p> <pre><code>import logging\nimport os\nfrom typing import Any, Optional\n\nfrom pydantic import Field, field_validator\n\nfrom runnable import defaults\nfrom runnable.defaults import IterableParameterModel, LoopIndexModel, LOOP_PLACEHOLDER\nfrom runnable.graph import Graph\nfrom runnable.nodes import CompositeNode\n\nlogger = logging.getLogger(defaults.LOGGER_NAME)\n\n\nclass LoopNode(CompositeNode):\n    \"\"\"\n    A loop node that iterates over a branch until a break condition is met.\n\n    The branch executes repeatedly until either:\n    - parameters[break_on] == True\n    - max_iterations is reached\n\n    Each iteration gets its own branch log using LOOP_PLACEHOLDER pattern.\n    \"\"\"\n\n    node_type: str = Field(default=\"loop\", serialization_alias=\"type\")\n\n    # The sub-graph to execute repeatedly\n    branch: Graph\n\n    # Maximum iterations (safety limit)\n    max_iterations: int\n\n    # Boolean parameter name - when True, loop exits\n    break_on: str\n\n    # Environment variable name for iteration index (no prefix)\n    index_as: str\n\n    @field_validator(\"break_on\", mode=\"after\")\n    @classmethod\n    def check_break_on(cls, break_on: str) -&gt; str:\n        \"\"\"Validate that the break_on parameter name is alphanumeric.\"\"\"\n        if not break_on.isalnum():\n            raise ValueError(f\"Parameter '{break_on}' must be alphanumeric.\")\n        return break_on\n\n    @field_validator(\"index_as\", mode=\"after\")\n    @classmethod\n    def check_index_as(cls, index_as: str) -&gt; str:\n        \"\"\"Validate that the index_as variable name is alphanumeric.\"\"\"\n        if not index_as.isalnum():\n            raise ValueError(f\"Variable '{index_as}' must be alphanumeric.\")\n        return index_as\n\n    def get_summary(self) -&gt; dict[str, Any]:\n        summary = {\n            \"name\": self.name,\n            \"type\": self.node_type,\n            \"branch\": self.branch.get_summary(),\n            \"max_iterations\": self.max_iterations,\n            \"break_on\": self.break_on,\n            \"index_as\": self.index_as,\n        }\n        return summary\n\n    def _get_iteration_branch_name(\n        self,\n        iter_variable: Optional[IterableParameterModel] = None\n    ) -&gt; str:\n        \"\"\"Get branch name for current iteration using placeholder resolution.\"\"\"\n        # Create branch name template with loop placeholder\n        branch_template = f\"{self.internal_name}.{LOOP_PLACEHOLDER}\"\n\n        # Resolve using the refactored method\n        return self._resolve_iter_placeholders(branch_template, iter_variable)\n\n    def fan_out(self, iter_variable: Optional[IterableParameterModel] = None):\n        \"\"\"Create branch log and set up parameters - implementation in next task.\"\"\"\n        pass\n\n    def execute_as_graph(self, iter_variable: Optional[IterableParameterModel] = None):\n        \"\"\"Execute the loop locally - implementation in next task.\"\"\"\n        pass\n\n    def fan_in(\n        self,\n        iter_variable: Optional[IterableParameterModel] = None\n    ) -&gt; bool:\n        \"\"\"Check conditions and return should_exit flag - implementation in next task.\"\"\"\n        return True\n</code></pre> <p>Step 4: Run test to verify it passes</p> <p>Run: <code>uv run pytest tests/extensions/nodes/test_loop.py::test_loop_node_creation -v</code> Expected: PASS</p> <p>Step 5: Commit</p> <pre><code>git add extensions/nodes/loop.py tests/extensions/nodes/test_loop.py\ngit commit -m \"feat: add basic LoopNode class with placeholder support\n\n- Create LoopNode class following composite node patterns\n- Use LOOP_PLACEHOLDER for branch name generation\n- Add validation for break_on and index_as parameters\n- Implement _get_iteration_branch_name using refactored placeholder resolution\"\n</code></pre>"},{"location":"plans/2026-01-13-loop-node-local-implementation/#task_4_loop_node_parameter_and_branch_management","title":"Task 4: Loop Node Parameter and Branch Management","text":"<p>Files: - Modify: <code>extensions/nodes/loop.py</code> - Test: <code>tests/extensions/nodes/test_loop.py</code></p> <p>Step 1: Write the failing test</p> <pre><code>def test_get_break_condition_value():\n    \"\"\"Test reading break condition from parameters.\"\"\"\n    from runnable.datastore import Parameter\n    from runnable.defaults import IterableParameterModel, LoopIndexModel\n    from unittest.mock import Mock, patch\n\n    # Mock context and run_log_store\n    mock_context = Mock()\n    mock_run_log_store = Mock()\n    mock_context.run_log_store = mock_run_log_store\n    mock_context.run_id = \"test-run-123\"\n\n    # Set up parameters\n    parameters = {\n        \"should_stop\": Parameter.create_unknown(\"should_stop\", False, \"json\")\n    }\n    mock_run_log_store.get_parameters.return_value = parameters\n\n    loop = LoopNode(\n        name=\"test_loop\",\n        branch=Mock(),\n        max_iterations=5,\n        break_on=\"should_stop\",\n        index_as=\"iteration\"\n    )\n    loop._context = mock_context\n    loop.internal_name = \"test_loop\"\n\n    # Create iter_variable for iteration 1\n    iter_var = IterableParameterModel()\n    iter_var.loop_variable = [LoopIndexModel(value=1)]\n\n    result = loop.get_break_condition_value(iter_var)\n    assert result is False\n    mock_run_log_store.get_parameters.assert_called_with(\n        run_id=\"test-run-123\",\n        internal_branch_name=\"test_loop.1\"\n    )\n\n\ndef test_create_iteration_branch_log():\n    \"\"\"Test creating branch logs with proper iteration naming.\"\"\"\n    from runnable.defaults import IterableParameterModel, LoopIndexModel\n    from unittest.mock import Mock\n\n    mock_context = Mock()\n    mock_run_log_store = Mock()\n    mock_context.run_log_store = mock_run_log_store\n    mock_context.run_id = \"test-run-456\"\n\n    # Mock branch log creation\n    mock_branch_log = Mock()\n    mock_run_log_store.create_branch_log.return_value = mock_branch_log\n\n    loop = LoopNode(\n        name=\"test_loop\",\n        branch=Mock(),\n        max_iterations=3,\n        break_on=\"done\",\n        index_as=\"idx\"\n    )\n    loop._context = mock_context\n    loop.internal_name = \"test_loop\"\n\n    # Create iter_variable for iteration 1\n    iter_var = IterableParameterModel()\n    iter_var.loop_variable = [LoopIndexModel(value=1)]\n\n    loop._create_iteration_branch_log(iter_var)\n\n    # Should create branch log with resolved name\n    expected_name = \"test_loop.1\"\n    mock_run_log_store.create_branch_log.assert_called_with(expected_name)\n    mock_run_log_store.add_branch_log.assert_called_with(mock_branch_log, \"test-run-456\")\n</code></pre> <p>Step 2: Run test to verify it fails</p> <p>Run: <code>uv run pytest tests/extensions/nodes/test_loop.py::test_get_break_condition_value -v</code> Expected: FAIL - methods don't exist</p> <p>Step 3: Write minimal implementation</p> <p>Add to <code>LoopNode</code> class in <code>extensions/nodes/loop.py</code>:</p> <pre><code>from runnable.datastore import Parameter\n\ndef get_break_condition_value(\n    self,\n    iter_variable: Optional[IterableParameterModel] = None\n) -&gt; bool:\n    \"\"\"Get the break condition parameter value from current iteration branch.\"\"\"\n    # Get parameters from current iteration branch scope\n    current_branch_name = self._get_iteration_branch_name(iter_variable)\n\n    parameters: dict[str, Parameter] = self._context.run_log_store.get_parameters(\n        run_id=self._context.run_id,\n        internal_branch_name=current_branch_name\n    )\n\n    if self.break_on not in parameters:\n        return False  # Default to continue if parameter doesn't exist\n\n    condition_value = parameters[self.break_on].get_value()\n\n    if not isinstance(condition_value, bool):\n        raise ValueError(\n            f\"Break condition '{self.break_on}' must be boolean, \"\n            f\"got {type(condition_value).__name__}\"\n        )\n\n    return condition_value\n\ndef _create_iteration_branch_log(\n    self,\n    iter_variable: Optional[IterableParameterModel] = None\n):\n    \"\"\"Create branch log for the current iteration.\"\"\"\n    branch_name = self._get_iteration_branch_name(iter_variable)\n\n    try:\n        branch_log = self._context.run_log_store.get_branch_log(\n            branch_name, self._context.run_id\n        )\n        logger.debug(f\"Branch log already exists for {branch_name}\")\n    except Exception:  # BranchLogNotFoundError\n        branch_log = self._context.run_log_store.create_branch_log(branch_name)\n        logger.debug(f\"Branch log created for {branch_name}\")\n\n    branch_log.status = defaults.PROCESSING\n    self._context.run_log_store.add_branch_log(branch_log, self._context.run_id)\n    return branch_log\n\ndef _build_iteration_iter_variable(\n    self,\n    parent_iter_variable: Optional[IterableParameterModel],\n    iteration: int\n) -&gt; IterableParameterModel:\n    \"\"\"Build iter_variable for current iteration.\"\"\"\n    if parent_iter_variable:\n        iter_var = parent_iter_variable.model_copy(deep=True)\n    else:\n        iter_var = IterableParameterModel()\n\n    # Initialize loop_variable if None\n    if iter_var.loop_variable is None:\n        iter_var.loop_variable = []\n\n    # Add current iteration index\n    iter_var.loop_variable.append(LoopIndexModel(value=iteration))\n\n    return iter_var\n</code></pre> <p>Step 4: Run test to verify it passes</p> <p>Run: <code>uv run pytest tests/extensions/nodes/test_loop.py -k \"test_get_break_condition_value or test_create_iteration_branch_log\" -v</code> Expected: PASS</p> <p>Step 5: Commit</p> <pre><code>git add extensions/nodes/loop.py tests/extensions/nodes/test_loop.py\ngit commit -m \"feat: add loop node parameter and branch log management\n\n- Add get_break_condition_value() method with proper validation\n- Add _create_iteration_branch_log() using placeholder resolution\n- Add _build_iteration_iter_variable() for iteration context\n- Proper error handling for missing/invalid parameters\"\n</code></pre>"},{"location":"plans/2026-01-13-loop-node-local-implementation/#task_5_fan-out_implementation","title":"Task 5: Fan-out Implementation","text":"<p>Files: - Modify: <code>extensions/nodes/loop.py</code> - Test: <code>tests/extensions/nodes/test_loop.py</code></p> <p>Step 1: Write the failing test</p> <pre><code>def test_fan_out_initial_iteration():\n    \"\"\"Test fan_out creates branch log and copies parent parameters.\"\"\"\n    from unittest.mock import Mock, patch\n\n    mock_context = Mock()\n    mock_run_log_store = Mock()\n    mock_context.run_log_store = mock_run_log_store\n    mock_context.run_id = \"test-run-123\"\n\n    # Mock parent parameters\n    parent_params = {\"param1\": Mock()}\n    mock_run_log_store.get_parameters.return_value = parent_params\n\n    loop = LoopNode(\n        name=\"test_loop\",\n        branch=Mock(),\n        max_iterations=5,\n        break_on=\"should_stop\",\n        index_as=\"iteration\"\n    )\n    loop._context = mock_context\n    loop.internal_name = \"test_loop\"\n    loop.internal_branch_name = \"root\"\n\n    # Create iter_variable for iteration 0\n    from runnable.defaults import IterableParameterModel, LoopIndexModel\n    iter_var = IterableParameterModel()\n    iter_var.loop_variable = [LoopIndexModel(value=0)]\n\n    with patch.object(loop, '_create_iteration_branch_log') as mock_create_branch:\n        loop.fan_out(iter_var)\n\n    # Should create branch log\n    mock_create_branch.assert_called_once_with(iter_var)\n\n    # Should copy parent parameters to iteration branch\n    mock_run_log_store.set_parameters.assert_called_once_with(\n        parameters=parent_params,\n        run_id=\"test-run-123\",\n        internal_branch_name=\"test_loop.0\"\n    )\n\n\ndef test_fan_out_subsequent_iteration():\n    \"\"\"Test fan_out copies from previous iteration.\"\"\"\n    from unittest.mock import Mock, patch\n    from runnable.defaults import IterableParameterModel, LoopIndexModel\n\n    mock_context = Mock()\n    mock_run_log_store = Mock()\n    mock_context.run_log_store = mock_run_log_store\n    mock_context.run_id = \"test-run-123\"\n\n    # Mock previous iteration parameters\n    prev_params = {\"param1\": Mock(), \"result\": Mock()}\n    mock_run_log_store.get_parameters.return_value = prev_params\n\n    loop = LoopNode(\n        name=\"test_loop\",\n        branch=Mock(),\n        max_iterations=5,\n        break_on=\"should_stop\",\n        index_as=\"iteration\"\n    )\n    loop._context = mock_context\n    loop.internal_name = \"test_loop\"\n\n    # Create iter_variable for iteration 2\n    iter_var = IterableParameterModel()\n    iter_var.loop_variable = [LoopIndexModel(value=2)]\n\n    with patch.object(loop, '_create_iteration_branch_log') as mock_create_branch:\n        loop.fan_out(iter_var)\n\n    # Should get parameters from iteration 1\n    prev_iter_var = IterableParameterModel()\n    prev_iter_var.loop_variable = [LoopIndexModel(value=1)]\n    expected_prev_name = \"test_loop.1\"\n\n    mock_run_log_store.get_parameters.assert_called_with(\n        run_id=\"test-run-123\",\n        internal_branch_name=expected_prev_name\n    )\n\n    # Should copy to iteration 2 branch\n    mock_run_log_store.set_parameters.assert_called_once_with(\n        parameters=prev_params,\n        run_id=\"test-run-123\",\n        internal_branch_name=\"test_loop.2\"\n    )\n</code></pre> <p>Step 2: Run test to verify it fails</p> <p>Run: <code>uv run pytest tests/extensions/nodes/test_loop.py::test_fan_out_initial_iteration -v</code> Expected: FAIL - fan_out not implemented</p> <p>Step 3: Write minimal implementation</p> <p>Replace the <code>fan_out</code> method in <code>extensions/nodes/loop.py</code>:</p> <pre><code>def fan_out(\n    self,\n    iter_variable: Optional[IterableParameterModel] = None\n):\n    \"\"\"\n    Create branch log for current iteration and copy parameters.\n\n    For iteration 0: copy from parent scope\n    For iteration N: copy from previous iteration (N-1) scope\n    \"\"\"\n    # Create branch log for current iteration\n    self._create_iteration_branch_log(iter_variable)\n\n    # Determine current iteration from iter_variable\n    current_iteration = 0\n    if iter_variable and iter_variable.loop_variable:\n        current_iteration = iter_variable.loop_variable[-1].value\n\n    # Determine source of parameters\n    if current_iteration == 0:\n        # Copy from parent scope\n        source_branch_name = self.internal_branch_name\n    else:\n        # Copy from previous iteration\n        prev_iter_var = iter_variable.model_copy(deep=True) if iter_variable else IterableParameterModel()\n        if prev_iter_var.loop_variable is None:\n            prev_iter_var.loop_variable = []\n        # Replace last loop index with previous iteration\n        prev_iter_var.loop_variable[-1] = LoopIndexModel(value=current_iteration - 1)\n        source_branch_name = self._get_iteration_branch_name(prev_iter_var)\n\n    # Get source parameters\n    source_params = self._context.run_log_store.get_parameters(\n        run_id=self._context.run_id,\n        internal_branch_name=source_branch_name\n    )\n\n    # Copy to current iteration branch\n    target_branch_name = self._get_iteration_branch_name(iter_variable)\n    self._context.run_log_store.set_parameters(\n        parameters=source_params,\n        run_id=self._context.run_id,\n        internal_branch_name=target_branch_name\n    )\n</code></pre> <p>Step 4: Run test to verify it passes</p> <p>Run: <code>uv run pytest tests/extensions/nodes/test_loop.py -k \"test_fan_out\" -v</code> Expected: PASS</p> <p>Step 5: Commit</p> <pre><code>git add extensions/nodes/loop.py tests/extensions/nodes/test_loop.py\ngit commit -m \"feat: implement LoopNode fan_out method\n\n- Create branch logs for current iteration\n- Copy parameters from parent (iteration 0) or previous iteration\n- Use proper placeholder resolution for branch names\n- Support both initial and subsequent iterations\"\n</code></pre>"},{"location":"plans/2026-01-13-loop-node-local-implementation/#task_6_fan-in_implementation","title":"Task 6: Fan-in Implementation","text":"<p>Files: - Modify: <code>extensions/nodes/loop.py</code> - Test: <code>tests/extensions/nodes/test_loop.py</code></p> <p>Step 1: Write the failing test</p> <pre><code>def test_fan_in_should_continue():\n    \"\"\"Test fan_in returns False when break condition not met.\"\"\"\n    from unittest.mock import Mock\n    from runnable.datastore import Parameter\n    from runnable.defaults import IterableParameterModel, LoopIndexModel\n\n    mock_context = Mock()\n    mock_run_log_store = Mock()\n    mock_context.run_log_store = mock_run_log_store\n    mock_context.run_id = \"test-run-123\"\n\n    # Break condition is False, should continue\n    parameters = {\n        \"should_stop\": Parameter.create_unknown(\"should_stop\", False, \"json\")\n    }\n    mock_run_log_store.get_parameters.return_value = parameters\n\n    loop = LoopNode(\n        name=\"test_loop\",\n        branch=Mock(),\n        max_iterations=5,\n        break_on=\"should_stop\",\n        index_as=\"iteration\"\n    )\n    loop._context = mock_context\n\n    # Iteration 1 (0-indexed), not at max yet\n    iter_var = IterableParameterModel()\n    iter_var.loop_variable = [LoopIndexModel(value=1)]\n\n    should_exit = loop.fan_in(iter_var)\n\n    assert should_exit is False  # Should continue looping\n\n\ndef test_fan_in_should_exit_break_condition():\n    \"\"\"Test fan_in returns True when break condition is met.\"\"\"\n    from unittest.mock import Mock, patch\n    from runnable.datastore import Parameter\n    from runnable.defaults import IterableParameterModel, LoopIndexModel\n\n    mock_context = Mock()\n    mock_run_log_store = Mock()\n    mock_context.run_log_store = mock_run_log_store\n    mock_context.run_id = \"test-run-123\"\n\n    # Break condition is True, should exit\n    parameters = {\n        \"should_stop\": Parameter.create_unknown(\"should_stop\", True, \"json\")\n    }\n    mock_run_log_store.get_parameters.return_value = parameters\n\n    loop = LoopNode(\n        name=\"test_loop\",\n        branch=Mock(),\n        max_iterations=5,\n        break_on=\"should_stop\",\n        index_as=\"iteration\"\n    )\n    loop._context = mock_context\n\n    iter_var = IterableParameterModel()\n    iter_var.loop_variable = [LoopIndexModel(value=2)]\n\n    with patch.object(loop, '_rollback_parameters_to_parent') as mock_rollback, \\\n         patch.object(loop, '_set_final_step_status') as mock_set_status:\n\n        should_exit = loop.fan_in(iter_var)\n\n        assert should_exit is True\n        mock_rollback.assert_called_once_with(iter_var)\n        mock_set_status.assert_called_once_with(iter_var)\n\n\ndef test_fan_in_should_exit_max_iterations():\n    \"\"\"Test fan_in returns True when max iterations reached.\"\"\"\n    from unittest.mock import Mock, patch\n    from runnable.datastore import Parameter\n    from runnable.defaults import IterableParameterModel, LoopIndexModel\n\n    mock_context = Mock()\n    mock_run_log_store = Mock()\n    mock_context.run_log_store = mock_run_log_store\n    mock_context.run_id = \"test-run-123\"\n\n    # Break condition is False but max iterations reached\n    parameters = {\n        \"should_stop\": Parameter.create_unknown(\"should_stop\", False, \"json\")\n    }\n    mock_run_log_store.get_parameters.return_value = parameters\n\n    loop = LoopNode(\n        name=\"test_loop\",\n        branch=Mock(),\n        max_iterations=3,  # 0, 1, 2 (3 iterations)\n        break_on=\"should_stop\",\n        index_as=\"iteration\"\n    )\n    loop._context = mock_context\n\n    # Iteration 2 (0-indexed) = 3rd iteration = max reached\n    iter_var = IterableParameterModel()\n    iter_var.loop_variable = [LoopIndexModel(value=2)]\n\n    with patch.object(loop, '_rollback_parameters_to_parent'), \\\n         patch.object(loop, '_set_final_step_status'):\n\n        should_exit = loop.fan_in(iter_var)\n\n        assert should_exit is True  # Should exit due to max iterations\n</code></pre> <p>Step 2: Run test to verify it fails</p> <p>Run: <code>uv run pytest tests/extensions/nodes/test_loop.py -k \"test_fan_in_should\" -v</code> Expected: FAIL - fan_in not implemented properly</p> <p>Step 3: Write minimal implementation</p> <p>Replace the <code>fan_in</code> method in <code>extensions/nodes/loop.py</code>:</p> <pre><code>def fan_in(\n    self,\n    iter_variable: Optional[IterableParameterModel] = None\n) -&gt; bool:\n    \"\"\"\n    Check termination conditions and handle loop completion.\n\n    Returns:\n        bool: True if loop should exit, False if should continue\n    \"\"\"\n    # Get current iteration from iter_variable\n    current_iteration = 0\n    if iter_variable and iter_variable.loop_variable:\n        current_iteration = iter_variable.loop_variable[-1].value\n\n    # Check break condition\n    break_condition_met = False\n    try:\n        break_condition_met = self.get_break_condition_value(iter_variable)\n    except (KeyError, ValueError):\n        # If break parameter doesn't exist or invalid, continue\n        break_condition_met = False\n\n    # Check max iterations (0-indexed, so iteration N means N+1 total iterations)\n    max_iterations_reached = current_iteration &gt;= (self.max_iterations - 1)\n\n    should_exit = break_condition_met or max_iterations_reached\n\n    if should_exit:\n        # Roll back parameters to parent and set status on exit\n        self._rollback_parameters_to_parent(iter_variable)\n        self._set_final_step_status(iter_variable)\n\n    return should_exit\n\ndef _rollback_parameters_to_parent(\n    self,\n    iter_variable: Optional[IterableParameterModel] = None\n):\n    \"\"\"Copy parameters from current iteration back to parent scope.\"\"\"\n    current_branch_name = self._get_iteration_branch_name(iter_variable)\n\n    current_params = self._context.run_log_store.get_parameters(\n        run_id=self._context.run_id,\n        internal_branch_name=current_branch_name\n    )\n\n    # Copy back to parent\n    self._context.run_log_store.set_parameters(\n        parameters=current_params,\n        run_id=self._context.run_id,\n        internal_branch_name=self.internal_branch_name\n    )\n\ndef _set_final_step_status(\n    self,\n    iter_variable: Optional[IterableParameterModel] = None\n):\n    \"\"\"Set the loop node's final status based on branch execution.\"\"\"\n    effective_internal_name = self._resolve_iter_placeholders(\n        self.internal_name, iter_variable=iter_variable\n    )\n\n    step_log = self._context.run_log_store.get_step_log(\n        effective_internal_name, self._context.run_id\n    )\n\n    # Check current iteration branch status\n    current_branch_name = self._get_iteration_branch_name(iter_variable)\n    try:\n        current_branch_log = self._context.run_log_store.get_branch_log(\n            current_branch_name, self._context.run_id\n        )\n\n        if current_branch_log.status == defaults.SUCCESS:\n            step_log.status = defaults.SUCCESS\n        else:\n            step_log.status = defaults.FAIL\n\n    except Exception:\n        # If branch log not found, mark as failed\n        step_log.status = defaults.FAIL\n\n    self._context.run_log_store.add_step_log(step_log, self._context.run_id)\n</code></pre> <p>Step 4: Run test to verify it passes</p> <p>Run: <code>uv run pytest tests/extensions/nodes/test_loop.py -k \"test_fan_in\" -v</code> Expected: PASS</p> <p>Step 5: Commit</p> <pre><code>git add extensions/nodes/loop.py tests/extensions/nodes/test_loop.py\ngit commit -m \"feat: implement LoopNode fan_in method\n\n- Check break condition and max iterations correctly\n- Roll back parameters to parent on loop exit\n- Set final step status based on branch execution\n- Extract current iteration from iter_variable properly\"\n</code></pre>"},{"location":"plans/2026-01-13-loop-node-local-implementation/#task_7_local_loop_execution","title":"Task 7: Local Loop Execution","text":"<p>Files: - Modify: <code>extensions/nodes/loop.py</code> - Test: <code>tests/extensions/nodes/test_loop.py</code></p> <p>Step 1: Write the failing test</p> <pre><code>def test_execute_as_graph_single_iteration():\n    \"\"\"Test loop executes once then exits on break condition.\"\"\"\n    from unittest.mock import Mock, patch\n    import os\n\n    mock_context = Mock()\n    mock_pipeline_executor = Mock()\n    mock_context.pipeline_executor = mock_pipeline_executor\n\n    branch = Mock()\n\n    loop = LoopNode(\n        name=\"test_loop\",\n        branch=branch,\n        max_iterations=5,\n        break_on=\"should_stop\",\n        index_as=\"iteration\"\n    )\n    loop._context = mock_context\n\n    with patch.object(loop, 'fan_out') as mock_fan_out, \\\n         patch.object(loop, 'fan_in') as mock_fan_in, \\\n         patch.dict(os.environ, {}, clear=True):\n\n        # First iteration should exit\n        mock_fan_in.return_value = True\n\n        loop.execute_as_graph()\n\n        # Should call fan_out for iteration 0\n        mock_fan_out.assert_called_once()\n\n        # Should execute branch once\n        mock_pipeline_executor.execute_graph.assert_called_once()\n\n        # Should call fan_in for iteration 0\n        mock_fan_in.assert_called_once()\n\n        # Should set environment variable\n        assert os.environ.get(\"iteration\") == \"0\"\n\n\ndef test_execute_as_graph_multiple_iterations():\n    \"\"\"Test loop executes multiple times before break condition.\"\"\"\n    from unittest.mock import Mock, patch, call\n    import os\n\n    mock_context = Mock()\n    mock_pipeline_executor = Mock()\n    mock_context.pipeline_executor = mock_pipeline_executor\n\n    branch = Mock()\n\n    loop = LoopNode(\n        name=\"test_loop\",\n        branch=branch,\n        max_iterations=5,\n        break_on=\"should_stop\",\n        index_as=\"attempt\"\n    )\n    loop._context = mock_context\n\n    with patch.object(loop, 'fan_out') as mock_fan_out, \\\n         patch.object(loop, 'fan_in') as mock_fan_in, \\\n         patch.dict(os.environ, {}, clear=True):\n\n        # Return False twice, then True (3 iterations total)\n        mock_fan_in.side_effect = [False, False, True]\n\n        loop.execute_as_graph()\n\n        # Should call fan_out 3 times\n        assert mock_fan_out.call_count == 3\n\n        # Should execute branch 3 times\n        assert mock_pipeline_executor.execute_graph.call_count == 3\n\n        # Should call fan_in 3 times\n        assert mock_fan_in.call_count == 3\n\n        # Final env var should be \"2\"\n        assert os.environ.get(\"attempt\") == \"2\"\n</code></pre> <p>Step 2: Run test to verify it fails</p> <p>Run: <code>uv run pytest tests/extensions/nodes/test_loop.py::test_execute_as_graph_single_iteration -v</code> Expected: FAIL - execute_as_graph not implemented</p> <p>Step 3: Write minimal implementation</p> <p>Replace the <code>execute_as_graph</code> method in <code>extensions/nodes/loop.py</code>:</p> <pre><code>def execute_as_graph(\n    self,\n    iter_variable: Optional[IterableParameterModel] = None\n):\n    \"\"\"Execute the loop locally until break condition or max iterations.\"\"\"\n    iteration = 0\n\n    while True:\n        # Set iteration index environment variable\n        os.environ[self.index_as] = str(iteration)\n\n        # Build iter_variable for current iteration\n        current_iter_variable = self._build_iteration_iter_variable(\n            iter_variable, iteration\n        )\n\n        # Set up branch log and parameters for this iteration\n        self.fan_out(current_iter_variable)\n\n        # Execute the branch\n        self._context.pipeline_executor.execute_graph(\n            self.branch,\n            iter_variable=current_iter_variable\n        )\n\n        # Check if we should exit\n        should_exit = self.fan_in(current_iter_variable)\n\n        if should_exit:\n            break\n\n        iteration += 1\n</code></pre> <p>Step 4: Run test to verify it passes</p> <p>Run: <code>uv run pytest tests/extensions/nodes/test_loop.py -k \"test_execute_as_graph\" -v</code> Expected: PASS</p> <p>Step 5: Commit</p> <pre><code>git add extensions/nodes/loop.py tests/extensions/nodes/test_loop.py\ngit commit -m \"feat: implement LoopNode local execution\n\n- Add execute_as_graph method for local loop execution\n- Set iteration index as environment variable\n- Build proper iter_variable with loop index for each iteration\n- Support both single and multiple iteration scenarios\"\n</code></pre>"},{"location":"plans/2026-01-13-loop-node-local-implementation/#task_8_sdk_interface_and_entry_point","title":"Task 8: SDK Interface and Entry Point","text":"<p>Files: - Modify: <code>runnable/sdk.py</code> - Modify: <code>runnable/__init__.py</code> - Modify: <code>pyproject.toml</code> - Test: <code>tests/runnable/test_sdk.py</code></p> <p>Step 1: Write the failing test</p> <p>Add to <code>tests/runnable/test_sdk.py</code>:</p> <pre><code>def test_loop_sdk_interface():\n    \"\"\"Test Loop class creates LoopNode correctly.\"\"\"\n    from runnable import Pipeline, PythonTask, Loop\n    from extensions.nodes.loop import LoopNode\n    from runnable.returns import json\n\n    def dummy_func():\n        return {\"result\": True, \"should_stop\": True}\n\n    task = PythonTask(\n        name=\"dummy_task\",\n        function=dummy_func,\n        returns=[json(\"result\"), json(\"should_stop\")]\n    )\n\n    branch_pipeline = Pipeline(steps=[task])\n\n    loop = Loop(\n        name=\"test_loop\",\n        branch=branch_pipeline,\n        max_iterations=3,\n        break_on=\"should_stop\",\n        index_as=\"iteration_num\"\n    )\n\n    # Should create LoopNode\n    assert isinstance(loop, LoopNode)\n    assert loop.name == \"test_loop\"\n    assert loop.max_iterations == 3\n    assert loop.break_on == \"should_stop\"\n    assert loop.index_as == \"iteration_num\"\n    assert loop.branch == branch_pipeline._get_dag()\n</code></pre> <p>Step 2: Run test to verify it fails</p> <p>Run: <code>uv run pytest tests/runnable/test_sdk.py::test_loop_sdk_interface -v</code> Expected: FAIL with \"cannot import name 'Loop'\"</p> <p>Step 3: Write minimal implementation</p> <p>Add to <code>runnable/sdk.py</code>:</p> <pre><code># Add import at top with other node imports\nfrom extensions.nodes.loop import LoopNode\n\n# Add Loop class after other composite node classes\nclass Loop(LoopNode):\n    \"\"\"\n    SDK interface for creating loop nodes.\n\n    A loop node iterates over a branch pipeline until a break condition\n    is met or max iterations is reached.\n\n    Args:\n        name: Name of the loop node\n        branch: Pipeline to execute repeatedly\n        max_iterations: Maximum number of iterations (safety limit)\n        break_on: Name of boolean parameter that triggers loop exit when True\n        index_as: Name of environment variable for iteration index (0, 1, 2...)\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        branch: \"Pipeline\",\n        max_iterations: int,\n        break_on: str,\n        index_as: str,\n        **kwargs\n    ):\n        # Convert Pipeline to Graph\n        branch_dag = branch._get_dag()\n\n        super().__init__(\n            name=name,\n            branch=branch_dag,\n            max_iterations=max_iterations,\n            break_on=break_on,\n            index_as=index_as,\n            **kwargs\n        )\n</code></pre> <p>Step 4: Update exports</p> <p>Add to <code>runnable/__init__.py</code>:</p> <pre><code>from runnable.sdk import Loop  # Add this import\n</code></pre> <p>Step 5: Register entry point</p> <p>Add to the <code>[project.entry-points.\"nodes\"]</code> section in <code>pyproject.toml</code>:</p> <pre><code>loop = \"extensions.nodes.loop:LoopNode\"\n</code></pre> <p>Step 6: Run test to verify it passes</p> <p>Run: <code>uv run pytest tests/runnable/test_sdk.py::test_loop_sdk_interface -v</code> Expected: PASS</p> <p>Step 7: Commit</p> <pre><code>git add runnable/sdk.py runnable/__init__.py pyproject.toml tests/runnable/test_sdk.py\ngit commit -m \"feat: add Loop SDK interface and entry point\n\n- Add Loop class to sdk.py for user-friendly loop creation\n- Register loop node entry point in pyproject.toml\n- Export Loop from runnable package\n- Add SDK integration test with proper imports\"\n</code></pre>"},{"location":"plans/2026-01-13-loop-node-local-implementation/#task_9_integration_tests_and_examples","title":"Task 9: Integration Tests and Examples","text":"<p>Files: - Create: <code>tests/extensions/nodes/test_loop_integration.py</code> - Create: <code>examples/05-loops/simple_retry_loop.py</code></p> <p>Step 1: Create integration test</p> <pre><code>\"\"\"Integration tests for loop node functionality.\"\"\"\n\nimport pytest\nimport os\nfrom unittest.mock import Mock, patch\n\ndef test_loop_integration_with_parameters():\n    \"\"\"Test loop node with actual parameter flow.\"\"\"\n    from runnable import Pipeline, PythonTask, Loop\n    from runnable.returns import json\n    from runnable.datastore import Parameter\n\n    call_count = 0\n\n    def increment_and_check():\n        nonlocal call_count\n        call_count += 1\n        iteration = int(os.environ.get(\"loop_idx\", \"0\"))\n\n        return {\n            \"count\": call_count,\n            \"iteration\": iteration,\n            \"should_stop\": call_count &gt;= 3\n        }\n\n    task = PythonTask(\n        name=\"counter\",\n        function=increment_and_check,\n        returns=[json(\"count\"), json(\"iteration\"), json(\"should_stop\")]\n    )\n\n    branch = Pipeline(steps=[task])\n\n    loop = Loop(\n        name=\"counter_loop\",\n        branch=branch,\n        max_iterations=10,  # High limit, should exit on condition\n        break_on=\"should_stop\",\n        index_as=\"loop_idx\"\n    )\n\n    # Test the iteration iter_variable building\n    from runnable.defaults import IterableParameterModel, LoopIndexModel\n\n    iter_var = loop._build_iteration_iter_variable(None, 2)\n\n    assert iter_var.loop_variable is not None\n    assert len(iter_var.loop_variable) == 1\n    assert iter_var.loop_variable[0].value == 2\n\n\ndef test_loop_branch_naming():\n    \"\"\"Test branch names are generated correctly.\"\"\"\n    from extensions.nodes.loop import LoopNode\n    from runnable.defaults import IterableParameterModel, LoopIndexModel\n    from runnable.graph import Graph\n\n    loop = LoopNode(\n        name=\"test_loop\",\n        branch=Graph(),\n        max_iterations=3,\n        break_on=\"done\",\n        index_as=\"idx\"\n    )\n    loop.internal_name = \"parent.test_loop\"\n\n    # Test iteration 0\n    iter_var_0 = IterableParameterModel()\n    iter_var_0.loop_variable = [LoopIndexModel(value=0)]\n\n    name_0 = loop._get_iteration_branch_name(iter_var_0)\n    assert name_0 == \"parent.test_loop.0\"\n\n    # Test iteration 5\n    iter_var_5 = IterableParameterModel()\n    iter_var_5.loop_variable = [LoopIndexModel(value=5)]\n\n    name_5 = loop._get_iteration_branch_name(iter_var_5)\n    assert name_5 == \"parent.test_loop.5\"\n\n\ndef test_loop_max_iterations():\n    \"\"\"Test loop respects max_iterations limit.\"\"\"\n    from extensions.nodes.loop import LoopNode\n    from runnable.defaults import IterableParameterModel, LoopIndexModel\n    from runnable.graph import Graph\n    from unittest.mock import Mock\n\n    mock_context = Mock()\n    mock_run_log_store = Mock()\n    mock_context.run_log_store = mock_run_log_store\n    mock_context.run_id = \"test\"\n\n    # Break condition never met\n    from runnable.datastore import Parameter\n    parameters = {\n        \"never_stop\": Parameter.create_unknown(\"never_stop\", False, \"json\")\n    }\n    mock_run_log_store.get_parameters.return_value = parameters\n\n    loop = LoopNode(\n        name=\"limited_loop\",\n        branch=Graph(),\n        max_iterations=2,  # Should stop after 2 iterations (0, 1)\n        break_on=\"never_stop\",\n        index_as=\"iteration\"\n    )\n    loop._context = mock_context\n\n    # Test iteration 1 (should continue)\n    iter_var_1 = IterableParameterModel()\n    iter_var_1.loop_variable = [LoopIndexModel(value=1)]\n\n    should_exit_1 = loop.fan_in(iter_var_1)\n    assert should_exit_1 is True  # max_iterations=2 means 0,1 are allowed; 1 is last\n\n    # Test iteration 0 (should continue)\n    iter_var_0 = IterableParameterModel()\n    iter_var_0.loop_variable = [LoopIndexModel(value=0)]\n\n    with patch.object(loop, '_rollback_parameters_to_parent'), \\\n         patch.object(loop, '_set_final_step_status'):\n        should_exit_0 = loop.fan_in(iter_var_0)\n\n    assert should_exit_0 is False  # Should continue to iteration 1\n</code></pre> <p>Step 2: Create example</p> <pre><code>\"\"\"Simple retry loop example demonstrating loop node usage.\"\"\"\n\nimport os\nimport random\nfrom runnable import Pipeline, PythonTask, Loop\nfrom runnable.returns import json\n\n\ndef unreliable_operation():\n    \"\"\"Simulates an operation that might fail.\"\"\"\n    attempt = int(os.environ.get(\"attempt\", \"0\"))\n    print(f\"Attempt {attempt}: Calling unreliable service...\")\n\n    # Simulate 70% failure rate\n    if random.random() &lt; 0.7:\n        print(f\"Attempt {attempt}: Failed!\")\n        return {\n            \"success\": False,\n            \"should_stop\": False  # Continue trying\n        }\n    else:\n        print(f\"Attempt {attempt}: Success!\")\n        return {\n            \"success\": True,\n            \"should_stop\": True  # Stop on success\n        }\n\n\nif __name__ == \"__main__\":\n    # Create the retry task\n    retry_task = PythonTask(\n        name=\"call_service\",\n        function=unreliable_operation,\n        returns=[json(\"success\"), json(\"should_stop\")]\n    )\n\n    # Wrap in a pipeline (required for loop branches)\n    retry_branch = Pipeline(steps=[retry_task])\n\n    # Create retry loop\n    retry_loop = Loop(\n        name=\"service_retry\",\n        branch=retry_branch,\n        max_iterations=5,  # Try up to 5 times\n        break_on=\"should_stop\",  # Stop when this parameter is True\n        index_as=\"attempt\"  # Available as 'attempt' env var (0, 1, 2...)\n    )\n\n    # Create main pipeline\n    pipeline = Pipeline(steps=[retry_loop])\n\n    print(\"Starting retry loop...\")\n    print(\"This will try up to 5 times or until success\")\n    print()\n\n    # Execute\n    try:\n        result = pipeline.execute()\n        print(f\"\\nPipeline completed successfully!\")\n        print(f\"Final result: {result}\")\n    except Exception as e:\n        print(f\"\\nPipeline failed: {e}\")\n</code></pre> <p>Step 3: Run tests</p> <p>Run: <code>uv run pytest tests/extensions/nodes/test_loop_integration.py -v</code> Expected: PASS</p> <p>Step 4: Test example (optional)</p> <pre><code>mkdir -p examples/05-loops\n# Then run: uv run examples/05-loops/simple_retry_loop.py\n</code></pre> <p>Step 5: Commit</p> <pre><code>git add tests/extensions/nodes/test_loop_integration.py examples/05-loops/simple_retry_loop.py\ngit commit -m \"feat: add loop node integration tests and example\n\n- Integration tests for parameter flow and branch naming\n- Test max_iterations termination correctly\n- Simple retry example demonstrating loop usage\n- Tests cover iter_variable building and placeholder resolution\"\n</code></pre>"},{"location":"plans/2026-01-13-loop-node-local-implementation/#task_10_final_testing","title":"Task 10: Final Testing","text":"<p>Files: - Run comprehensive tests</p> <p>Step 1: Run all loop tests</p> <pre><code>uv run pytest tests/extensions/nodes/test_loop.py -v\nuv run pytest tests/extensions/nodes/test_loop_integration.py -v\nuv run pytest tests/runnable/test_sdk.py -k loop -v\nuv run pytest tests/runnable/test_nodes.py -k \"iter_placeholder\" -v\n</code></pre> <p>Step 2: Run full test suite to ensure no regressions</p> <pre><code>uv run pytest tests/ -x\n</code></pre> <p>Step 3: Fix any failing tests</p> <p>Fix issues as they arise, committing each fix separately.</p> <p>Step 4: Final validation commit</p> <pre><code>git add .\ngit commit -m \"feat: finalize loop node local implementation\n\n- All tests passing\n- Proper placeholder resolution for map and loop variables\n- Local execution fully working\n- Example validated\n- Ready for use in local pipelines\"\n</code></pre> <p>This plan focuses only on local execution and gets the placeholder resolution pattern correct. The key insight is refactoring <code>_resolve_map_placeholders</code> to <code>_resolve_iter_placeholders</code> to handle both map and loop variables properly, following the existing patterns in the codebase.</p>"},{"location":"plans/branch-parameter-rollback/","title":"Branch Parameter Rollback Implementation Plan","text":"<p>For Claude: This work is complete in conditional.py and parallel.py. Only need end-to-end examples and assertions.</p> <p>Goal: Create end-to-end testable pipeline examples demonstrating that parameters set in conditional and parallel branches correctly roll back to parent scope after successful execution.</p> <p>Context: Parameter rollback has been implemented in both ConditionalNode and ParallelNode fan_in() methods. When branches execute successfully, their parameters are merged into the parent scope. This is analogous to the map node's reducer pattern, but without reduction - just a simple merge/overwrite.</p> <p>Architecture: Follow the existing pattern in test_pipeline_examples.py where examples are Python files in examples/ directory with assertions that verify run log state after execution.</p>"},{"location":"plans/branch-parameter-rollback/#critical_files","title":"Critical Files","text":""},{"location":"plans/branch-parameter-rollback/#to_create","title":"To Create:","text":"<ul> <li><code>examples/10-branch-parameters/conditional_rollback.py</code> - Conditional parameter rollback example</li> <li><code>examples/10-branch-parameters/parallel_rollback.py</code> - Parallel parameter rollback example</li> <li><code>examples/common/functions.py</code> - Add helper functions (modify existing file)</li> <li><code>tests/assertions.py</code> - Add root parameter assertion (modify existing file)</li> </ul>"},{"location":"plans/branch-parameter-rollback/#to_modify","title":"To Modify:","text":"<ul> <li><code>tests/test_pipeline_examples.py</code> - Add new examples to python_examples list</li> </ul>"},{"location":"plans/branch-parameter-rollback/#implementation_steps","title":"Implementation Steps","text":""},{"location":"plans/branch-parameter-rollback/#step_1_add_assertion_for_root_parameters","title":"Step 1: Add Assertion for Root Parameters","text":"<p>File: <code>tests/assertions.py</code></p> <p>Add new assertion function to verify parameters exist at root level:</p> <pre><code>def should_have_root_parameters(parameters: dict):\n    \"\"\"Verify parameters exist at run log root level with expected values.\"\"\"\n    from runnable import defaults\n\n    run_id = os.environ[defaults.ENV_RUN_ID]\n    run_log = load_run_log(run_id)\n\n    for param_name, expected_value in parameters.items():\n        assert param_name in run_log.parameters, f\"Parameter {param_name} not found in root parameters\"\n        actual_value = run_log.parameters[param_name].get_value()\n        assert actual_value == expected_value, f\"Expected {param_name}={expected_value}, got {actual_value}\"\n</code></pre>"},{"location":"plans/branch-parameter-rollback/#step_2_add_helper_functions_to_common_functions","title":"Step 2: Add Helper Functions to Common Functions","text":"<p>File: <code>examples/common/functions.py</code></p> <p>Add functions for conditional branches:</p> <pre><code>def set_conditional_heads_param():\n    \"\"\"Set parameter when heads branch executes.\"\"\"\n    return \"heads_value\"\n\n\ndef set_conditional_tails_param():\n    \"\"\"Set parameter when tails branch executes.\"\"\"\n    return \"tails_value\"\n\n\ndef set_conditional_multiple():\n    \"\"\"Set multiple parameters in conditional branch.\"\"\"\n    return \"param1_value\", \"param2_value\", \"param3_value\"\n\n\ndef verify_conditional_rollback(branch_param: str):\n    \"\"\"Verify rolled back parameter from conditional branch.\"\"\"\n    assert branch_param in [\"heads_value\", \"tails_value\"]\n    return branch_param\n</code></pre> <p>Add functions for parallel branches:</p> <pre><code>def set_parallel_branch1():\n    \"\"\"Set parameter in parallel branch 1.\"\"\"\n    return \"branch1_value\"\n\n\ndef set_parallel_branch2():\n    \"\"\"Set parameter in parallel branch 2.\"\"\"\n    return \"branch2_value\"\n\n\ndef set_parallel_branch3():\n    \"\"\"Set parameter in parallel branch 3.\"\"\"\n    return \"branch3_value\"\n\n\ndef verify_parallel_rollback(result1: str, result2: str, result3: str):\n    \"\"\"Verify all parallel branch parameters rolled back.\"\"\"\n    assert result1 == \"branch1_value\"\n    assert result2 == \"branch2_value\"\n    assert result3 == \"branch3_value\"\n    return \"verified\"\n\n\ndef set_shared_param_a():\n    \"\"\"Set shared parameter to value A.\"\"\"\n    return \"value_a\"\n\n\ndef set_shared_param_b():\n    \"\"\"Set shared parameter to value B.\"\"\"\n    return \"value_b\"\n</code></pre>"},{"location":"plans/branch-parameter-rollback/#step_3_create_conditional_parameter_rollback_example","title":"Step 3: Create Conditional Parameter Rollback Example","text":"<p>File: <code>examples/10-branch-parameters/conditional_rollback.py</code></p> <pre><code>\"\"\"\nDemonstrate parameter rollback from conditional branches.\n\nWhen a conditional branch executes and succeeds, parameters set within\nthat branch roll back to the parent scope.\n\nExecute with:\n    python examples/10-branch-parameters/conditional_rollback.py\n\"\"\"\n\nfrom examples.common.functions import (\n    set_conditional_heads_param,\n    set_conditional_tails_param,\n    verify_conditional_rollback,\n)\nfrom runnable import Conditional, Pipeline, PythonTask, Stub\n\n\ndef decide_heads():\n    \"\"\"Return 'heads' to select heads branch.\"\"\"\n    return \"heads\"\n\n\ndef main():\n    # Create branch pipelines that set parameters\n    heads_pipeline = PythonTask(\n        name=\"heads_task\",\n        function=set_conditional_heads_param,\n        returns=[\"branch_param\"],\n    ).as_pipeline()\n\n    tails_pipeline = PythonTask(\n        name=\"tails_task\",\n        function=set_conditional_tails_param,\n        returns=[\"branch_param\"],\n    ).as_pipeline()\n\n    # Conditional node selects branch based on 'choice' parameter\n    conditional = Conditional(\n        name=\"conditional\",\n        branches={\"heads\": heads_pipeline, \"tails\": tails_pipeline},\n        parameter=\"choice\",\n    )\n\n    # Task to set the choice parameter\n    decide_task = PythonTask(\n        name=\"decide\",\n        function=decide_heads,\n        returns=[\"choice\"],\n    )\n\n    # Task to verify the parameter rolled back from branch\n    verify_task = PythonTask(\n        name=\"verify\",\n        function=verify_conditional_rollback,\n        terminate_with_success=True,\n    )\n\n    pipeline = Pipeline(steps=[decide_task, conditional, verify_task])\n    pipeline.execute()\n\n    return pipeline\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"plans/branch-parameter-rollback/#step_4_create_parallel_parameter_rollback_example","title":"Step 4: Create Parallel Parameter Rollback Example","text":"<p>File: <code>examples/10-branch-parameters/parallel_rollback.py</code></p> <pre><code>\"\"\"\nDemonstrate parameter rollback from parallel branches.\n\nWhen parallel branches execute and succeed, parameters from all branches\nroll back to the parent scope.\n\nExecute with:\n    python examples/10-branch-parameters/parallel_rollback.py\n\"\"\"\n\nfrom examples.common.functions import (\n    set_parallel_branch1,\n    set_parallel_branch2,\n    set_parallel_branch3,\n    verify_parallel_rollback,\n)\nfrom runnable import Parallel, Pipeline, PythonTask, Stub\n\n\ndef main():\n    # Create branch pipelines that set different parameters\n    branch1_pipeline = PythonTask(\n        name=\"branch1_task\",\n        function=set_parallel_branch1,\n        returns=[\"result1\"],\n    ).as_pipeline()\n\n    branch2_pipeline = PythonTask(\n        name=\"branch2_task\",\n        function=set_parallel_branch2,\n        returns=[\"result2\"],\n    ).as_pipeline()\n\n    branch3_pipeline = PythonTask(\n        name=\"branch3_task\",\n        function=set_parallel_branch3,\n        returns=[\"result3\"],\n    ).as_pipeline()\n\n    # Parallel node executes all branches\n    parallel = Parallel(\n        name=\"parallel\",\n        branches={\n            \"branch1\": branch1_pipeline,\n            \"branch2\": branch2_pipeline,\n            \"branch3\": branch3_pipeline,\n        },\n    )\n\n    # Task to verify all parameters rolled back from branches\n    verify_task = PythonTask(\n        name=\"verify\",\n        function=verify_parallel_rollback,\n        terminate_with_success=True,\n    )\n\n    pipeline = Pipeline(steps=[parallel, verify_task])\n    pipeline.execute()\n\n    return pipeline\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"plans/branch-parameter-rollback/#step_5_create_parameter_conflict_example_parallel","title":"Step 5: Create Parameter Conflict Example (Parallel)","text":"<p>File: <code>examples/10-branch-parameters/parallel_conflict.py</code></p> <pre><code>\"\"\"\nDemonstrate parameter conflict resolution in parallel branches.\n\nWhen multiple branches set the same parameter, last write wins\nbased on dictionary iteration order.\n\nExecute with:\n    python examples/10-branch-parameters/parallel_conflict.py\n\"\"\"\n\nfrom examples.common.functions import set_shared_param_a, set_shared_param_b\nfrom runnable import Parallel, Pipeline, PythonTask\n\n\ndef main():\n    # Both branches set the same parameter name\n    branch1_pipeline = PythonTask(\n        name=\"branch1_task\",\n        function=set_shared_param_a,\n        returns=[\"shared\"],\n    ).as_pipeline()\n\n    branch2_pipeline = PythonTask(\n        name=\"branch2_task\",\n        function=set_shared_param_b,\n        returns=[\"shared\"],\n    ).as_pipeline()\n\n    parallel = Parallel(\n        name=\"parallel\",\n        branches={\"branch1\": branch1_pipeline, \"branch2\": branch2_pipeline},\n    )\n\n    pipeline = Pipeline(steps=[parallel])\n    pipeline.execute()\n\n    return pipeline\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"plans/branch-parameter-rollback/#step_6_add_examples_to_test_suite","title":"Step 6: Add Examples to Test Suite","text":"<p>File: <code>tests/test_pipeline_examples.py</code></p> <p>Add to <code>python_examples</code> list (after the last entry, before contexts are defined):</p> <pre><code>    (\n        \"10-branch-parameters/conditional_rollback\",\n        True,  # no_yaml\n        False,  # fails\n        [],  # ignore_contexts\n        \"\",  # parameters_file\n        [\n            partial(conditions.should_have_num_steps, 4),\n            partial(conditions.should_be_successful),\n            partial(conditions.should_step_be_successful, \"decide\"),\n            partial(conditions.should_step_be_successful, \"conditional\"),\n            partial(conditions.should_step_be_successful, \"verify\"),\n            partial(conditions.should_branch_have_steps, \"conditional\", \"heads\", 2),\n            partial(conditions.should_branch_be_successful, \"conditional\", \"heads\"),\n            # Verify parameter rolled back to root\n            partial(conditions.should_have_root_parameters, {\"branch_param\": \"heads_value\"}),\n        ],\n    ),\n    (\n        \"10-branch-parameters/parallel_rollback\",\n        True,  # no_yaml\n        False,  # fails\n        [],  # ignore_contexts\n        \"\",  # parameters_file\n        [\n            partial(conditions.should_have_num_steps, 3),\n            partial(conditions.should_be_successful),\n            partial(conditions.should_step_be_successful, \"parallel\"),\n            partial(conditions.should_step_be_successful, \"verify\"),\n            partial(conditions.should_branch_have_steps, \"parallel\", \"branch1\", 2),\n            partial(conditions.should_branch_have_steps, \"parallel\", \"branch2\", 2),\n            partial(conditions.should_branch_have_steps, \"parallel\", \"branch3\", 2),\n            partial(conditions.should_branch_be_successful, \"parallel\", \"branch1\"),\n            partial(conditions.should_branch_be_successful, \"parallel\", \"branch2\"),\n            partial(conditions.should_branch_be_successful, \"parallel\", \"branch3\"),\n            # Verify all parameters rolled back to root\n            partial(\n                conditions.should_have_root_parameters,\n                {\"result1\": \"branch1_value\", \"result2\": \"branch2_value\", \"result3\": \"branch3_value\"},\n            ),\n        ],\n    ),\n    (\n        \"10-branch-parameters/parallel_conflict\",\n        True,  # no_yaml\n        False,  # fails\n        [],  # ignore_contexts\n        \"\",  # parameters_file\n        [\n            partial(conditions.should_have_num_steps, 2),\n            partial(conditions.should_be_successful),\n            partial(conditions.should_step_be_successful, \"parallel\"),\n            partial(conditions.should_branch_have_steps, \"parallel\", \"branch1\", 2),\n            partial(conditions.should_branch_have_steps, \"parallel\", \"branch2\", 2),\n            # Verify one of the values is present (last write wins)\n            # We can't assert which specific value because dict iteration order\n            # Just verify the parameter exists at root\n            partial(conditions.should_have_root_parameters, {\"shared\": \"value_b\"}),\n        ],\n    ),\n</code></pre>"},{"location":"plans/branch-parameter-rollback/#verification","title":"Verification","text":""},{"location":"plans/branch-parameter-rollback/#manual_testing","title":"Manual Testing","text":"<p>Run the examples directly:</p> <pre><code># Test conditional rollback\nuv run python examples/10-branch-parameters/conditional_rollback.py\n\n# Test parallel rollback\nuv run python examples/10-branch-parameters/parallel_rollback.py\n\n# Test parallel conflict\nuv run python examples/10-branch-parameters/parallel_conflict.py\n</code></pre>"},{"location":"plans/branch-parameter-rollback/#automated_testing","title":"Automated Testing","text":"<p>Run the test suite:</p> <pre><code># Run only the new parameter rollback tests\nuv run pytest tests/test_pipeline_examples.py::test_python_examples -k \"branch-parameters\" -v\n\n# Run all example tests\nuv run pytest tests/test_pipeline_examples.py -v\n</code></pre>"},{"location":"plans/branch-parameter-rollback/#expected_outcomes","title":"Expected Outcomes","text":"<ol> <li>Conditional Rollback:</li> <li><code>decide</code> task sets <code>choice=\"heads\"</code></li> <li>Conditional executes <code>heads</code> branch which sets <code>branch_param=\"heads_value\"</code></li> <li>After fan_in, <code>branch_param</code> exists in root parameters</li> <li> <p><code>verify</code> task receives <code>branch_param</code> and succeeds</p> </li> <li> <p>Parallel Rollback:</p> </li> <li>All 3 branches execute in parallel</li> <li>Each sets its own unique parameter (<code>result1</code>, <code>result2</code>, <code>result3</code>)</li> <li>After fan_in, all 3 parameters exist in root</li> <li> <p><code>verify</code> task receives all 3 parameters and succeeds</p> </li> <li> <p>Parallel Conflict:</p> </li> <li>Both branches set <code>shared</code> parameter</li> <li>After fan_in, <code>shared</code> exists with one of the values (dict iteration determines which)</li> <li>Parameter exists at root level</li> </ol>"},{"location":"plans/branch-parameter-rollback/#what_to_look_for","title":"What to Look For","text":"<ul> <li>All tests pass without errors</li> <li>Run logs show parameters at root level after composite nodes</li> <li>Branch parameters are properly isolated during execution</li> <li>Verification tasks can access rolled-back parameters</li> <li>No test failures in other examples (no regressions)</li> </ul>"},{"location":"plans/branch-parameter-rollback/#notes","title":"Notes","text":"<ul> <li>The implementation in <code>conditional.py</code> and <code>parallel.py</code> is already complete</li> <li>Only failure handling differs: conditional has one executed branch, parallel has multiple</li> <li>Both use same pattern: get parent params \u2192 merge branch params \u2192 set back to parent</li> <li>Map node has similar logic but with reducer function - this is just direct merge</li> <li>Examples follow existing patterns from <code>07-map</code> and <code>02-sequential</code> directories</li> <li>Integration tests were removed - examples with assertions are the preferred testing approach</li> </ul>"},{"location":"production/catalog/","title":"Data Catalog Configuration","text":"<p>Catalogs manage data flow between pipeline steps - they store and retrieve data artifacts, ensuring your workflows have access to the data they need.</p>"},{"location":"production/catalog/#why_data_catalogs_matter","title":"Why Data Catalogs Matter","text":"<p>Seamless Data Flow</p> <p>Automatic data management: Focus on your logic, not data plumbing</p> <ul> <li>\ud83d\udcca Cross-step data sharing: Pass data between pipeline steps automatically</li> <li>\ud83d\udcbe Artifact versioning: Each pipeline run gets isolated data storage</li> <li>\ud83d\udd0d Data lineage: Track which data was used by which steps</li> <li>\ud83c\udfaf Type safety: Automatic serialization/deserialization with type hints</li> <li>\u267b\ufe0f Reproducibility: Exact data artifacts preserved for every run</li> </ul>"},{"location":"production/catalog/#available_catalog_stores","title":"Available Catalog Stores","text":"Store Type Environment Best For <code>do-nothing</code> Any Testing without data persistence <code>file-system</code> Any environment with mounted storage Local development and single-machine production <code>s3</code> / <code>minio</code> Object storage Distributed systems and cloud deployments"},{"location":"production/catalog/#do-nothing","title":"do-nothing","text":"<p>No data persistence - useful for testing pipeline logic without data storage overhead.</p> <p>No Data Persistence</p> <ul> <li>Testing only: Data artifacts are not stored or retrieved</li> <li>Pipeline validation: Verify workflow logic without data management</li> <li>Fast execution: No I/O overhead for development iteration</li> </ul>"},{"location":"production/catalog/#configuration","title":"Configuration","text":"<pre><code>catalog:\n  type: do-nothing\n</code></pre>"},{"location":"production/catalog/#file-system","title":"file-system","text":"<p>Stores data artifacts in the local filesystem - reliable and simple for most use cases.</p> <p>Works Everywhere with Mounted Storage</p> <p>Runs in any environment where catalog_location is accessible</p> <ul> <li>\ud83d\udcbe Persistent storage: Data artifacts saved to mounted filesystem</li> <li>\ud83d\udcc1 Organized structure: Each run gets isolated directory by run_id</li> <li>\ud83c\udfe0 Local development: Direct filesystem access</li> <li>\ud83d\udc33 Containers: Works with volume mounts</li> <li>\u2638\ufe0f Kubernetes: Works with persistent volumes</li> </ul>"},{"location":"production/catalog/#configuration_1","title":"Configuration","text":"<pre><code>catalog:\n  type: file-system\n  config:\n    catalog_location: \".catalog\"  # Optional: defaults to \".catalog\"\n</code></pre>"},{"location":"production/catalog/#example","title":"Example","text":"pipeline.pyconfig.yaml <pre><code>from runnable import Pipeline, PythonTask, pickled\nimport pandas as pd\n\ndef load_data():\n    # Load some sample data\n    data = pd.DataFrame({\"x\": [1, 2, 3], \"y\": [4, 5, 6]})\n    return data\n\ndef process_data(raw_data: pd.DataFrame):\n    # Process the data\n    processed = raw_data * 2\n    return processed\n\ndef main():\n    pipeline = Pipeline(steps=[\n        PythonTask(\n            function=load_data,\n            name=\"load\",\n            returns=[pickled(\"raw_data\")]  # Store in catalog\n        ),\n        PythonTask(\n            function=process_data,\n            name=\"process\",\n            returns=[pickled(\"processed_data\")]\n        )\n    ])\n\n    pipeline.execute()\n    return pipeline\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <pre><code>catalog:\n  type: file-system\n  config:\n    catalog_location: \".catalog\"\n</code></pre> <p>Run the example: <pre><code>RUNNABLE_CONFIGURATION_FILE=config.yaml uv run pipeline.py\n</code></pre></p> <p>Result: Data artifacts stored in <code>.catalog/{run_id}/</code> with automatic serialization and data lineage tracking.</p>"},{"location":"production/catalog/#object_storage_s3_minio","title":"Object Storage (s3 / minio)","text":"<p>For distributed systems and cloud deployments, use object storage catalogs:</p> <p>Installation Required</p> <p>S3 storage requires the optional S3 dependency: <pre><code>pip install runnable[s3]\n</code></pre></p>"},{"location":"production/catalog/#s3","title":"s3","text":"<pre><code>catalog:\n  type: s3\n  config:\n    bucket_name: \"my-pipeline-data\"\n    prefix: \"runnable-artifacts\"\n    aws_access_key_id: \"${AWS_ACCESS_KEY_ID}\"\n    aws_secret_access_key: \"${AWS_SECRET_ACCESS_KEY}\"\n    region_name: \"us-west-2\"\n</code></pre>"},{"location":"production/catalog/#minio","title":"minio","text":"<pre><code>catalog:\n  type: minio\n  config:\n    endpoint: \"https://minio.company.com\"\n    access_key: \"${MINIO_ACCESS_KEY}\"\n    secret_key: \"${MINIO_SECRET_KEY}\"\n    bucket_name: \"pipeline-artifacts\"\n</code></pre>"},{"location":"production/catalog/#custom_data_catalogs","title":"Custom Data Catalogs","text":"<p>Need to integrate with your existing data infrastructure? Build custom catalogs that store artifacts in any system using Runnable's plugin architecture.</p> <p>Enterprise Data Integration</p> <p>Connect to your existing data systems: Never be limited by built-in storage options</p> <ul> <li>\ud83c\udfe2 Data warehouses: Store artifacts in Snowflake, BigQuery, Redshift</li> <li>\ud83d\udcca Data lakes: Integrate with Delta Lake, Iceberg, Hudi</li> <li>\ud83d\uddc4\ufe0f Corporate storage: Connect to existing NFS, HDFS, object stores</li> <li>\ud83d\udd10 Governed data: Meet data governance and lineage requirements</li> </ul>"},{"location":"production/catalog/#building_a_custom_catalog","title":"Building a Custom Catalog","text":"<p>Creating a custom catalog takes just 3 steps:</p> <p>Custom Catalog Implementation</p> <p>1. Implement the catalog interface: <pre><code>from runnable.catalog import BaseCatalog\nfrom typing import Any\n\nclass SnowflakeCatalog(BaseCatalog):\n    service_name: str = \"snowflake\"\n\n    # Configuration fields\n    account: str\n    user: str\n    password: str\n    warehouse: str\n    database: str = \"PIPELINE_ARTIFACTS\"\n\n    def put(self, name: str, data: Any, run_id: str) -&gt; str:\n        \"\"\"Store data artifact in Snowflake\"\"\"\n        # Serialize data (pickle, parquet, etc.)\n        serialized_data = self._serialize(data, name)\n\n        # Create table if needed\n        table_name = f\"artifacts_{run_id.replace('-', '_')}\"\n\n        # Store in Snowflake\n        self._execute_sql(f\"\"\"\n            CREATE TABLE IF NOT EXISTS {table_name} (\n                artifact_name STRING,\n                data_blob BINARY,\n                created_at TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()\n            )\n        \"\"\")\n\n        # Insert artifact\n        self._execute_sql(f\"\"\"\n            INSERT INTO {table_name} (artifact_name, data_blob)\n            VALUES ('{name}', '{serialized_data}')\n        \"\"\")\n\n        return f\"{table_name}.{name}\"\n\n    def get(self, name: str, run_id: str, **kwargs) -&gt; Any:\n        \"\"\"Retrieve data artifact from Snowflake\"\"\"\n        table_name = f\"artifacts_{run_id.replace('-', '_')}\"\n\n        result = self._execute_sql(f\"\"\"\n            SELECT data_blob FROM {table_name}\n            WHERE artifact_name = '{name}'\n            LIMIT 1\n        \"\"\")\n\n        return self._deserialize(result[0]['DATA_BLOB'], name)\n\n    def _execute_sql(self, query: str):\n        # Execute SQL using Snowflake connector\n        return self.snowflake_connection.execute(query)\n</code></pre></p> <p>2. Register via entry point in <code>pyproject.toml</code>: <pre><code>[project.entry-points.'catalog']\n\"snowflake\" = \"my_package.catalogs:SnowflakeCatalog\"\n</code></pre></p> <p>3. Use in your configuration: <pre><code>catalog:\n  type: snowflake\n  config:\n    account: \"mycompany.snowflakecomputing.com\"\n    user: \"${SNOWFLAKE_USER}\"\n    password: \"${SNOWFLAKE_PASSWORD}\"\n    warehouse: \"ANALYTICS_WH\"\n    database: \"PIPELINE_DATA\"\n</code></pre></p>"},{"location":"production/catalog/#real-world_custom_catalog_examples","title":"Real-World Custom Catalog Examples","text":"<p>Production Use Cases</p> <p>Data Lake Integration: <pre><code>class DeltaLakeCatalog(BaseCatalog):\n    \"\"\"Store artifacts in Delta Lake with versioning\"\"\"\n    service_name = \"delta-lake\"\n\n    def put(self, name: str, data: Any, run_id: str):\n        # Write to Delta Lake with automatic versioning\n        delta_table = f\"artifacts.{run_id}_{name}\"\n        self._write_delta_table(delta_table, data)\n</code></pre></p> <p>Enterprise Data Warehouse: <pre><code>class BigQueryCatalog(BaseCatalog):\n    \"\"\"Store artifacts in Google BigQuery\"\"\"\n    service_name = \"bigquery\"\n\n    def put(self, name: str, data: Any, run_id: str):\n        # Upload to BigQuery table with metadata\n        table_id = f\"pipeline_artifacts.{run_id}_{name}\"\n        self._upload_to_bq(table_id, data)\n</code></pre></p> <p>High-Performance Storage: <pre><code>class HDFSCatalog(BaseCatalog):\n    \"\"\"Store large artifacts in Hadoop HDFS\"\"\"\n    service_name = \"hdfs\"\n\n    def put(self, name: str, data: Any, run_id: str):\n        # Store in HDFS with compression\n        path = f\"/pipeline_data/{run_id}/{name}.parquet\"\n        self._write_hdfs_parquet(path, data)\n</code></pre></p>"},{"location":"production/catalog/#integration_patterns","title":"Integration Patterns","text":"<p>Common Integration Approaches</p> <p>Database storage: Store artifacts in relational/NoSQL databases <pre><code>def put(self, name: str, data: Any, run_id: str):\n    serialized = pickle.dumps(data)\n    self.db.execute(\n        \"INSERT INTO artifacts (run_id, name, data) VALUES (?, ?, ?)\",\n        (run_id, name, serialized)\n    )\n</code></pre></p> <p>File-based systems: Store in distributed filesystems <pre><code>def put(self, name: str, data: Any, run_id: str):\n    path = f\"{self.base_path}/{run_id}/{name}.pkl\"\n    self.filesystem.write_bytes(path, pickle.dumps(data))\n</code></pre></p> <p>Hybrid storage: Metadata in database, data in object store <pre><code>def put(self, name: str, data: Any, run_id: str):\n    # Store data in S3/GCS\n    data_url = self._upload_to_storage(data, run_id, name)\n\n    # Store metadata in database\n    self._store_metadata(run_id, name, data_url, type(data).__name__)\n</code></pre></p>"},{"location":"production/catalog/#choosing_the_right_catalog","title":"Choosing the Right Catalog","text":"<p>Decision Guide</p> <p>For most users: Use <code>file-system</code> - works in any environment with mounted storage</p> <p>For development/testing: Use <code>do-nothing</code> for fast iteration without data persistence</p> <p>Distributed systems: Use <code>s3</code>/<code>minio</code> when execution environments can't mount shared storage</p> <p>Enterprise integration: Build custom catalogs to integrate with existing data infrastructure</p> <p>Filesystem vs Object Storage</p> <p>Filesystem catalogs (<code>file-system</code>): Work in any execution environment where the <code>catalog_location</code> can be mounted</p> <p>Object storage (<code>s3</code>, <code>minio</code>): Use when shared filesystem mounting isn't available or for cloud-native deployments</p>"},{"location":"production/custom-run-log-stores/","title":"Building Custom Run Log Stores","text":"<p>Store execution metadata and logs in any database or cloud storage system by creating custom run log stores that integrate with Runnable's plugin architecture.</p> <p>Real-World Examples</p> <p>The <code>extensions/run_log_store/</code> directory contains working implementations for file system, MinIO, and chunked storage that demonstrate these patterns in production code.</p> <p>The below is a rough guideline for database and cloud storage integrations.</p>"},{"location":"production/custom-run-log-stores/#run_log_storage_workflow","title":"Run Log Storage Workflow","text":"<p>Custom run log stores handle the persistent storage of execution metadata, providing durability and queryability for pipeline runs:</p>"},{"location":"production/custom-run-log-stores/#core_integration_pattern","title":"Core Integration Pattern","text":"<pre><code>from runnable.datastore import BaseRunLogStore, RunLog, JobLog, StepLog, JsonParameter\nfrom typing import Dict, Any, List, Optional\n\nclass DatabaseRunLogStore(BaseRunLogStore):\n    service_name: str = \"database\"\n\n    def create_run_log(self, run_id: str, **kwargs) -&gt; RunLog:\n        \"\"\"Create a new run log entry - called at pipeline start\"\"\"\n        pass\n\n    def get_run_log_by_id(self, run_id: str, full: bool = True) -&gt; RunLog:\n        \"\"\"Retrieve run log by ID - most frequently called method\"\"\"\n        pass\n\n    def put_run_log(self, run_log: RunLog):\n        \"\"\"Store/update complete run log - called at pipeline completion\"\"\"\n        pass\n\n    def get_run_logs(self, run_ids: List[str] = None, **kwargs) -&gt; List[RunLog]:\n        \"\"\"Query multiple run logs with filters - for analysis and debugging\"\"\"\n        pass\n\n    def set_parameters(self, run_id: str, parameters: Dict[str, JsonParameter]):\n        \"\"\"Store pipeline parameters - called early in execution\"\"\"\n        pass\n\n    def set_run_config(self, run_id: str, run_config: Dict[str, Any]):\n        \"\"\"Store pipeline configuration - called during setup\"\"\"\n        pass\n\n    def create_step_log(self, run_id: str, step_log: StepLog):\n        \"\"\"Create step log entry - called for each pipeline step\"\"\"\n        pass\n\n    def create_job_log(self) -&gt; JobLog:\n        \"\"\"Create job log entry - called for job execution\"\"\"\n        pass\n\n    def add_job_log(self, run_id: str, job_log: JobLog):\n        \"\"\"Add job log to run - called after job completion\"\"\"\n        pass\n</code></pre> <p>The workflow ensures:</p> <ul> <li>Metadata persistence: Execution details survive beyond process lifetime</li> <li>Query capability: Run logs can be searched and analyzed</li> <li>Audit trail: Complete execution history for compliance and debugging</li> <li>Parallel safety: Multiple concurrent executions don't conflict</li> </ul>"},{"location":"production/custom-run-log-stores/#implementation_template","title":"Implementation Template","text":"<p>Here's a stubbed implementation template for integrating with databases or cloud storage:</p> <pre><code>from typing import Dict, Any, List, Optional\nfrom pydantic import Field\nimport json\n\nfrom runnable.datastore import BaseRunLogStore, RunLog, JobLog, StepLog, JsonParameter\nfrom runnable import defaults, exceptions\n\nclass CloudDatabaseRunLogStore(BaseRunLogStore):\n    \"\"\"Store run logs in cloud database or storage systems\"\"\"\n\n    service_name: str = \"cloud-database\"\n\n    # Configuration fields - these map to YAML config\n    connection_string: str = Field(..., description=\"Database connection string or storage endpoint\")\n    table_name: str = Field(default=\"runnable_logs\", description=\"Table/collection name for run logs\")\n    retention_days: int = Field(default=90, description=\"How long to keep run logs\")\n    enable_compression: bool = Field(default=True, description=\"Compress large run logs\")\n\n    def create_run_log(\n        self,\n        run_id: str,\n        dag_hash: str = \"\",\n        tag: str = \"\",\n        status: str = defaults.CREATED\n    ) -&gt; RunLog:\n        \"\"\"Create new run log entry in storage\"\"\"\n\n        # STEP 1: Create RunLog object\n        run_log = RunLog(\n            run_id=run_id,\n            dag_hash=dag_hash,\n            tag=tag,\n            status=status\n        )\n\n        # STEP 2: Store in your database/storage system\n        self._store_run_log_metadata(run_log)\n        # TODO: Insert initial run log record into your database/storage\n\n        return run_log\n\n    def get_run_log_by_id(self, run_id: str, full: bool = True) -&gt; RunLog:\n        \"\"\"Retrieve run log from storage\"\"\"\n\n        # STEP 1: Query your storage system\n        raw_data = self._fetch_run_log_data(run_id)\n        # TODO: Query your database/storage for run_id\n\n        if not raw_data:\n            raise exceptions.RunLogNotFoundError(f\"Run log {run_id} not found\")\n\n        # STEP 2: Convert to RunLog object\n        run_log = self._deserialize_run_log(raw_data, full=full)\n\n        return run_log\n\n    def put_run_log(self, run_log: RunLog):\n        \"\"\"Store/update complete run log\"\"\"\n\n        # STEP 1: Serialize run log data\n        serialized_data = self._serialize_run_log(run_log)\n\n        # STEP 2: Store in your database/storage system\n        self._update_run_log_storage(run_log.run_id, serialized_data)\n        # TODO: Update/insert complete run log in your database/storage\n\n    def get_run_logs(\n        self,\n        run_ids: List[str] = None,\n        tag: str = \"\",\n        status: str = \"\",\n        **kwargs\n    ) -&gt; List[RunLog]:\n        \"\"\"Query multiple run logs with filters\"\"\"\n\n        # STEP 1: Build query based on filters\n        query_conditions = self._build_query_conditions(run_ids, tag, status, **kwargs)\n\n        # STEP 2: Execute query against your storage\n        raw_results = self._query_run_logs(query_conditions)\n        # TODO: Execute filtered query against your database/storage\n\n        # STEP 3: Convert results to RunLog objects\n        run_logs = [self._deserialize_run_log(data) for data in raw_results]\n\n        return run_logs\n\n    def _store_run_log_metadata(self, run_log: RunLog):\n        \"\"\"Store initial run log in your database/storage\"\"\"\n        # TODO: Implement storage-specific logic\n        # Examples:\n        # - SQL: INSERT INTO runnable_logs (run_id, status, created_at) VALUES (...)\n        # - NoSQL: collection.insert_one({\"run_id\": run_log.run_id, ...})\n        # - Cloud Storage: upload_object(f\"runs/{run_log.run_id}/metadata.json\", ...)\n        pass\n\n    def _fetch_run_log_data(self, run_id: str) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Fetch run log data from your storage system\"\"\"\n        # TODO: Implement retrieval logic\n        # Examples:\n        # - SQL: SELECT * FROM runnable_logs WHERE run_id = ?\n        # - NoSQL: collection.find_one({\"run_id\": run_id})\n        # - Cloud Storage: download_object(f\"runs/{run_id}/log.json\")\n\n        return {}  # Replace with actual data\n\n    def _update_run_log_storage(self, run_id: str, data: Dict[str, Any]):\n        \"\"\"Update complete run log in storage\"\"\"\n        # TODO: Implement update/upsert logic\n        # Handle large run logs based on your storage capabilities\n\n        if self.enable_compression and len(json.dumps(data)) &gt; 1000000:  # 1MB threshold\n            data = self._compress_run_log_data(data)  # TODO: Implement compression\n\n        # Store the data in your system\n        pass\n\n    def _serialize_run_log(self, run_log: RunLog) -&gt; Dict[str, Any]:\n        \"\"\"Convert RunLog to storage format\"\"\"\n        # Use RunLog's built-in serialization\n        return run_log.model_dump()\n\n    def _deserialize_run_log(self, data: Dict[str, Any], full: bool = True) -&gt; RunLog:\n        \"\"\"Convert storage data to RunLog object\"\"\"\n        # Handle decompression if needed\n        if self.enable_compression and 'compressed' in data:\n            data = self._decompress_run_log_data(data)  # TODO: Implement decompression\n\n        # Create RunLog from stored data\n        return RunLog(**data)\n\n    def _build_query_conditions(self, run_ids: List[str], tag: str, status: str, **kwargs):\n        \"\"\"Build database query conditions\"\"\"\n        # TODO: Translate filters to your storage query format\n        conditions = {}\n\n        if run_ids:\n            conditions['run_id'] = {'$in': run_ids}  # MongoDB style - adapt to your DB\n        if tag:\n            conditions['tag'] = tag\n        if status:\n            conditions['status'] = status\n\n        return conditions\n\n    def _query_run_logs(self, conditions: Dict[str, Any]) -&gt; List[Dict[str, Any]]:\n        \"\"\"Execute query against your storage system\"\"\"\n        # TODO: Execute filtered query\n        # Examples:\n        # - SQL: SELECT * FROM runnable_logs WHERE conditions\n        # - NoSQL: collection.find(conditions)\n        # - Cloud Storage: list_objects_with_filters(conditions)\n\n        return []  # Replace with actual results\n\n    def get_summary(self) -&gt; Dict[str, Any]:\n        \"\"\"Return storage system summary\"\"\"\n        return {\n            \"Type\": self.service_name,\n            \"Connection\": self.connection_string,  # May want to mask sensitive parts\n            \"Table\": self.table_name,\n            \"Retention\": f\"{self.retention_days} days\",\n            \"Compression\": self.enable_compression\n        }\n</code></pre> <p>Key Integration Points:</p> <ul> <li><code>create_run_log()</code>: Called at pipeline start - store initial metadata</li> <li><code>get_run_log_by_id()</code>: Most frequently called - optimize for fast retrieval</li> <li><code>put_run_log()</code>: Called at pipeline completion - store full execution results</li> <li><code>get_run_logs()</code>: For querying and analysis - support filtering and pagination</li> </ul>"},{"location":"production/custom-run-log-stores/#configuration_plugin_registration","title":"Configuration &amp; Plugin Registration","text":""},{"location":"production/custom-run-log-stores/#yaml_to_pydantic_field_mapping","title":"YAML to Pydantic Field Mapping","text":"<p>Understanding how YAML configuration maps to your run log store class fields is crucial:</p> <p>Your Pydantic Class: <pre><code>class CloudDatabaseRunLogStore(BaseRunLogStore):\n    service_name: str = \"cloud-database\"\n    service_type: str = \"run_log_store\"  # Always set to \"run_log_store\"\n\n    # Required fields (must be provided in YAML)\n    connection_string: str = Field(..., description=\"Database connection string or storage endpoint\")\n    table_name: str = Field(..., description=\"Table/collection name for run logs\")\n\n    # Optional fields with defaults\n    retention_days: int = Field(default=90, description=\"How long to keep run logs\")\n    enable_compression: bool = Field(default=True, description=\"Compress large run logs\")\n    max_connections: int = Field(default=20, description=\"Database connection pool size\")\n    timeout_seconds: int = Field(default=30, description=\"Query timeout\")\n\n    # Thread safety support\n    supports_parallel_writes: bool = Field(default=False, description=\"Enable for parallel execution\")\n</code></pre></p> <p>Maps to YAML Configuration: cloud-database-config.yaml<pre><code>run-log-store:\n  type: cloud-database              # \u2192 matches service_name in your class\n  config:\n    # Required fields\n    connection_string: \"postgresql://user:pass@host:5432/db\"  # \u2192 self.connection_string\n    table_name: \"pipeline_execution_logs\"     # \u2192 self.table_name\n\n    # Optional fields (override defaults)\n    retention_days: 180             # \u2192 self.retention_days (overrides default 90)\n    enable_compression: false       # \u2192 self.enable_compression (overrides default True)\n    max_connections: 50             # \u2192 self.max_connections (overrides default 20)\n    timeout_seconds: 60             # \u2192 self.timeout_seconds (overrides default 30)\n    supports_parallel_writes: true # \u2192 self.supports_parallel_writes (enables parallel execution)\n</code></pre></p> <p>In your code, access config as class attributes: <pre><code>def create_run_log(self, run_id: str, **kwargs) -&gt; RunLog:\n    # Access your configuration fields directly\n    connection = self._get_connection()  # Uses self.connection_string\n\n    query = f\"INSERT INTO {self.table_name} (run_id, status) VALUES (%s, %s)\"\n    #                     \u2191 From YAML config.table_name\n\n    with connection.cursor() as cursor:\n        cursor.execute(query, (run_id, \"CREATED\"))\n\n    return RunLog(run_id=run_id)\n\ndef _get_connection(self):\n    \"\"\"Create database connection using configuration\"\"\"\n    return psycopg2.connect(\n        self.connection_string,     # From YAML config.connection_string\n        connect_timeout=self.timeout_seconds  # From YAML config.timeout_seconds\n    )\n\ndef put_run_log(self, run_log: RunLog):\n    \"\"\"Store run log, optionally compressing large data\"\"\"\n    data = run_log.model_dump()\n\n    if self.enable_compression and len(json.dumps(data)) &gt; 1000000:  # 1MB\n        data = self._compress_data(data)  # Compression enabled via YAML\n\n    # Store in table specified by YAML config\n    query = f\"UPDATE {self.table_name} SET data = %s WHERE run_id = %s\"\n    # ... store data\n</code></pre></p>"},{"location":"production/custom-run-log-stores/#configuration_validation","title":"Configuration Validation","text":"<p>Pydantic automatically validates your config:</p> <ul> <li>Required fields: Pipeline fails with clear error if missing from YAML</li> <li>Type checking: <code>retention_days: \"invalid\"</code> raises validation error before execution</li> <li>Defaults applied: Optional fields use defaults when not specified in YAML</li> <li>Custom validation: Add Pydantic validators for complex field validation</li> </ul> <p>Example validation error: <pre><code>ValidationError: 1 validation error for CloudDatabaseRunLogStore\nconnection_string\n  field required (type=value_error.missing)\n</code></pre></p>"},{"location":"production/custom-run-log-stores/#plugin_registration","title":"Plugin Registration","text":"<p>1. Register via entry point in <code>pyproject.toml</code>: pyproject.toml<pre><code>[project.entry-points.'run_log_store']\n\"cloud-database\" = \"my_package.stores:CloudDatabaseRunLogStore\"\n</code></pre></p> <p>2. Runnable discovers your store automatically</p>"},{"location":"production/custom-run-log-stores/#usage_pattern","title":"Usage Pattern","text":"<p>Pipelines use your run log store transparently: <pre><code>from runnable import Pipeline, PythonTask\nfrom examples.common.functions import hello\n\ndef main():\n    pipeline = Pipeline(steps=[\n        PythonTask(function=hello, name=\"task1\"),\n        PythonTask(function=hello, name=\"task2\")\n    ])\n    pipeline.execute(configuration_file=\"cloud-database-config.yaml\")\n    return pipeline\n\nif __name__ == \"__main__\":\n    main()\n</code></pre></p>"},{"location":"production/custom-run-log-stores/#storage_system_considerations","title":"Storage System Considerations","text":""},{"location":"production/custom-run-log-stores/#database_integration_patterns","title":"Database Integration Patterns","text":"<p>Relational Databases (PostgreSQL, MySQL): - Store run metadata in structured tables - Use JSON columns for flexible step log data - Index on run_id, status, tag, created_at for fast queries - Consider partitioning for high-volume deployments</p> <p>NoSQL Databases (MongoDB, DynamoDB): - Store complete run logs as documents - Use compound indexes for query patterns - Handle large documents with compression or splitting - Leverage native JSON querying capabilities</p> <p>Cloud Storage (S3, Azure Blob, GCS): - Store run logs as individual files - Use object metadata for filtering - Implement listing and querying via object keys - Consider data lakes for analytics integration</p>"},{"location":"production/custom-run-log-stores/#performance_optimization","title":"Performance Optimization","text":"<p>For High-Volume Deployments: <pre><code>class OptimizedRunLogStore(BaseRunLogStore):\n    # Add connection pooling\n    max_connections: int = Field(default=20)\n\n    # Add caching\n    cache_ttl_seconds: int = Field(default=300)\n\n    # Add batching for writes\n    batch_size: int = Field(default=100)\n\n    # Add async operations\n    async_writes: bool = Field(default=True)\n</code></pre></p>"},{"location":"production/custom-run-log-stores/#parallel_execution_support","title":"Parallel Execution Support","text":"<p>Thread-Safe Implementation: <pre><code>class ThreadSafeRunLogStore(BaseRunLogStore):\n    supports_parallel_writes: bool = True  # Enable parallel pipeline execution\n\n    # Use appropriate locking/coordination for your storage system\n    def put_run_log(self, run_log: RunLog):\n        # Implement thread-safe storage updates\n        # Use database transactions, file locking, etc.\n</code></pre></p>"},{"location":"production/custom-run-log-stores/#testing_your_custom_run_log_store","title":"Testing Your Custom Run Log Store","text":""},{"location":"production/custom-run-log-stores/#development_testing","title":"Development Testing","text":"<pre><code>class CloudDatabaseRunLogStore(BaseRunLogStore):\n    mock: bool = Field(default=False, description=\"Enable mock mode for testing\")\n\n    def _store_run_log_metadata(self, run_log: RunLog):\n        if self.mock:\n            # Store in memory or local files for testing\n            self._mock_storage[run_log.run_id] = run_log.model_dump()\n        else:\n            # Real database/storage integration\n            self._execute_database_insert(run_log)\n</code></pre>"},{"location":"production/custom-run-log-stores/#test_configuration","title":"Test Configuration","text":"mock-config.yaml<pre><code>run-log-store:\n  type: cloud-database\n  config:\n    connection_string: \"sqlite:///:memory:\"  # In-memory for testing\n    table_name: \"test_runs\"\n    mock: true  # Skip real database calls\n</code></pre>"},{"location":"production/custom-run-log-stores/#integration_testing","title":"Integration Testing","text":"<p>Test with pipeline execution: <pre><code>from runnable import Pipeline, PythonTask\n\ndef test_function():\n    print(\"Testing custom run log store!\")\n    return {\"test\": \"completed\"}\n\ndef main():\n    pipeline = Pipeline(steps=[\n        PythonTask(function=test_function, name=\"test_task\")\n    ])\n    pipeline.execute(configuration_file=\"mock-config.yaml\")\n    return pipeline\n\nif __name__ == \"__main__\":\n    main()\n</code></pre></p>"},{"location":"production/custom-run-log-stores/#development_workflow","title":"Development Workflow","text":""},{"location":"production/custom-run-log-stores/#1_start_with_stubbed_template","title":"1. Start with Stubbed Template","text":"<ul> <li>Copy the <code>CloudDatabaseRunLogStore</code> template above</li> <li>Replace database-specific fields with your storage system's configuration</li> <li>Keep all TODO comments initially</li> </ul>"},{"location":"production/custom-run-log-stores/#2_test_storage_integration","title":"2. Test Storage Integration","text":"<ul> <li>Enable mock mode to test runnable integration</li> <li>Implement basic create/get/put operations</li> <li>Verify run logs are stored and retrieved correctly</li> </ul>"},{"location":"production/custom-run-log-stores/#3_add_query_and_analytics_features","title":"3. Add Query and Analytics Features","text":"<ul> <li>Implement <code>get_run_logs()</code> with filtering</li> <li>Add indexing for performance</li> <li>Support pagination for large result sets</li> </ul>"},{"location":"production/custom-run-log-stores/#4_production_hardening","title":"4. Production Hardening","text":"<ul> <li>Add connection pooling and retry logic</li> <li>Implement proper error handling and logging</li> <li>Add monitoring and health checks</li> <li>Consider backup and disaster recovery</li> </ul>"},{"location":"production/custom-run-log-stores/#existing_implementation_examples","title":"Existing Implementation Examples","text":"<p>Before building your custom run log store, study the existing implementations in <code>extensions/run_log_store/</code>:</p>"},{"location":"production/custom-run-log-stores/#simple_pattern_examples","title":"Simple Pattern Examples","text":"<ul> <li><code>file_system.py</code> - Basic file-based storage pattern using <code>AnyPathRunLogStore</code></li> <li><code>minio.py</code> - Object storage integration with cloud APIs</li> </ul>"},{"location":"production/custom-run-log-stores/#advanced_pattern_examples","title":"Advanced Pattern Examples","text":"<ul> <li><code>chunked_fs.py</code> - Thread-safe file system storage for parallel execution</li> <li><code>chunked_minio.py</code> - Thread-safe cloud object storage</li> <li><code>generic_chunked.py</code> - Base class for thread-safe implementations</li> </ul>"},{"location":"production/custom-run-log-stores/#key_patterns_to_learn_from","title":"Key Patterns to Learn From","text":"<p>Thread Safety for Parallel Execution: <pre><code># From chunked_fs.py - see how it handles concurrent writes\nsupports_parallel_writes: bool = True\n\ndef put_run_log(self, run_log: RunLog):\n    # Thread-safe file operations with locking\n</code></pre></p> <p>Storage Abstraction: <pre><code># From any_path.py - see the abstraction pattern\n@abstractmethod\ndef write_to_path(self, run_log: RunLog): ...\n\n@abstractmethod\ndef read_from_path(self, run_id: str) -&gt; RunLog: ...\n</code></pre></p> <p>Cloud Storage Integration: <pre><code># From minio.py - see how it handles object storage APIs\ndef put_run_log(self, run_log: RunLog):\n    # Object storage with proper error handling\n</code></pre></p> <p>Learn from Production Code</p> <p>These implementations show real-world patterns for:</p> <ul> <li>Error handling and retry logic</li> <li>Serialization and data format decisions</li> <li>Performance optimization for different storage types</li> <li>Configuration patterns and validation</li> <li>Thread safety and parallel execution support</li> </ul>"},{"location":"production/custom-run-log-stores/#need_help","title":"Need Help?","text":"<p>Custom run log stores involve complex data persistence and query patterns that require understanding both runnable's execution metadata model and your target storage system's capabilities.</p> <p>Get Support</p> <p>We're here to help you succeed! Building custom run log stores involves detailed knowledge of:</p> <ul> <li>RunLog and StepLog data structures and serialization</li> <li>Query patterns and performance optimization</li> <li>Parallel execution safety and data consistency</li> <li>Storage system integration and error handling</li> </ul> <p>Don't hesitate to reach out:</p> <ul> <li>\ud83d\udce7 Contact the team for architecture guidance and data model support</li> <li>\ud83e\udd1d Collaboration opportunities - we're interested in supporting database and analytics integrations</li> <li>\ud83d\udcd6 Documentation feedback - help us improve these guides based on your implementation experience</li> </ul> <p>Better together: Data persistence integrations benefit from collaboration between storage experts (you) and runnable data model experts (us).</p> <p>Complex Integration</p> <p>These are sophisticated integrations that involve:</p> <ul> <li>Understanding runnable's internal data models and serialization</li> <li>Designing efficient query patterns for your storage system</li> <li>Handling concurrent access and data consistency</li> <li>Managing large data volumes and performance optimization</li> </ul> <p>Success is much more likely with collaboration. The existing implementations took significant effort to get right - leverage our experience to avoid common pitfalls.</p> <p>Your success with custom run log stores helps the entire runnable community!</p>"},{"location":"production/deploy-anywhere/","title":"\ud83c\udf0d Deploy Anywhere","text":"<p>Here's the ultimate superpower: Same code runs everywhere. Just change the configuration.</p> <p>Good news: Your runnable pipeline is already production-ready. You've been building with production in mind this whole time.</p>"},{"location":"production/deploy-anywhere/#your_code_never_changes","title":"Your code never changes","text":"<pre><code>from runnable import Pipeline, PythonTask\n\ndef train_model():\n    # Your model training logic\n    print(\"Training model...\")\n    return \"model_v1.pkl\"\n\ndef main():\n    pipeline = Pipeline(steps=[\n        PythonTask(function=train_model, name=\"training\")\n    ])\n\n    pipeline.execute()\n    return pipeline\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"production/deploy-anywhere/#change_where_it_runs","title":"Change where it runs","text":""},{"location":"production/deploy-anywhere/#local_development","title":"\ud83d\udcbb Local development","text":"<pre><code>pipeline.execute()  # Runs on your laptop\n</code></pre>"},{"location":"production/deploy-anywhere/#container_execution","title":"\ud83d\udc33 Container execution","text":"<pre><code>pipeline.execute(configuration_file=\"local-container.yaml\")  # Runs in Docker\n</code></pre>"},{"location":"production/deploy-anywhere/#cloud_platforms","title":"\u2601\ufe0f Cloud platforms","text":"<pre><code>pipeline.execute(configuration_file=\"argo.yaml\")  # Runs on Argo Workflows\n</code></pre>"},{"location":"production/deploy-anywhere/#configuration_files_define_the_environment","title":"Configuration files define the environment","text":"<p>The same pipeline code runs in any environment - just change the configuration:</p> DevelopmentTestingContainerizedProduction <p>Fast local development (default configuration): local.yaml<pre><code>pipeline-executor:\n  type: local\n\nrun-log-store:\n  type: file-system\n\ncatalog:\n  type: file-system\n\nsecrets:\n  type: env-secrets\n</code></pre></p> <pre><code># Uses default configuration\nuv run my_pipeline.py\n</code></pre> <p>Pipeline validation without execution: mocked.yaml<pre><code>pipeline-executor:\n  type: mocked\n\nrun-log-store:\n  type: file-system\n\ncatalog:\n  type: file-system\n\nsecrets:\n  type: env-secrets\n</code></pre></p> <pre><code>export RUNNABLE_CONFIGURATION_FILE=mocked.yaml\nuv run my_pipeline.py\n</code></pre> <p>Isolated execution in containers: local-container.yaml<pre><code>pipeline-executor:\n  type: local-container\n  config:\n    # Build from project root to include runnable + dependencies\n    docker_image: \"my-pipeline:latest\"  # or use existing image with runnable installed\n\nrun-log-store:\n  type: file-system\n\ncatalog:\n  type: file-system\n\nsecrets:\n  type: env-secrets\n</code></pre></p> <p>Container Image Requirements</p> <p>The Docker image must have Runnable installed. Either:</p> <ul> <li>Build from your project root: <code>docker build -t my-pipeline:latest .</code> (includes your code + runnable)</li> <li>Use a base image with runnable: <code>FROM python:3.11</code> then <code>RUN pip install runnable</code></li> <li>Never use bare <code>python:3.11</code> - it doesn't include runnable</li> </ul> <pre><code>export RUNNABLE_CONFIGURATION_FILE=local-container.yaml\nuv run my_pipeline.py\n</code></pre> <p>Production orchestration on Kubernetes: argo.yaml<pre><code>pipeline-executor:\n  type: argo\n  config:\n    pvc_for_runnable: runnable\n    defaults:\n      image: \"my-pipeline:v1.0\"\n      resources:\n        limits:\n          cpu: \"2\"\n          memory: 4Gi\n        requests:\n          cpu: \"1\"\n          memory: 2Gi\n    argoWorkflow:\n      metadata:\n        generateName: \"pipeline-\"\n        namespace: production\n      spec:\n        serviceAccountName: \"pipeline-executor\"\n\nrun-log-store:\n  type: chunked-fs\n  config:\n    log_folder: /mnt/run_log_store\n\ncatalog:\n  type: s3\n  config:\n    bucket: production-data\n\nsecrets:\n  type: env-secrets\n</code></pre></p> <pre><code>export RUNNABLE_CONFIGURATION_FILE=argo.yaml\nuv run my_pipeline.py\n</code></pre> <p>Same code, different environments - just change the <code>RUNNABLE_CONFIGURATION_FILE</code>.</p>"},{"location":"production/deploy-anywhere/#the_power_of_environment-agnostic_code","title":"The power of environment-agnostic code","text":"<p>Development: <pre><code># Quick local testing\nuv run my_pipeline.py\n</code></pre></p> <p>Production: <pre><code># Same code, production environment\nexport RUNNABLE_CONFIGURATION_FILE=production-argo.yaml\nuv run my_pipeline.py\n</code></pre></p>"},{"location":"production/deploy-anywhere/#real-world_example","title":"Real-world example","text":"<p>From development to production:</p> <pre><code>from runnable import Pipeline, PythonTask\nfrom examples.common.functions import hello\n\ndef main():\n    # Same code, different environments\n    task = PythonTask(function=hello, name=\"say_hello\")\n    pipeline = Pipeline(steps=[task])\n\n    # This execute() call works for both development and production\n    # Environment determined by RUNNABLE_CONFIGURATION_FILE\n    pipeline.execute()\n    return pipeline\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Development (uses default local config): <pre><code>uv run my_pipeline.py\n</code></pre></p> <p>Production (same code, argo config): <pre><code>export RUNNABLE_CONFIGURATION_FILE=argo.yaml\nuv run my_pipeline.py\n</code></pre></p> See complete runnable code examples/01-tasks/python_tasks.py<pre><code>\"\"\"\nYou can execute this pipeline by:\n\n    python examples/01-tasks/python_tasks.py\n\nThe stdout of \"Hello World!\" would be captured as execution\nlog and stored in the catalog.\n\"\"\"\n\nfrom examples.common.functions import hello\nfrom runnable import Pipeline, PythonTask\n\n\ndef main():\n    # Create a tasks which calls the function \"hello\"\n    # If this step executes successfully,\n    # the pipeline will terminate with success\n    hello_task = PythonTask(  # [concept:task]\n        name=\"hello\",\n        function=hello,\n    )\n\n    # The pipeline has only one step.\n    pipeline = Pipeline(steps=[hello_task])  # [concept:pipeline]\n\n    pipeline.execute()  # [concept:execution]\n    return pipeline\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Try it now: <pre><code>uv run examples/01-tasks/python_tasks.py\n</code></pre></p> <p>Dev environment: Runs in 2 seconds on your laptop Prod environment: Runs on Kubernetes with monitoring, logging, and auto-scaling</p> <p>Zero code changes.</p>"},{"location":"production/deploy-anywhere/#configuration_reference","title":"Configuration Reference","text":"<p>Ready to customize your deployment? Check the configuration documentation:</p> <ul> <li>Pipeline Executors - Choose where pipelines run (local, argo, etc.)</li> <li>Job Executors - Configure task execution (local, containers, kubernetes)</li> <li>Storage - Set up data persistence (local, S3, MinIO)</li> <li>Logging - Configure execution logs</li> <li>Secrets - Manage sensitive configuration</li> </ul> <p>You now know the core concepts! Start with the examples above, then dive into the configuration reference for advanced setups.</p>"},{"location":"production/run-log/","title":"Run Log Store Configuration","text":"<p>Run logs make reproducibility simple - they capture everything needed to understand, debug, and recreate your pipeline executions.</p>"},{"location":"production/run-log/#why_run_logs_matter_for_reproducibility","title":"Why Run Logs Matter for Reproducibility","text":"<p>Complete Execution History</p> <p>Every run is fully documented: Run logs capture the complete context of your pipeline execution</p> <ul> <li>\ud83d\udcca Parameters and inputs: What data and settings were used</li> <li>\ud83d\udd04 Execution timeline: When each step ran and how long it took</li> <li>\ud83d\udcbe Data lineage: Which data artifacts were created and consumed</li> <li>\ud83d\udcdd Code snapshots: Git commits and code versions used</li> <li>\u274c Failure details: Exact error messages and stack traces</li> <li>\ud83c\udff7\ufe0f Environment metadata: Configuration and infrastructure used</li> </ul> <p>Reproducibility Made Easy</p> <p>Run logs enable you to:</p> <ul> <li>Debug production failures by recreating exact conditions locally</li> <li>Compare experiments across different parameter sets or code versions</li> <li>Audit model training with complete training history and data lineage</li> <li>Resume failed pipelines from the exact point of failure</li> </ul>"},{"location":"production/run-log/#available_run_log_stores","title":"Available Run Log Stores","text":"Store Type Environment Best For <code>buffered</code> In-memory only Quick testing and development <code>file-system</code> Any environment with mounted log_folder Sequential execution, simple setup <code>chunked-fs</code> Any environment with mounted log_folder Parallel execution, universal choice <code>minio</code> / <code>chunked-minio</code> Object storage Distributed systems without shared filesystem"},{"location":"production/run-log/#buffered","title":"buffered","text":"<p>Stores run logs in-memory only. No persistence - data is lost when execution completes.</p> <p>In-Memory Only</p> <ul> <li>No persistence: Run logs are lost after execution</li> <li>Testing only: Not suitable for production or reproducibility</li> <li>No parallel support: Race conditions occur with concurrent execution</li> </ul> <p>Use case: Quick testing and debugging during development.</p>"},{"location":"production/run-log/#configuration","title":"Configuration","text":"<pre><code>run-log-store:\n  type: buffered\n</code></pre>"},{"location":"production/run-log/#file-system","title":"file-system","text":"<p>Stores run logs as single JSON files in the filesystem - simple and reliable for sequential execution.</p> <p>Works Everywhere with Mounted Storage</p> <p>Runs in any environment where log_folder is accessible</p> <ul> <li>\ud83d\udcbe Persistent storage: Run logs saved to mounted filesystem</li> <li>\ud83d\udcc1 Simple structure: One JSON file per pipeline run</li> <li>\ud83d\udd0d Easy debugging: Human-readable JSON format</li> <li>\ud83c\udfe0 Local development: Direct filesystem access</li> <li>\ud83d\udc33 Containers: Works with volume mounts</li> <li>\u2638\ufe0f Kubernetes: Works with persistent volumes</li> </ul> <p>Sequential Only</p> <p>Not suitable for parallel execution - use <code>chunked-fs</code> for parallel workflows</p>"},{"location":"production/run-log/#configuration_1","title":"Configuration","text":"<pre><code>run-log-store:\n  type: file-system\n  config:\n    log_folder: \".run_log_store\"  # Optional: defaults to \".run_log_store\"\n</code></pre>"},{"location":"production/run-log/#example","title":"Example","text":"job.pyconfig.yaml <pre><code>from runnable import PythonJob\nfrom examples.common.functions import hello\n\ndef main():\n    job = PythonJob(function=hello)\n    job.execute()\n    return job\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <pre><code>run-log-store:\n  type: file-system\n  config:\n    log_folder: \".run_log_store\"\n</code></pre> <p>Run with file-system logging: <pre><code>RUNNABLE_CONFIGURATION_FILE=config.yaml uv run job.py\n</code></pre></p> <p>Result: Run log stored as <code>.run_log_store/{run_id}.json</code> with complete execution metadata for reproducibility.</p>"},{"location":"production/run-log/#chunked-fs","title":"chunked-fs","text":"<p>Thread-safe run log store - works everywhere with parallel execution support. The recommended choice for most use cases.</p> <p>Works Everywhere with Mounted Storage</p> <p>Runs in any environment where log_folder is accessible</p> <ul> <li>\u2705 Thread-safe: Supports parallel execution without race conditions</li> <li>\ud83c\udfe0 Local development: Direct filesystem access</li> <li>\ud83d\udc33 Containers: Works with volume mounts (Docker, local-container executor)</li> <li>\u2638\ufe0f Kubernetes: Works with persistent volumes (Argo, k8s-job executor)</li> <li>\u26a1 Parallel execution: Enable <code>enable_parallel: true</code> safely</li> <li>\ud83d\udcbe Persistent: Full reproducibility with detailed execution history</li> </ul> <p>Recommended Default</p> <p>Use <code>chunked-fs</code> unless you have specific requirements - it provides parallel safety and works in all execution environments where the log_folder can be mounted.</p>"},{"location":"production/run-log/#configuration_2","title":"Configuration","text":"<pre><code>run-log-store:\n  type: chunked-fs\n  config:\n    log_folder: \".run_log_store\"  # Optional: defaults to \".run_log_store\"\n</code></pre>"},{"location":"production/run-log/#example_1","title":"Example","text":"pipeline.pyconfig.yaml <pre><code>from runnable import Pipeline, PythonTask, Parallel\nfrom examples.common.functions import hello\n\ndef main():\n    # Parallel execution safe with chunked-fs\n    parallel_node = Parallel(\n        name=\"parallel_tasks\",\n        branches={\n            \"task_a\": PythonTask(function=hello, name=\"hello_a\"),\n            \"task_b\": PythonTask(function=hello, name=\"hello_b\")\n        }\n    )\n\n    pipeline = Pipeline(steps=[parallel_node])\n    pipeline.execute()\n    return pipeline\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <pre><code>run-log-store:\n  type: chunked-fs\n\npipeline-executor:\n  type: local\n  config:\n    enable_parallel: true  # Safe with chunked-fs\n</code></pre> <p>Run with chunked-fs logging: <pre><code>RUNNABLE_CONFIGURATION_FILE=config.yaml uv run pipeline.py\n</code></pre></p> <p>Result: Run logs stored as separate files in <code>.run_log_store/{run_id}/</code> directory:</p> <ul> <li><code>RunLog.json</code> - Pipeline metadata and configuration</li> <li><code>StepLog-{step}-{timestamp}.json</code> - Individual step execution details</li> </ul> <p>This chunked structure enables thread-safe parallel writes while maintaining complete execution history for reproducibility.</p>"},{"location":"production/run-log/#object_storage_minio_chunked-minio","title":"Object Storage (minio / chunked-minio)","text":"<p>For distributed systems and cloud deployments, use object storage-based run log stores:</p>"},{"location":"production/run-log/#minio","title":"minio","text":"<pre><code>run-log-store:\n  type: minio\n  config:\n    endpoint: \"https://s3.amazonaws.com\"\n    access_key: \"your-access-key\"\n    secret_key: \"your-secret-key\"\n    bucket_name: \"runnable-logs\"\n</code></pre>"},{"location":"production/run-log/#chunked-minio_recommended","title":"chunked-minio (Recommended)","text":"<pre><code>run-log-store:\n  type: chunked-minio\n  config:\n    endpoint: \"https://s3.amazonaws.com\"\n    access_key: \"your-access-key\"\n    secret_key: \"your-secret-key\"\n    bucket_name: \"runnable-logs\"\n</code></pre> <p>Cloud Deployment</p> <p>Use <code>chunked-minio</code> for distributed systems - it provides the same parallel execution safety as <code>chunked-fs</code> but with cloud storage scalability.</p>"},{"location":"production/run-log/#choosing_the_right_run_log_store","title":"Choosing the Right Run Log Store","text":"<p>Decision Guide</p> <p>For most users: Use <code>chunked-fs</code> - works in any environment with mounted storage and supports parallel execution</p> <p>For development/testing: Use <code>buffered</code> for quick iterations where persistence isn't needed</p> <p>Sequential workflows: Use <code>file-system</code> - works in any environment with mounted storage but only for sequential execution</p> <p>Distributed systems without shared filesystem: Use <code>chunked-minio</code> when execution environments can't mount a shared log_folder</p> <p>Filesystem vs Object Storage</p> <p>Filesystem stores (<code>file-system</code>, <code>chunked-fs</code>): Work in any execution environment where the <code>log_folder</code> can be mounted</p> <ul> <li>\u2705 Local development (direct filesystem access)</li> <li>\u2705 Docker containers (volume mounts)</li> <li>\u2705 Kubernetes (persistent volumes)</li> <li>\u2705 Any containerized environment with volume mounting</li> </ul> <p>Object storage (<code>minio</code>, <code>chunked-minio</code>): Use when shared filesystem mounting isn't available</p> <p>Remember: Run logs are your key to reproducibility - they capture everything needed to understand, debug, and recreate your pipeline executions.</p>"},{"location":"production/run-log/#custom_run_log_stores","title":"Custom Run Log Stores","text":"<p>Need to integrate with your existing logging infrastructure? Build custom run log stores that send execution data anywhere using Runnable's extensible architecture.</p> <p>Enterprise Integration</p> <p>Integrate with your existing systems: Never be limited by built-in storage options</p> <ul> <li>\ud83d\udcca Enterprise logging: Send to Splunk, ELK Stack, Datadog, New Relic</li> <li>\ud83c\udfe2 Corporate databases: Store in existing data warehouses, time-series databases</li> <li>\ud83d\udd10 Compliance systems: Meet audit and governance requirements</li> <li>\ud83c\udf10 Multi-region storage: Distribute logs across geographic regions</li> </ul>"},{"location":"production/run-log/#building_custom_run_log_stores","title":"Building Custom Run Log Stores","text":"<p>Learn how to create production-ready custom run log stores:</p> <p>\ud83d\udcd6 Custom Run Log Stores Development Guide</p> <p>The guide provides:</p> <ul> <li>Complete stubbed implementation for database and cloud storage integration</li> <li>YAML to Pydantic configuration mapping with validation</li> <li>Storage system patterns for SQL, NoSQL, and cloud storage</li> <li>Performance optimization for high-volume deployments</li> </ul> <p>Quick Example</p> <p>Create a custom run log store in just 3 steps:</p> <ol> <li>Implement key methods by extending <code>BaseRunLogStore</code></li> <li>Register via entry point in your <code>pyproject.toml</code></li> <li>Configure via YAML for seamless integration</li> </ol> <pre><code>from runnable.datastore import BaseRunLogStore\n\nclass MyDatabaseRunLogStore(BaseRunLogStore):\n    service_name: str = \"my-database\"\n\n    def create_run_log(self, run_id: str, **kwargs):\n        # Your database integration here\n        pass\n</code></pre> <p>Ready to build? See the development guide for implementation patterns and examples.</p>"},{"location":"production/secrets/","title":"Secrets","text":"<p>Secrets are exposed as environmental variables in <code>runnable</code>.</p>"},{"location":"production/secrets/#do-nothing","title":"do-nothing","text":"<p>A no-op implementation of a secret manager. This is useful when you do not have need for secrets in your application.</p>"},{"location":"production/secrets/#configuration","title":"configuration","text":"<pre><code>secrets:\n  type: do-nothing\n</code></pre> <p>Note that this is the default configuration if nothing is specified.</p>"},{"location":"production/secrets/#environment_secret_manager","title":"Environment Secret Manager","text":"<p>A secrets manager to access secrets from environment variables. Many cloud based executors, especially K8's, have capabilities to send in secrets as environment variables and this secrets provider could used in those environments.</p>"},{"location":"production/secrets/#configuration_1","title":"Configuration","text":"<pre><code>secrets:\n  type: env-secrets-manager\n  config:\n    prefix: \"\" # default value\n    suffix: \"\" # default value\n</code></pre> <p>Use <code>suffix</code> and <code>prefix</code> the uniquely identify the secrets. The actual key while calling the secrets manager via the API, <code>get_secret(secret_key)</code> is <code>&lt;prefix&gt;&lt;secret_key&gt;&lt;suffix&gt;</code>.</p>"},{"location":"production/secrets/#example","title":"Example","text":"PipelineDefault ConfigurationPrefixed and Suffixed Configuration <p>Below is a simple pipeline to demonstrate the use of secrets.</p> <p>The configuration file to use can be dynamically specified via the environment variable <code>runnable_CONFIGURATION_FILE</code>.</p> <p>The example can be found in <code>examples/secrets_env.py</code></p> <pre><code>\n</code></pre> <p>We can execute the pipeline using this configuration by: <code>secret=\"secret_value\" runnable_CONFIGURATION_FILE=examples/configs/secrets-env-default.yaml python examples/secrets_env.py</code></p> <p>The configuration file is located at <code>examples/configs/secrets-env-default.yaml</code></p> <pre><code>\n</code></pre> <p>We can execute the pipeline using this configuration by: <code>runnable_secret=\"secret_value\" runnable_CONFIGURATION_FILE=examples/configs/secrets-env-ps.yaml python examples/secrets_env.py</code></p> <p>The configuration file is located at <code>examples/configs/secrets-env-ps.yaml</code></p> <pre><code>\n</code></pre>"},{"location":"production/secrets/#dotenv","title":"dotenv","text":"<p><code>.env</code> files are routinely used to provide configuration parameters and secrets during development phase. runnable can dotenv files as a secret store and can surface them to tasks.</p>"},{"location":"production/secrets/#configuration_2","title":"Configuration","text":"<pre><code>secrets:\n  type: dotenv\n  config:\n    location: .env # default value\n</code></pre> <p>The format of the <code>.env</code> file is <code>key=value</code> pairs. Any content after <code>#</code> is considered as a comment and will be ignored. Using <code>export</code> or <code>set</code>, case insensitive, as used for shell scripts are allowed.</p>"},{"location":"production/secrets/#example_1","title":"Example","text":".env fileExample configurationPipeline in python <p>Assumed to be present at <code>examples/secrets.env</code></p> <pre><code>\n</code></pre> <ol> <li>Shell scripts style are supported.</li> <li>Key value based format is also supported.</li> </ol> <p>Configuration to use the dotenv format file.</p> <p>Assumed to be present at <code>examples/configs/dotenv.yaml</code></p> <pre><code>secrets:\n  type: dotenv # (1)\n  config:\n    location: examples/secrets.env # (2)\n</code></pre> <ol> <li>Use dotenv secrets manager.</li> <li>Location of the dotenv file, defaults to <code>.env</code> in project root.</li> </ol> <p>The example is present in <code>examples/secrets.py</code></p> <pre><code>\n</code></pre> <ol> <li>The key of the secret that you want to retrieve.</li> </ol>"},{"location":"production/telemetry/","title":"Telemetry &amp; Observability","text":"<p>Runnable provides built-in telemetry for monitoring pipeline and task execution. The telemetry system supports two output modes:</p> <ol> <li>OpenTelemetry/logfire - Structured spans for observability backends (Jaeger, Datadog, etc.)</li> <li>SSE Streaming - Real-time events for web UI integration</li> </ol>"},{"location":"production/telemetry/#how_it_works","title":"How It Works","text":"<p>Every task execution emits telemetry:</p> <pre><code># In runnable/tasks.py\nclass PythonTaskType(BaseTaskType):\n    def execute_command(self, ...):\n        # OpenTelemetry span\n        with logfire.span(\"task:{task_name}\", task_name=self.command):\n            logfire.info(\"Task started\", inputs=...)\n\n            # SSE event (if queue is set)\n            self._emit_event({\"type\": \"task_started\", \"name\": self.command})\n\n            # ... execute task ...\n\n            self._emit_event({\"type\": \"task_completed\", \"name\": self.command})\n            logfire.info(\"Task completed\", outputs=...)\n</code></pre>"},{"location":"production/telemetry/#opentelemetry_integration","title":"OpenTelemetry Integration","text":"<p>Runnable uses logfire-api as a zero-dependency shim. When logfire is not installed, all telemetry calls are no-ops.</p>"},{"location":"production/telemetry/#installation","title":"Installation","text":"<pre><code>uv add logfire\n</code></pre>"},{"location":"production/telemetry/#configuration","title":"Configuration","text":"<p>There are two ways to configure telemetry:</p> <ol> <li>Environment Variables (recommended for containers)</li> <li>Programmatic (for local development or custom setups)</li> </ol>"},{"location":"production/telemetry/#environment_variable_configuration","title":"Environment Variable Configuration","text":"<p>For containerized execution (local-container, Argo, Kubernetes), set environment variables in your container or config. Runnable auto-configures logfire at import time.</p> Variable Description <code>RUNNABLE_TELEMETRY_CONSOLE</code> Set to <code>true</code> for console span output <code>OTEL_EXPORTER_OTLP_ENDPOINT</code> OTLP collector endpoint (e.g., <code>http://collector:4317</code>) <code>LOGFIRE_TOKEN</code> Logfire cloud token (enables cloud export) <p>Example in a container config:</p> <pre><code># argo-config.yaml or k8s config\nexecutor:\n  type: local-container\n  config:\n    docker_image: my-image:latest\n    environment:\n      RUNNABLE_TELEMETRY_CONSOLE: \"true\"\n      OTEL_EXPORTER_OTLP_ENDPOINT: \"http://otel-collector:4317\"\n</code></pre> <p>When the container runs <code>runnable execute_single_node ...</code>, telemetry is automatically configured before any task execution.</p>"},{"location":"production/telemetry/#programmatic_configuration","title":"Programmatic Configuration","text":"<p>For local development or custom setups, configure logfire in your application:</p> <pre><code>import logfire\n\n# Console output (development)\nlogfire.configure(\n    send_to_logfire=False,\n    console=logfire.ConsoleOptions(\n        colors=\"auto\",\n        span_style=\"indented\",\n        verbose=True,\n    ),\n)\n\n# Or send to OTEL collector\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\n\nlogfire.configure(\n    send_to_logfire=False,\n    additional_span_processors=[\n        BatchSpanProcessor(\n            OTLPSpanExporter(endpoint=\"http://localhost:4317\")\n        )\n    ],\n)\n</code></pre>"},{"location":"production/telemetry/#span_hierarchy","title":"Span Hierarchy","text":"<p>Pipeline execution creates nested spans:</p> <pre><code>pipeline:example                              [2.5s]\n\u251c\u2500\u2500 task:my_module.compute                    [1.0s]\n\u2502   \u251c\u2500\u2500 inputs: {\"x\": 10}\n\u2502   \u2514\u2500\u2500 outputs: {\"result\": 20}\n\u2514\u2500\u2500 task:my_module.finalize                   [0.5s]\n    \u251c\u2500\u2500 inputs: {\"result\": 20}\n    \u2514\u2500\u2500 outputs: {\"final\": \"Result: 20\"}\n</code></pre>"},{"location":"production/telemetry/#sse_streaming_for_web_ui","title":"SSE Streaming for Web UI","text":"<p>For real-time updates in a web interface, use the SSE streaming mechanism.</p>"},{"location":"production/telemetry/#setting_up_the_queue","title":"Setting Up the Queue","text":"<pre><code>from queue import Queue\nfrom runnable import set_stream_queue\n\n# Create queue for this request\nevent_queue = Queue()\n\n# Wire up the queue (must be in same thread as pipeline execution)\nset_stream_queue(event_queue)\n\n# Execute pipeline - tasks will emit events to queue\npipeline.execute()\n\n# Cleanup\nset_stream_queue(None)\n</code></pre>"},{"location":"production/telemetry/#event_types","title":"Event Types","text":"Type Fields Description <code>pipeline_started</code> <code>name</code> Pipeline execution began <code>task_started</code> <code>name</code> A task began execution <code>task_completed</code> <code>name</code> A task completed successfully <code>task_error</code> <code>name</code>, <code>error</code> A task failed <code>pipeline_completed</code> <code>status</code> Pipeline finished (success/error)"},{"location":"production/telemetry/#local_telemetry_example","title":"Local Telemetry Example","text":"<p>See examples/telemetry-local/simple_telemetry_test.py for a complete working example of local telemetry with console output.</p> <pre><code>from runnable import Pipeline, PythonTask, pickled\nimport logfire\n\ndef step_one(x: int = 5) -&gt; int:\n    result = x * 2\n    return result\n\ndef step_two(doubled: int) -&gt; str:\n    result = f\"Final result: {doubled}\"\n    return result\n\ndef main():\n    pipeline = Pipeline(\n        steps=[\n            PythonTask(\n                function=step_one,\n                name=\"step_one\",\n                returns=[pickled(\"doubled\")]\n            ),\n            PythonTask(\n                function=step_two,\n                name=\"step_two\",\n                returns=[pickled(\"final_result\")]\n            ),\n        ]\n    )\n\n    pipeline.execute()\n    return pipeline\n\nif __name__ == \"__main__\":\n    # Configure console telemetry output\n    logfire.configure(\n        send_to_logfire=False,\n        console=logfire.ConsoleOptions(\n            colors=\"auto\",\n            span_style=\"indented\",\n            verbose=True,\n        ),\n    )\n\n    main()\n</code></pre> <p>Run with: <code>uv run examples/telemetry-local/simple_telemetry_test.py</code></p>"},{"location":"production/telemetry/#fastapi_integration","title":"FastAPI Integration","text":"<p>For FastAPI applications, you can integrate telemetry with both traditional batch pipelines and async streaming workflows:</p> <p>Traditional Pipeline with SSE Events: <pre><code>from fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse\nfrom queue import Queue\nfrom runnable import set_stream_queue\nimport json\n\n@app.post(\"/run-pipeline\")\ndef run_pipeline_with_events():\n    event_queue = Queue()\n    set_stream_queue(event_queue)\n\n    def event_stream():\n        try:\n            # Execute pipeline in background\n            pipeline.execute()\n\n            # Stream events to client\n            while not event_queue.empty():\n                event = event_queue.get_nowait()\n                yield f\"data: {json.dumps(event)}\\n\\n\"\n        finally:\n            set_stream_queue(None)\n\n    return StreamingResponse(event_stream(), media_type=\"text/event-stream\")\n</code></pre></p> <p>Async Streaming Workflows: For real-time streaming with AsyncPipeline, see Async &amp; Streaming and the FastAPI LLM examples.</p>"},{"location":"production/telemetry/#telemetry_attributes","title":"Telemetry Attributes","text":"<p>Each task span includes:</p> Attribute Description <code>task_name</code> Full module path (e.g., <code>my_module.compute</code>) <code>task_type</code> Type of task (<code>python</code>, <code>notebook</code>, <code>shell</code>) <code>inputs</code> Serialized input parameters (truncated to 256 bytes) <code>outputs</code> Serialized output parameters (truncated to 256 bytes) <code>status</code> Execution status (<code>success</code> or error message)"},{"location":"production/telemetry/#dual_output_architecture","title":"Dual Output Architecture","text":"<p>The telemetry system supports simultaneous output to both OpenTelemetry and SSE:</p> <pre><code>Task Execution\n      \u2502\n      \u251c\u2500\u2500\u25ba logfire.span()     \u2500\u2500\u25ba OpenTelemetry Collector \u2500\u2500\u25ba Jaeger/Datadog/etc.\n      \u2502\n      \u2514\u2500\u2500\u25ba _emit_event()      \u2500\u2500\u25ba Queue \u2500\u2500\u25ba SSE \u2500\u2500\u25ba Web UI\n</code></pre> <p>Both outputs happen from the same code path, ensuring consistency between observability data and real-time UI updates.</p>"},{"location":"production/job-execution/custom-job-executors/","title":"Building Custom Job Executors","text":"<p>Execute jobs on any compute platform by understanding the two fundamental execution patterns.</p>"},{"location":"production/job-execution/custom-job-executors/#the_core_patterns","title":"The Core Patterns","text":"<p>There are only two ways runnable executes jobs:</p>"},{"location":"production/job-execution/custom-job-executors/#pattern_1_direct_execution_local","title":"Pattern 1: Direct Execution (Local)","text":"<p>Execute the job function directly in the current process - no CLI command needed.</p>"},{"location":"production/job-execution/custom-job-executors/#pattern_2_remote_execution_container_k8s_cloud","title":"Pattern 2: Remote Execution (Container, K8s, Cloud)","text":"<p>Cross environment boundaries by using the CLI command to recreate the job context in the remote environment.</p> <pre><code># Remote executors run this CLI command in the target environment\nrunnable execute-job my_job.py run_123 --config config.yaml --parameters params.yaml\n</code></pre>"},{"location":"production/job-execution/custom-job-executors/#implementation_examples","title":"Implementation Examples","text":""},{"location":"production/job-execution/custom-job-executors/#local_executor_-_direct_execution","title":"Local Executor - Direct Execution","text":"<pre><code>def submit_job(self, job, catalog_settings):\n    # Set up run log locally (storage is accessible)\n    self._set_up_run_log()\n    # Execute directly in same process\n    self.execute_job(job, catalog_settings)\n\ndef execute_job(self, job, catalog_settings):\n    # Direct function call - no CLI command\n    attempt_log = job.execute_command()\n    # Handle results and catalog sync\n    self._sync_catalog(catalog_settings)\n</code></pre>"},{"location":"production/job-execution/custom-job-executors/#remote_executor_-_cli_command_execution","title":"Remote Executor - CLI Command Execution","text":"<pre><code>def submit_job(self, job, catalog_settings):\n    # Do NOT set up run log here - storage not accessible locally!\n    # Make storage accessible to remote environment\n    self._setup_remote_storage_access()\n    # Get the CLI command and run it remotely\n    command = self._context.get_job_callable_command()\n    self._execute_on_remote_platform(command)\n\ndef execute_job(self, job, catalog_settings):\n    # This runs IN the remote environment\n    # Now we can set up run log - storage is accessible here\n    self._set_up_run_log()\n    # Point to mounted/accessible storage locations\n    self._use_remote_storage_locations()\n    # Execute the job\n    attempt_log = job.execute_command()\n    # Handle results and catalog sync\n    self._sync_catalog(catalog_settings)\n</code></pre>"},{"location":"production/job-execution/custom-job-executors/#the_critical_issue_storage_access","title":"The Critical Issue: Storage Access","text":"<p>\u26a0\ufe0f Common Bug: Calling <code>self._set_up_run_log()</code> when storage isn't accessible locally.</p> <p>Local execution: Storage is accessible from your machine Remote execution: Storage is only accessible inside the remote environment</p>"},{"location":"production/job-execution/custom-job-executors/#real_example_local-container_storage_access","title":"Real Example: Local-Container Storage Access","text":"<p>The local-container executor shows the correct pattern:</p> <pre><code>class LocalContainerJobExecutor(GenericJobExecutor):\n\n    def submit_job(self, job, catalog_settings):\n        # Set up run log BEFORE container (storage accessible locally)\n        self._set_up_run_log()\n        # Make storage accessible to container\n        self._mount_volumes()\n        # Run CLI command in container\n        self.spin_container()\n\n    def _mount_volumes(self):\n        \"\"\"Mount local storage into container\"\"\"\n        # Run log store\n        if self._context.run_log_store.service_name == \"file-system\":\n            host_path = self._context.run_log_store.log_folder\n            container_path = \"/tmp/run_logs/\"\n            self._volumes[host_path] = {\"bind\": container_path, \"mode\": \"rw\"}\n\n        # Catalog storage\n        if self._context.catalog.service_name == \"file-system\":\n            host_path = self._context.catalog.catalog_location\n            container_path = \"/tmp/catalog/\"\n            self._volumes[host_path] = {\"bind\": container_path, \"mode\": \"rw\"}\n\n    def execute_job(self, job, catalog_settings):\n        # This runs INSIDE the container\n        # Point to mounted locations\n        self._use_volumes()\n        # Now execute\n        attempt_log = job.execute_command()\n\n    def _use_volumes(self):\n        \"\"\"Update context to use mounted paths\"\"\"\n        if self._context.run_log_store.service_name == \"file-system\":\n            self._context.run_log_store.log_folder = \"/tmp/run_logs/\"\n\n        if self._context.catalog.service_name == \"file-system\":\n            self._context.catalog.catalog_location = \"/tmp/catalog/\"\n</code></pre>"},{"location":"production/job-execution/custom-job-executors/#for_cloud_platforms","title":"For Cloud Platforms","text":"<p>K8s: Use PersistentVolumeClaims AWS Batch: Mount EFS or use S3-compatible storage Your Platform: However your platform provides shared storage</p> <p>The pattern is always:</p> <ol> <li><code>submit_job()</code>: Make storage accessible to remote environment</li> <li><code>execute_job()</code>: Use the accessible storage locations</li> </ol>"},{"location":"production/job-execution/custom-job-executors/#plugin_registration","title":"Plugin Registration","text":"<p>Create your executor class and register it:</p> <pre><code>from extensions.job_executor import GenericJobExecutor\nfrom pydantic import Field\n\nclass MyPlatformJobExecutor(GenericJobExecutor):\n    service_name: str = \"my-platform\"\n\n    # Your platform config fields\n    api_endpoint: str = Field(...)\n    queue_name: str = Field(...)\n\n    def submit_job(self, job, catalog_settings):\n        # Your platform submission logic\n        pass\n\n    def execute_job(self, job, catalog_settings):\n        # Your execution logic (runs in remote environment)\n        pass\n</code></pre> <p>Register in <code>pyproject.toml</code>: <pre><code>[project.entry-points.'job_executor']\n\"my-platform\" = \"my_package.executors:MyPlatformJobExecutor\"\n</code></pre></p> <p>Use in configuration: <pre><code>job-executor:\n  type: my-platform\n  config:\n    api_endpoint: \"https://my-platform.com\"\n    queue_name: \"production\"\n</code></pre></p>"},{"location":"production/job-execution/custom-job-executors/#integration_advantage","title":"Integration Advantage","text":"<p>\ud83d\udd11 Key Benefit: Custom job executors live entirely in your codebase, not in public repositories or external dependencies.</p>"},{"location":"production/job-execution/custom-job-executors/#complete_control_privacy","title":"Complete Control &amp; Privacy","text":"<pre><code># In your private repository\n# my-company/internal-compute/executors/company_executor.py\n\nclass CompanyHPCJobExecutor(GenericJobExecutor):\n    service_name: str = \"company-hpc\"\n\n    # Your internal configuration\n    internal_scheduler_api: str = Field(...)\n    security_domain: str = Field(...)\n    cost_tracking_enabled: bool = Field(default=True)\n    compliance_level: str = Field(default=\"confidential\")\n\n    def submit_job(self, job, catalog_settings):\n        # Your proprietary HPC integration\n        # Company-specific security, audit trails, resource allocation\n        pass\n\n    def execute_job(self, job, catalog_settings):\n        # Company-specific monitoring and logging\n        # Internal cost tracking and usage metrics\n        pass\n</code></pre> <p>Integration benefits:</p> <ul> <li>\ud83d\udd12 Security: No external dependencies or public code exposure</li> <li>\ud83c\udfe2 Compliance: Implement organization-specific security and audit requirements</li> <li>\ud83d\udcb0 Cost Control: Direct integration with internal cost tracking and budgeting systems</li> <li>\ud83d\udd27 Customization: Build job executors for your exact compute infrastructure</li> <li>\ud83d\udcca Monitoring: Seamless integration with monitoring and alerting systems</li> </ul>"},{"location":"production/job-execution/custom-job-executors/#reusable_templates","title":"Reusable Templates","text":"<p>Teams can create internal libraries of job executors:</p> <pre><code># Internal package: company-runnable-executors\nfrom company_runnable_executors import (\n    ProductionHPCExecutor,      # Your HPC cluster setup\n    DevelopmentGPUExecutor,     # Development GPU nodes\n    ComplianceJobExecutor,      # SOC2/HIPAA job execution\n    SpotInstanceExecutor,       # Cost-optimized cloud compute\n)\n\n# Teams use your standardized executors\njob = PythonJob(function=train_model)\njob.execute(configuration_file=\"production-hpc-config.yaml\")\n</code></pre>"},{"location":"production/job-execution/custom-job-executors/#ecosystem_integration","title":"Ecosystem Integration","text":"<pre><code># Your company's standard job execution templates\njob-executor:\n  type: company-hpc\n  config:\n    internal_scheduler_api: \"https://hpc-scheduler.company.com\"\n    security_domain: \"ml-compute-domain\"\n    cost_tracking_enabled: true\n    compliance_level: \"confidential\"\n    resource_limits:\n      max_cpu: \"64\"\n      max_memory: \"512Gi\"\n      max_gpu: \"8\"\n    monitoring:\n      metrics_endpoint: \"https://company-metrics.com/jobs\"\n      alert_channels: [\"#compute-alerts\", \"#ml-ops\"]\n    audit:\n      log_level: \"detailed\"\n      retention_days: 365\n</code></pre> <p>This makes runnable a platform for standardizing job execution across your entire compute infrastructure - from development laptops to production HPC clusters.</p>"},{"location":"production/job-execution/custom-job-executors/#need_help","title":"Need Help?","text":"<p>Custom job executors involve complex cloud service integrations that require understanding both runnable's job execution model and your target platform's batch processing capabilities.</p> <p>Get Support</p> <p>We're here to help you succeed! Building custom job executors involves detailed knowledge of:</p> <ul> <li>Job context and command generation</li> <li>Run log and job log coordination</li> <li>Catalog synchronization patterns</li> <li>Platform-specific job submission and monitoring</li> </ul> <p>Don't hesitate to reach out:</p> <ul> <li>\ud83d\udce7 Contact the team for architecture guidance and integration support</li> <li>\ud83e\udd1d Collaboration opportunities - we're interested in supporting cloud platform integrations</li> <li>\ud83d\udcd6 Documentation feedback - help us improve these guides based on your implementation experience</li> </ul> <p>Better together: Cloud service integrations benefit from collaboration between platform experts (you) and runnable job execution experts (us).</p> <p>Complex Integration</p> <p>These are sophisticated cloud integrations that involve:</p> <ul> <li>Understanding runnable's job execution lifecycle and context management</li> <li>Integrating with cloud APIs that have varying reliability and rate limits</li> <li>Handling distributed execution, networking, and failure scenarios</li> <li>Managing container images, environment variables, and resource allocation</li> </ul> <p>Success is much more likely with collaboration. The existing cloud integrations required deep understanding of both runnable internals and platform specifics - leverage our experience to avoid common pitfalls.</p> <p>Your success with custom job executors helps the entire runnable community!</p>"},{"location":"production/job-execution/k8s-scheduling/","title":"Kubernetes Job Scheduling","text":"<p>Schedule jobs to run automatically using Kubernetes CronJobs with cron expressions.</p> <p>Installation Required</p> <p>Kubernetes execution requires the optional Kubernetes dependency: <pre><code>pip install runnable[k8s]\n</code></pre></p>"},{"location":"production/job-execution/k8s-scheduling/#why_use_job_scheduling","title":"Why Use Job Scheduling?","text":"<p>Scheduling Benefits</p> <p>Automate recurring workflows: Schedule jobs to run on a regular cadence</p> <ul> <li>\u23f0 Cron-based scheduling: Use familiar cron expressions</li> <li>\ud83d\udd04 Automatic execution: Kubernetes handles job execution</li> <li>\ud83d\udcca No immediate execution: Schedule and let Kubernetes manage it</li> <li>\ud83c\udfaf Production-ready: Native Kubernetes CronJob support</li> </ul> <p>Trade-offs</p> <ul> <li>\u23f1\ufe0f No immediate results: Job runs according to schedule, not immediately</li> <li>\ud83d\udd0d Monitoring required: Track scheduled job execution via Kubernetes</li> <li>\ud83d\udcdd UTC timezone: Kubernetes CronJobs run in UTC timezone</li> </ul>"},{"location":"production/job-execution/k8s-scheduling/#basic_configuration","title":"Basic Configuration","text":"<p>Add a <code>schedule</code> field to your Kubernetes job executor configuration:</p> <pre><code>job-executor:\n  type: \"k8s-job\"\n  config:\n    schedule: \"0 2 * * *\"  # Daily at 2 AM UTC\n    pvc_claim_name: \"runnable-storage\"\n    jobSpec:\n      template:\n        spec:\n          container:\n            image: \"my-project:v1.0\"\n</code></pre> <p>When Schedule is Present</p> <ul> <li>Creates a Kubernetes CronJob instead of a regular Job</li> <li>Displays scheduled job information to console</li> <li>No immediate execution - job runs according to schedule</li> <li>Kubernetes handles the scheduling automatically</li> </ul> <p>When Schedule is Absent</p> <p>Normal behavior with immediate Job execution - fully backward compatible with existing configurations.</p>"},{"location":"production/job-execution/k8s-scheduling/#schedule_format","title":"Schedule Format","text":"<p>The schedule field accepts standard cron expressions with 5 fields:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 minute (0 - 59)\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 hour (0 - 23)\n\u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of month (1 - 31)\n\u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 month (1 - 12)\n\u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of week (0 - 6) (Sunday = 0)\n\u2502 \u2502 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 \u2502 \u2502\n* * * * *\n</code></pre>"},{"location":"production/job-execution/k8s-scheduling/#common_schedule_examples","title":"Common Schedule Examples","text":"<pre><code># Every day at 2 AM UTC\nschedule: \"0 2 * * *\"\n\n# Every hour (at minute 0)\nschedule: \"0 * * * *\"\n\n# Every Monday at 9 AM UTC\nschedule: \"0 9 * * 1\"\n\n# Every 15 minutes\nschedule: \"*/15 * * * *\"\n\n# Every Sunday at 3:30 AM UTC\nschedule: \"30 3 * * 0\"\n\n# First day of month at midnight UTC\nschedule: \"0 0 1 * *\"\n</code></pre>"},{"location":"production/job-execution/k8s-scheduling/#complete_example","title":"Complete Example","text":"job.pyk8s-scheduled-job.yaml examples/11-jobs/python_tasks.py<pre><code>from examples.common.functions import hello\nfrom runnable import PythonJob\n\ndef main():\n    job = PythonJob(function=hello)\n    job.execute()\n    return job\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> examples/11-jobs/k8s-scheduled-job.yaml<pre><code>job-executor:\n  type: \"k8s-job\"\n  config:\n    pvc_claim_name: runnable\n    config_path:\n    mock: false\n    namespace: enterprise-mlops\n    schedule: \"* * * * *\"  # Run daily at 2 AM UTC\n    jobSpec:\n      activeDeadlineSeconds: 32000\n      template:\n        spec:\n          activeDeadlineSeconds: 86400\n          container:\n            image: harbor.csis.astrazeneca.net/mlops/runnable:latest\n            resources:\n              limits:\n                cpu: \"1\"\n                memory: \"2Gi\"\n              requests:\n                cpu: \"500m\"\n                memory: \"1Gi\"\n</code></pre> <p>Schedule the job:</p> <pre><code># Push your image to the registry\ndocker push your-registry.com/your-project:latest\n\n# Schedule the job with Kubernetes CronJob\nRUNNABLE_CONFIGURATION_FILE=examples/11-jobs/k8s-scheduled-job.yaml uv run examples/11-jobs/python_tasks.py\n</code></pre> <p>Expected output:</p> <pre><code>\u2713 CronJob scheduled successfully\n  Name: run-20231129-143022-123\n  Namespace: enterprise-mlops\n  Schedule: 0 2 * * *\n\n  Job Spec:\n  - Image: harbor.csis.astrazeneca.net/mlops/runnable:latest\n  - Resources: {'limits': {'cpu': '1', 'memory': '2Gi'}}\n</code></pre> <p>No Immediate Execution</p> <p>When scheduling is enabled, the job does not execute immediately. Kubernetes will execute the job according to the specified schedule.</p>"},{"location":"production/job-execution/k8s-scheduling/#configuration_reference","title":"Configuration Reference","text":""},{"location":"production/job-execution/k8s-scheduling/#all_kubernetes_executors_support_scheduling","title":"All Kubernetes Executors Support Scheduling","text":"<p>The <code>schedule</code> field is available for all Kubernetes job executor variants:</p> Production Kubernetes (k8s-job)Minikube Development (mini-k8s-job) <pre><code>job-executor:\n  type: k8s-job\n  config:\n    schedule: \"0 2 * * *\"              # Optional: Cron schedule\n    pvc_claim_name: \"runnable-storage\" # Required: PVC for data\n    namespace: \"default\"               # Optional: K8s namespace\n    jobSpec:\n      template:\n        spec:\n          container:\n            image: \"my-project:v1.0\"   # Required: Docker image\n</code></pre> <pre><code>job-executor:\n  type: mini-k8s-job\n  config:\n    schedule: \"0 2 * * *\"              # Optional: Cron schedule\n    namespace: \"default\"               # Optional: K8s namespace\n    jobSpec:\n      template:\n        spec:\n          container:\n            image: \"my-project:latest\" # Required: Docker image\n</code></pre>"},{"location":"production/job-execution/k8s-scheduling/#schedule_configuration_options","title":"Schedule Configuration Options","text":"Field Type Required Description <code>schedule</code> str No Cron expression (5 fields) for scheduling <code>pvc_claim_name</code> str Yes (k8s-job) PVC name for data persistence <code>jobSpec.template.spec.container.image</code> str Yes Docker image for job execution <code>namespace</code> str No Kubernetes namespace (default: \"default\")"},{"location":"production/job-execution/k8s-scheduling/#managing_scheduled_jobs","title":"Managing Scheduled Jobs","text":""},{"location":"production/job-execution/k8s-scheduling/#view_scheduled_cronjobs","title":"View Scheduled CronJobs","text":"<pre><code># List all CronJobs\nkubectl get cronjobs -n &lt;namespace&gt;\n\n# Describe a specific CronJob\nkubectl describe cronjob &lt;run-id&gt; -n &lt;namespace&gt;\n\n# View CronJob schedule\nkubectl get cronjob &lt;run-id&gt; -n &lt;namespace&gt; -o jsonpath='{.spec.schedule}'\n</code></pre>"},{"location":"production/job-execution/k8s-scheduling/#view_job_executions","title":"View Job Executions","text":"<pre><code># List jobs created by CronJob\nkubectl get jobs -n &lt;namespace&gt; -l cronjob=&lt;run-id&gt;\n\n# View pods from scheduled jobs\nkubectl get pods -n &lt;namespace&gt; -l job-name=&lt;job-name&gt;\n\n# Check job logs\nkubectl logs -n &lt;namespace&gt; -l job-name=&lt;job-name&gt;\n</code></pre>"},{"location":"production/job-execution/k8s-scheduling/#delete_scheduled_cronjob","title":"Delete Scheduled CronJob","text":"<pre><code># Delete a CronJob (stops future executions)\nkubectl delete cronjob &lt;run-id&gt; -n &lt;namespace&gt;\n</code></pre>"},{"location":"production/job-execution/k8s-scheduling/#troubleshooting","title":"Troubleshooting","text":""},{"location":"production/job-execution/k8s-scheduling/#invalid_cron_expression","title":"Invalid Cron Expression","text":"<p>ValidationError: Schedule must be a valid cron expression</p> <p>Problem: Cron expression format is incorrect</p> <p>Solution: Ensure your cron expression has exactly 5 space-separated fields</p> <pre><code># \u274c Wrong: Too few fields\nschedule: \"0 2 * *\"\n\n# \u274c Wrong: Too many fields\nschedule: \"0 0 2 * * *\"\n\n# \u2713 Correct: Exactly 5 fields\nschedule: \"0 2 * * *\"\n</code></pre>"},{"location":"production/job-execution/k8s-scheduling/#permission_issues","title":"Permission Issues","text":"<p>Forbidden: CronJob creation not allowed</p> <p>Problem: Service account lacks permissions to create CronJobs</p> <p>Solution: Ensure your Kubernetes service account has permissions</p> <pre><code># Check current permissions\nkubectl auth can-i create cronjobs -n &lt;namespace&gt;\n\n# Grant permissions (requires cluster admin)\nkubectl create clusterrolebinding cronjob-creator \\\n  --clusterrole=edit \\\n  --serviceaccount=&lt;namespace&gt;:&lt;service-account&gt;\n</code></pre>"},{"location":"production/job-execution/k8s-scheduling/#cronjob_not_executing","title":"CronJob Not Executing","text":"<p>CronJob created but jobs not running</p> <p>Problem: CronJob exists but no Job executions</p> <p>Solution: Check CronJob status and events</p> <pre><code># Check CronJob status\nkubectl describe cronjob &lt;run-id&gt; -n &lt;namespace&gt;\n\n# View recent events\nkubectl get events -n &lt;namespace&gt; --sort-by='.lastTimestamp'\n\n# Verify CronJob is not suspended\nkubectl get cronjob &lt;run-id&gt; -n &lt;namespace&gt; -o jsonpath='{.spec.suspend}'\n</code></pre>"},{"location":"production/job-execution/k8s-scheduling/#timezone_considerations","title":"Timezone Considerations","text":"<p>All Times are UTC</p> <p>Kubernetes CronJobs always use UTC timezone. Convert your local time to UTC when setting schedules.</p> <pre><code># Example: Run at 2 AM EST (7 AM UTC)\nschedule: \"0 7 * * *\"\n\n# Example: Run at 5 PM PST (1 AM next day UTC)\nschedule: \"0 1 * * *\"\n</code></pre>"},{"location":"production/job-execution/k8s-scheduling/#debug_mode","title":"Debug Mode","text":"<p>Test scheduled job configuration without creating actual CronJobs:</p> <pre><code>job-executor:\n  type: k8s-job\n  config:\n    mock: true  # Logs CronJob spec without creating it\n    schedule: \"0 2 * * *\"\n    pvc_claim_name: \"runnable-storage\"\n    jobSpec:\n      template:\n        spec:\n          container:\n            image: \"my-project:latest\"\n</code></pre>"},{"location":"production/job-execution/k8s-scheduling/#cron_expression_reference","title":"Cron Expression Reference","text":""},{"location":"production/job-execution/k8s-scheduling/#special_characters","title":"Special Characters","text":"Character Description Example <code>*</code> Any value <code>* * * * *</code> = every minute <code>,</code> Value list <code>0,30 * * * *</code> = every hour at :00 and :30 <code>-</code> Range of values <code>0 9-17 * * *</code> = every hour from 9 AM to 5 PM <code>/</code> Step values <code>*/5 * * * *</code> = every 5 minutes"},{"location":"production/job-execution/k8s-scheduling/#common_patterns","title":"Common Patterns","text":"<pre><code># Every minute\nschedule: \"* * * * *\"\n\n# Every 5 minutes\nschedule: \"*/5 * * * *\"\n\n# Every hour at 30 minutes past\nschedule: \"30 * * * *\"\n\n# Every 6 hours\nschedule: \"0 */6 * * *\"\n\n# Every day at noon\nschedule: \"0 12 * * *\"\n\n# Every weekday at 9 AM\nschedule: \"0 9 * * 1-5\"\n\n# Every Saturday at midnight\nschedule: \"0 0 * * 6\"\n\n# First Monday of every month at 8 AM\nschedule: \"0 8 1-7 * 1\"\n</code></pre>"},{"location":"production/job-execution/k8s-scheduling/#best_practices","title":"Best Practices","text":"<p>Scheduling Best Practices</p> <p>Resource Management</p> <ul> <li>Set appropriate resource limits for scheduled jobs</li> <li>Consider cluster load during peak hours</li> <li>Use different schedules to spread load</li> </ul> <p>Monitoring</p> <ul> <li>Set up alerts for failed scheduled jobs</li> <li>Monitor CronJob execution history</li> <li>Track job completion times</li> </ul> <p>Cleanup</p> <ul> <li>Configure <code>successfulJobsHistoryLimit</code> and <code>failedJobsHistoryLimit</code></li> <li>Delete CronJobs that are no longer needed</li> <li>Archive old job logs</li> </ul>"},{"location":"production/job-execution/k8s-scheduling/#when_to_use_other_executors","title":"When to Use Other Executors","text":"<p>Consider alternatives when you need:</p> <p>Immediate Execution</p> <p>Kubernetes Jobs: For immediate job execution without scheduling</p> <p>Local: For simple development without containers</p> <p>Different Scheduling Systems</p> <p>External schedulers: If you need features like:</p> <ul> <li>Dependency-based scheduling</li> <li>Complex conditional execution</li> <li>Integration with existing scheduling systems</li> </ul> <p>Related: Kubernetes Jobs | Pipeline Argo Workflows | All Job Executors</p>"},{"location":"production/job-execution/kubernetes/","title":"Kubernetes Job Execution","text":"<p>Execute jobs on Kubernetes clusters with production-grade resource management, persistence, and scalability.</p> <p>Installation Required</p> <p>Kubernetes execution requires the optional Kubernetes dependency: <pre><code>pip install runnable[k8s]\n</code></pre></p>"},{"location":"production/job-execution/kubernetes/#why_use_kubernetes_execution","title":"Why Use Kubernetes Execution?","text":"<p>Production Benefits</p> <p>Enterprise-ready orchestration: Production-scale job execution with Kubernetes</p> <ul> <li>\ud83c\udfd7\ufe0f Resource management: CPU, memory, GPU limits and requests</li> <li>\ud83d\udcbe Persistent storage: Shared data across job runs</li> <li>\ud83d\udd04 Scalability: Leverage multi-node cluster resources</li> <li>\ud83d\udcca Monitoring: Native Kubernetes observability and logging</li> </ul> <p>Trade-offs</p> <ul> <li>\ud83d\udc33 Infrastructure requirement: Needs Kubernetes cluster setup</li> <li>\u2699\ufe0f Complexity: More moving parts than local executors</li> <li>\ud83d\ude80 Pod overhead: ~10-30 seconds startup time</li> </ul>"},{"location":"production/job-execution/kubernetes/#kubernetes_variants","title":"Kubernetes Variants","text":"<p>Runnable provides two Kubernetes job executors for different cluster setups:</p> Production KubernetesMinikube Development <p>Use for: Real Kubernetes clusters with persistent storage</p> <pre><code>job-executor:\n  type: k8s-job\n  config:\n    pvc_claim_name: \"runnable-storage\"  # REQUIRED\n    jobSpec:\n      template:\n        spec:\n          container:\n            image: \"my-project:v1.0\"  # REQUIRED - your project image\n</code></pre> <p>Use for: Local Kubernetes development with minikube</p> <pre><code>job-executor:\n  type: mini-k8s-job\n  config:\n    jobSpec:\n      template:\n        spec:\n          container:\n            image: \"my-project:latest\"  # REQUIRED - your project image\n</code></pre> <p>Container Setup Made Simple</p> <p>Just build a Docker image from your project root and push to your registry:</p> <pre><code># Build from your project root\ndocker build -t my-project:v1.0 .\ndocker push your-registry.com/my-project:v1.0\n</code></pre> <p>Standard Kubernetes Job Specification</p> <p>The <code>jobSpec</code> configuration follows the standard Kubernetes Job API specification. Runnable uses native Kubernetes Job configuration - you can reference the official Kubernetes documentation for all available fields and options.</p>"},{"location":"production/job-execution/kubernetes/#getting_started","title":"Getting Started","text":""},{"location":"production/job-execution/kubernetes/#simple_example","title":"Simple Example","text":"job.pyk8s-config.yaml <pre><code>from runnable import PythonJob\nfrom examples.common.functions import hello\n\ndef main():\n    job = PythonJob(function=hello)\n    job.execute()  # Configuration via RUNNABLE_CONFIGURATION_FILE\n    return job\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <pre><code>job-executor:\n  type: k8s-job\n  config:\n    pvc_claim_name: \"runnable-storage\"\n    jobSpec:\n      template:\n        spec:\n          container:\n            image: \"my-project:v1.0\"\n</code></pre> <p>Run the Kubernetes job: <pre><code># Push your image to registry first\ndocker push your-registry.com/my-project:v1.0\n\n# Run with Kubernetes executor\nRUNNABLE_CONFIGURATION_FILE=k8s-config.yaml uv run job.py\n</code></pre></p> <p>Pod Execution</p> <p>Your job runs in a Kubernetes pod with isolated resources and access to cluster storage.</p>"},{"location":"production/job-execution/kubernetes/#configuration_reference","title":"Configuration Reference","text":""},{"location":"production/job-execution/kubernetes/#production_kubernetes_k8s-job","title":"Production Kubernetes (k8s-job)","text":"<pre><code>job-executor:\n  type: k8s-job\n  config:\n    pvc_claim_name: \"runnable-storage\"  # Required: PVC for data persistence\n    namespace: \"default\"               # Optional: Kubernetes namespace\n    config_path: null                  # Optional: Path to kubeconfig file\n    mock: false                        # Optional: Skip execution for testing\n    jobSpec:                          # Required: Kubernetes Job specification\n      template:\n        spec:\n          container:\n            image: \"my-project:v1.0\"   # Required: Docker image to use\n            env:                       # Optional: Environment variables\n              - name: \"LOG_LEVEL\"\n                value: \"INFO\"\n            resources:                 # Optional: Resource limits\n              limits:\n                cpu: \"2\"\n                memory: \"4Gi\"\n              requests:\n                cpu: \"1\"\n                memory: \"2Gi\"\n</code></pre>"},{"location":"production/job-execution/kubernetes/#minikube_development_mini-k8s-job","title":"Minikube Development (mini-k8s-job)","text":"<pre><code>job-executor:\n  type: mini-k8s-job\n  config:\n    namespace: \"default\"               # Optional: Kubernetes namespace\n    mock: false                        # Optional: Skip execution for testing\n    jobSpec:                          # Required: Kubernetes Job specification\n      template:\n        spec:\n          container:\n            image: \"my-project:latest\" # Required: Docker image to use\n</code></pre>"},{"location":"production/job-execution/kubernetes/#common_configuration_options","title":"Common Configuration Options","text":"Field Type Required Description <code>pvc_claim_name</code> str Yes (k8s-job) PVC name for data persistence <code>jobSpec.template.spec.container.image</code> str Yes Docker image for job execution <code>namespace</code> str No Kubernetes namespace (default: \"default\") <code>config_path</code> str No Path to kubeconfig file <code>mock</code> bool No Simulate execution without creating job"},{"location":"production/job-execution/kubernetes/#complete_example","title":"Complete Example","text":"python_tasks.pyk8s-job.yaml examples/11-jobs/python_tasks.py<pre><code>from examples.common.functions import hello\nfrom runnable import PythonJob\n\ndef main():\n    job = PythonJob(function=hello)\n    job.execute()\n    return job\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> examples/11-jobs/k8s-job.yaml (simplified)<pre><code>job-executor:\n  type: \"k8s-job\"\n  config:\n    pvc_claim_name: runnable-storage\n    namespace: default\n    jobSpec:\n      template:\n        spec:\n          container:\n            image: your-registry.com/your-project:latest\n</code></pre> <p>Run the example: <pre><code># Push your image to the registry\ndocker push your-registry.com/your-project:latest\n\n# Run with Kubernetes executor\nRUNNABLE_CONFIGURATION_FILE=examples/11-jobs/k8s-job.yaml uv run examples/11-jobs/python_tasks.py\n</code></pre></p>"},{"location":"production/job-execution/kubernetes/#prerequisites","title":"Prerequisites","text":""},{"location":"production/job-execution/kubernetes/#kubernetes_setup","title":"Kubernetes Setup","text":"<p>Verify cluster access: <pre><code>kubectl cluster-info\nkubectl get nodes\n</code></pre></p> <p>Create PVC for production (k8s-job only): runnable-pvc.yaml<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: runnable-storage\nspec:\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 10Gi\n</code></pre></p> <pre><code>kubectl apply -f runnable-pvc.yaml\n</code></pre>"},{"location":"production/job-execution/kubernetes/#troubleshooting","title":"Troubleshooting","text":""},{"location":"production/job-execution/kubernetes/#common_issues","title":"Common Issues","text":"<p>Job Stuck in Pending</p> <p>Problem: Pod doesn't start</p> <p>Solution: <pre><code># Check pod status and events\nkubectl get pods -l job-name=&lt;run-id&gt;\nkubectl describe pod &lt;pod-name&gt;\n</code></pre></p> <p>Image Pull Errors</p> <p>Problem: Cannot pull container image</p> <p>Solution: <pre><code># Verify image exists and is accessible\ndocker pull &lt;your-image&gt;\n\n# Check image pull secrets for private registries\nkubectl get secrets\n</code></pre></p>"},{"location":"production/job-execution/kubernetes/#debug_mode","title":"Debug Mode","text":"<p>Test job configuration without creating actual pods:</p> <pre><code>job-executor:\n  type: k8s-job\n  config:\n    mock: true  # Logs job spec without creating actual job\n    pvc_claim_name: \"runnable-storage\"\n    jobSpec:\n      template:\n        spec:\n          container:\n            image: \"my-project:latest\"\n</code></pre>"},{"location":"production/job-execution/kubernetes/#advanced_features","title":"Advanced Features","text":""},{"location":"production/job-execution/kubernetes/#job_scheduling","title":"Job Scheduling","text":"<p>Schedule jobs to run automatically using Kubernetes CronJobs:</p> <pre><code>job-executor:\n  type: k8s-job\n  config:\n    schedule: \"0 2 * * *\"  # Run daily at 2 AM\n    pvc_claim_name: \"runnable-storage\"\n    jobSpec:\n      template:\n        spec:\n          container:\n            image: \"my-project:v1.0\"\n</code></pre> <p>Learn more: Kubernetes Job Scheduling Guide</p>"},{"location":"production/job-execution/kubernetes/#when_to_use_other_executors","title":"When to Use Other Executors","text":"<p>Consider alternatives when you need:</p> <p>Local Development</p> <p>Local: For simple development without container overhead</p> <p>Local Container: For containerized development without Kubernetes</p> <p>Simpler Infrastructure</p> <p>Local Container: When you need containers but not full Kubernetes orchestration</p> <p>Related: K8s Job Scheduling | Pipeline Argo Workflows | All Job Executors</p>"},{"location":"production/job-execution/local-container/","title":"Local Container Job Execution","text":"<p>Execute jobs in Docker containers on your local machine for environment isolation and consistency - perfect for testing containerized deployments locally.</p> <p>Installation Required</p> <p>Container execution requires the optional Docker dependency: <pre><code>pip install runnable[docker]\n</code></pre></p> <p>Container Setup Made Simple</p> <p>Just build a Docker image from your project root - it automatically includes your code, dependencies, and environment!</p> <pre><code>docker build -t my-project:latest .\n</code></pre>"},{"location":"production/job-execution/local-container/#why_use_container_execution","title":"Why Use Container Execution?","text":"<p>Container Benefits</p> <p>Environment isolation: Clean, reproducible execution environment</p> <ul> <li>\ud83d\udc33 Consistent environments: Same container across different machines</li> <li>\ud83d\udce6 Dependency isolation: Package everything needed in the image</li> <li>\ud83d\udd04 Production parity: Test with the same image you'll deploy</li> <li>\u2705 Reproducible builds: Eliminate \"works on my machine\" issues</li> </ul> <p>Trade-offs</p> <ul> <li>\ud83d\udc33 Container overhead: ~1-2 seconds startup time per job</li> <li>\ud83d\udcca Docker dependency: Requires Docker installation and running daemon</li> <li>\ud83d\udcbe Image management: Need to build and maintain Docker images</li> </ul>"},{"location":"production/job-execution/local-container/#getting_started","title":"Getting Started","text":""},{"location":"production/job-execution/local-container/#basic_configuration","title":"Basic Configuration","text":"<pre><code>job-executor:\n  type: local-container\n  config:\n    docker_image: \"my-project:latest\"\n</code></pre>"},{"location":"production/job-execution/local-container/#simple_example","title":"Simple Example","text":"job.pyconfig.yaml <pre><code>from runnable import PythonJob\nfrom examples.common.functions import hello\n\ndef main():\n    job = PythonJob(function=hello)\n    job.execute()  # Configuration via RUNNABLE_CONFIGURATION_FILE\n    return job\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <pre><code>job-executor:\n  type: local-container\n  config:\n    docker_image: \"my-project:latest\"\n</code></pre> <p>Run the containerized job: <pre><code># Build your image first\ndocker build -t my-project:latest .\n\n# Run with container executor\nRUNNABLE_CONFIGURATION_FILE=config.yaml uv run job.py\n</code></pre></p> <p>Container Isolation</p> <p>Each job runs in a fresh container, giving you clean isolation and consistent environments.</p>"},{"location":"production/job-execution/local-container/#configuration_reference","title":"Configuration Reference","text":"<pre><code>job-executor:\n  type: local-container\n  config:\n    docker_image: \"my-project:latest\"  # Required: Docker image to use\n    auto_remove_container: true        # Optional: Remove containers after execution\n    mock: false                        # Optional: Skip actual execution for testing\n    environment:                       # Optional: Environment variables\n      DATABASE_URL: \"postgresql://localhost/mydb\"\n      API_KEY: \"your-api-key\"\n</code></pre>"},{"location":"production/job-execution/local-container/#configuration_options","title":"Configuration Options","text":"Field Type Default Description <code>docker_image</code> str REQUIRED Docker image to use for execution <code>auto_remove_container</code> bool <code>true</code> Automatically remove container after execution <code>mock</code> bool <code>false</code> Skip execution, simulate success for testing <code>environment</code> dict <code>{}</code> Environment variables to pass to container"},{"location":"production/job-execution/local-container/#environment_variables","title":"Environment Variables","text":"<p>Pass configuration to your containerized jobs:</p> <pre><code>job-executor:\n  type: local-container\n  config:\n    docker_image: \"data-pipeline:latest\"\n    environment:\n      DATABASE_URL: \"postgresql://localhost/analytics\"\n      LOG_LEVEL: \"INFO\"\n      BATCH_SIZE: \"1000\"\n</code></pre>"},{"location":"production/job-execution/local-container/#debugging_configuration","title":"Debugging Configuration","text":"<p>Keep containers for inspection:</p> <pre><code>job-executor:\n  type: local-container\n  config:\n    docker_image: \"my-project:debug\"\n    auto_remove_container: false  # Keep container after execution\n    mock: false                   # Set to true for workflow testing\n</code></pre>"},{"location":"production/job-execution/local-container/#complete_example","title":"Complete Example","text":"python_tasks.pylocal-container.yaml examples/11-jobs/python_tasks.py<pre><code>from examples.common.functions import hello\nfrom runnable import PythonJob\n\ndef main():\n    job = PythonJob(function=hello)\n    job.execute()\n    return job\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> examples/11-jobs/local-container.yaml<pre><code>job-executor:\n  type: \"local-container\"\n  config:\n    docker_image: demo-runnable-m1:latest\n</code></pre> <p>Run the example: <pre><code># Build your image first\ndocker build -t demo-runnable-m1:latest .\n\n# Run with container configuration\nRUNNABLE_CONFIGURATION_FILE=examples/11-jobs/local-container.yaml uv run examples/11-jobs/python_tasks.py\n</code></pre></p>"},{"location":"production/job-execution/local-container/#docker_requirements","title":"Docker Requirements","text":"<p>Ensure Docker is running and build your image:</p> <pre><code># Check Docker is available\ndocker --version\n\n# Build your project image\ndocker build -t my-project:latest .\n</code></pre>"},{"location":"production/job-execution/local-container/#simple_dockerfile_example","title":"Simple Dockerfile Example","text":"<p>Basic Dockerfile</p> <pre><code>FROM python:3.11\n\nWORKDIR /app\n\n# Copy and install dependencies\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\n# Copy project code\nCOPY . .\n</code></pre> <p>Build and use: <pre><code>docker build -t my-project:latest .\n</code></pre></p>"},{"location":"production/job-execution/local-container/#troubleshooting","title":"Troubleshooting","text":""},{"location":"production/job-execution/local-container/#common_issues","title":"Common Issues","text":"<p>Module Import Errors</p> <p>Problem: <code>ModuleNotFoundError: No module named 'my_project'</code></p> <p>Solution: Ensure your container environment matches local setup:</p> <ul> <li>Check Python version: <code>python --version</code></li> <li>Verify dependencies are installed in container</li> <li>Test imports: <code>docker run --rm my-project:latest python -c \"import my_module\"</code></li> </ul> <p>Container Exits Immediately</p> <p>Problem: Container stops without running the job</p> <p>Solution:</p> <ul> <li>Verify Docker image has required dependencies</li> <li>Test basic functionality: <code>docker run --rm my-project:latest python -c \"from runnable import PythonJob\"</code></li> </ul>"},{"location":"production/job-execution/local-container/#debug_mode","title":"Debug Mode","text":"<p>Keep containers for inspection:</p> <pre><code>job-executor:\n  type: local-container\n  config:\n    docker_image: \"my-project:latest\"\n    auto_remove_container: false  # Container stays after execution\n</code></pre> <pre><code># List containers after job runs\ndocker ps -a\n\n# Inspect logs\ndocker logs &lt;container-id&gt;\n\n# Interactive shell\ndocker exec -it &lt;container-id&gt; /bin/bash\n</code></pre>"},{"location":"production/job-execution/local-container/#when_to_upgrade","title":"When to Upgrade","text":"<p>Consider other executors when you need:</p> <p>No Docker Dependency</p> <p>Local: For simple development without container overhead</p> <p>Production Orchestration</p> <p>Kubernetes: For production workloads with resource management and scaling</p> <p>Related: Pipeline Container Execution | All Job Executors</p>"},{"location":"production/job-execution/local/","title":"Local Job Execution","text":"<p>Execute jobs directly on your local machine without containerization or orchestration - perfect for development, experimentation, and simple task execution.</p>"},{"location":"production/job-execution/local/#why_use_local_execution","title":"Why Use Local Execution?","text":"<p>Development Benefits</p> <p>Fast iteration: No container overhead or deployment complexity</p> <ul> <li>\u26a1 Instant execution: Direct code execution on your machine</li> <li>\ud83d\udee0\ufe0f Easy debugging: Full access to development tools and debuggers</li> <li>\ud83d\udcc1 Direct file access: No mount points or volume mapping needed</li> <li>\ud83d\udd04 Quick changes: Edit and run immediately</li> </ul> <p>Trade-offs</p> <ul> <li>\ud83c\udfe0 Local only: Runs on your development machine</li> <li>\ud83d\udd17 No isolation: Shares your system environment</li> <li>\ud83d\udcbb Single machine: Limited to local compute resources</li> </ul>"},{"location":"production/job-execution/local/#getting_started","title":"Getting Started","text":""},{"location":"production/job-execution/local/#simple_example","title":"Simple Example","text":"job.pyRun It <pre><code>from runnable import PythonJob\nfrom examples.common.functions import hello\n\ndef main():\n    job = PythonJob(function=hello)\n    job.execute()  # Uses local executor by default\n    return job\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <pre><code># No configuration needed - local is the default\nuv run job.py\n</code></pre> <p>Result: Your job runs directly on your machine using the current Python environment.</p>"},{"location":"production/job-execution/local/#configuration","title":"Configuration","text":""},{"location":"production/job-execution/local/#no_configuration_required","title":"No Configuration Required","text":"<p>Local execution is the default - no configuration file needed:</p> <pre><code>def main():\n    job = PythonJob(function=my_function)\n    job.execute()  # Automatically uses local executor\n    return job\n</code></pre>"},{"location":"production/job-execution/local/#optional_configuration","title":"Optional Configuration","text":"<p>When you need explicit configuration or testing options:</p> local-job.yaml<pre><code>job-executor:\n  type: local\n  config:\n    mock: false  # Set to true for testing workflow logic\n</code></pre> <p>Recommended Usage (via environment variable): <pre><code>export RUNNABLE_CONFIGURATION_FILE=local-job.yaml\nuv run my_job.py\n</code></pre></p> <p>Alternative (inline in code): <pre><code>def main():\n    job = PythonJob(function=my_function)\n    job.execute(configuration_file=\"local-job.yaml\")\n    return job\n</code></pre></p>"},{"location":"production/job-execution/local/#configuration_reference","title":"Configuration Reference","text":"<pre><code>job-executor:\n  type: local\n  config:\n    mock: false  # Optional: Skip actual execution for testing\n</code></pre>"},{"location":"production/job-execution/local/#configuration_options","title":"Configuration Options","text":"Field Type Default Description <code>mock</code> bool <code>false</code> Skip actual execution, simulate success for testing <p>Mock Execution for Testing</p> <p>Test your job workflow without running actual code:</p> <pre><code>job-executor:\n  type: local\n  config:\n    mock: true  # Simulate execution without running code\n</code></pre> <p>Use cases:</p> <ul> <li>Testing job workflow logic</li> <li>Validating job configuration</li> <li>Dry-run scenarios during development</li> </ul>"},{"location":"production/job-execution/local/#complete_example","title":"Complete Example","text":"python_tasks.pyWith Configuration examples/11-jobs/python_tasks.py<pre><code>from examples.common.functions import hello\nfrom runnable import PythonJob\n\ndef main():\n    job = PythonJob(function=hello)\n    job.execute()\n    return job\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> local-config.yaml<pre><code>job-executor:\n  type: local\n  config:\n    mock: false\n</code></pre> <p>Run the example: <pre><code># No config needed - runs locally by default\nuv run examples/11-jobs/python_tasks.py\n\n# Or with explicit configuration\nRUNNABLE_CONFIGURATION_FILE=local-config.yaml uv run examples/11-jobs/python_tasks.py\n</code></pre></p>"},{"location":"production/job-execution/local/#environment_variables","title":"Environment Variables","text":"<p>Local jobs have direct access to your environment variables:</p> <pre><code>import os\nfrom runnable import PythonJob\n\ndef access_environment():\n    db_url = os.getenv('DATABASE_URL', 'sqlite://default.db')\n    return f\"Using database: {db_url}\"\n\ndef main():\n    job = PythonJob(function=access_environment)\n    job.execute()\n    return job\n</code></pre>"},{"location":"production/job-execution/local/#file_access","title":"File Access","text":"<p>Direct access to local filesystem without mount points:</p> <pre><code>from runnable import PythonJob\n\ndef process_file():\n    with open('data/input.csv', 'r') as f:\n        # Process file directly from local filesystem\n        return len(f.readlines())\n\ndef main():\n    job = PythonJob(function=process_file)\n    job.execute()\n    return job\n</code></pre>"},{"location":"production/job-execution/local/#performance_characteristics","title":"Performance Characteristics","text":"<p>Local Execution Benefits</p> <ul> <li>Fast startup: No container or orchestration overhead</li> <li>Direct I/O: No network or mount overhead for file access</li> <li>Full system access: All local resources and tools available</li> <li>Instant debugging: Use your IDE debugger directly</li> </ul> <p>Limitations</p> <ul> <li>No isolation: Shares environment with your system</li> <li>Single machine: Limited to local compute resources</li> <li>Environment drift: Different behavior across development machines</li> </ul>"},{"location":"production/job-execution/local/#when_to_upgrade","title":"When to Upgrade","text":"<p>Consider other executors when you need:</p> <p>Environment Consistency</p> <p>Local Container: For isolated, reproducible environments</p> <p>Production Deployment</p> <p>Kubernetes: For production workloads with resource management</p> <p>Related: Pipeline Local Execution | All Job Executors</p>"},{"location":"production/job-execution/overview/","title":"Job Execution Overview","text":"<p>Execute individual tasks across different environments with flexible runtime configurations.</p>"},{"location":"production/job-execution/overview/#jobs_vs_pipelines","title":"Jobs vs Pipelines","text":"<p>Jobs: Execute single tasks in isolation</p> <pre><code>from runnable import PythonJob\nfrom examples.common.functions import hello\n\ndef main():\n    job = PythonJob(function=hello)\n    job.execute()  # Single task execution\n    return job\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Pipelines: Orchestrate multiple connected tasks</p> <pre><code>from runnable import Pipeline, PythonTask\nfrom examples.common.functions import hello\n\ndef main():\n    pipeline = Pipeline(steps=[\n        PythonTask(function=hello, name=\"task1\"),\n        PythonTask(function=hello, name=\"task2\")\n    ])\n    pipeline.execute()  # Multi-task workflow\n    return pipeline\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"production/job-execution/overview/#available_job_executors","title":"Available Job Executors","text":"Executor Use Case Environment Execution Model Local Development Local machine Direct execution Local Container Isolated development Docker containers Containerized execution Kubernetes Production Kubernetes cluster Distributed execution"},{"location":"production/job-execution/overview/#configuration_pattern","title":"Configuration Pattern","text":"<p>All job executors use this configuration pattern:</p> config.yaml<pre><code>job-executor:\n  type: \"[executor-type]\"\n  config:\n    # executor-specific options\n</code></pre> <p>Recommended Usage (via environment variable): <pre><code># Keep configuration separate from code\nexport RUNNABLE_CONFIGURATION_FILE=config.yaml\nuv run my_job.py\n\n# Or inline for different environments\nRUNNABLE_CONFIGURATION_FILE=production.yaml uv run my_job.py\n</code></pre></p> <p>Alternative (inline in code): <pre><code>job.execute(configuration_file=\"config.yaml\")\n</code></pre></p> <p>Examples Directory</p> <p>Complete working examples are available in <code>examples/11-jobs/</code>. Each example includes both Python code and YAML configuration files you can run immediately.</p>"},{"location":"production/job-execution/overview/#custom_job_executors","title":"Custom Job Executors","text":"<p>Need to run jobs on your unique infrastructure? Runnable's plugin architecture makes it simple to build custom job executors for any compute platform.</p> <p>No Vendor Lock-in</p> <p>Your infrastructure, your way: Execute jobs on AWS Batch, Azure Container Apps, HPC clusters, or any custom compute platform.</p> <ul> <li>\ud83d\udd0c Cloud batch services: AWS Batch, Azure Container Apps, Google Cloud Run Jobs</li> <li>\ud83c\udfe2 HPC integration: Slurm, PBS, custom job schedulers</li> <li>\ud83c\udfaf Specialized hardware: GPUs, TPUs, edge devices</li> <li>\ud83d\udd10 Enterprise platforms: Custom orchestrators, proprietary compute services</li> </ul>"},{"location":"production/job-execution/overview/#building_custom_job_executors","title":"Building Custom Job Executors","text":"<p>Learn how to create production-ready custom job executors with our comprehensive development guide:</p> <p>\ud83d\udcd6 Custom Job Executors Development Guide</p> <p>The guide covers:</p> <ul> <li>AWS Batch integration example showing key integration patterns</li> <li>Job submission &amp; monitoring workflow that works with any compute platform</li> <li>Plugin registration and configuration for seamless integration with Runnable</li> <li>Testing and debugging strategies for custom executors</li> </ul> <p>Quick Example</p> <p>Create a custom executor in just 3 steps:</p> <ol> <li>Implement the interface by extending <code>GenericJobExecutor</code></li> <li>Register via entry point in your <code>pyproject.toml</code></li> <li>Configure via YAML for your users</li> </ol> <pre><code>from extensions.job_executor import GenericJobExecutor\n\nclass MyCustomJobExecutor(GenericJobExecutor):\n    service_name: str = \"my-platform\"\n\n    def submit_job(self, job, catalog_settings=None):\n        # Your platform integration here\n        pass\n</code></pre> <p>Ready to build? See the full development guide for implementation patterns and examples.</p>"},{"location":"production/job-execution/overview/#choosing_the_right_executor","title":"Choosing the Right Executor","text":""},{"location":"production/job-execution/overview/#development_testing","title":"Development &amp; Testing","text":"<ul> <li>Local: Quick development, debugging, simple tasks</li> <li>Local Container: Isolated development, dependency consistency</li> </ul>"},{"location":"production/job-execution/overview/#production_deployment","title":"Production Deployment","text":"<ul> <li>Kubernetes: Production scale, resource management, distributed execution</li> </ul>"},{"location":"production/job-execution/overview/#when_to_use_job_execution","title":"When to Use Job Execution","text":"<p>Choose job execution when you need:</p> <ul> <li>Single task execution without workflow orchestration</li> <li>Independent tasks that don't share data with other steps</li> <li>Simple execution without complex dependencies</li> </ul>"},{"location":"production/job-execution/overview/#when_to_use_pipeline_execution_instead","title":"When to Use Pipeline Execution Instead","text":"<p>For multi-task workflows, consider Pipeline Execution:</p> <ul> <li>Multi-step workflows with dependencies between tasks</li> <li>Cross-step data passing via parameters or catalog</li> <li>Complex orchestration with parallel branches or conditional logic</li> </ul>"},{"location":"production/job-execution/overview/#next_steps","title":"Next Steps","text":"<ol> <li>Start simple: Begin with Local execution for development</li> <li>Add isolation: Move to Local Container for consistent environments</li> <li>Scale up: Deploy with Kubernetes for production workloads</li> </ol> <p>Multi-Task Workflows</p> <p>For orchestrating multiple connected tasks, see Pipeline Execution which provides workflow management and cross-step data passing.</p>"},{"location":"production/pipeline-execution/argo/","title":"Argo Workflows Pipeline Execution","text":"<p>Scale your pipelines to the cloud with distributed parallel execution using Argo Workflows - the most powerful execution environment for production ML workflows.</p> <p>No Additional Dependencies</p> <p>Argo Workflows execution works with the base runnable installation - no additional dependencies required: <pre><code>pip install runnable\n</code></pre></p> <p>Note: The <code>runnable[k8s]</code> dependency is only needed for Kubernetes job executors (<code>k8s-job</code>, <code>mini-k8s-job</code>), not for Argo Workflows pipeline execution.</p> <p>Distributed Parallel Execution</p> <p>Cloud-scale parallelization! Argo runs your <code>parallel</code> and <code>map</code> nodes across multiple Kubernetes pods, providing distributed computing power and elastic scaling beyond what's possible on a single machine.</p>"},{"location":"production/pipeline-execution/argo/#getting_started","title":"Getting Started","text":"<p>Simple Cloud Setup</p> <p>Runnable generates standard Argo workflow YAML - your infrastructure team can deploy it using existing Kubernetes and Argo tooling!</p>"},{"location":"production/pipeline-execution/argo/#basic_configuration","title":"Basic Configuration","text":"<pre><code>pipeline-executor:\n  type: argo\n  config:\n    image: \"my-pipeline:latest\"  # Your containerized pipeline\n    output_file: \"workflow.yaml\"  # Generated Argo workflow\n</code></pre>"},{"location":"production/pipeline-execution/argo/#simple_example","title":"Simple Example","text":"pipeline.pyconfig.yaml <pre><code>from runnable import Pipeline, PythonTask, Parallel\n\ndef process_data_a():\n    print(\"Processing dataset A...\")\n    # Your ML logic here\n    return {\"accuracy\": 0.95}\n\ndef process_data_b():\n    print(\"Processing dataset B...\")\n    # Your ML logic here\n    return {\"accuracy\": 0.92}\n\ndef combine_results(results_a, results_b):\n    print(f\"A: {results_a['accuracy']}, B: {results_b['accuracy']}\")\n    return {\"best\": max(results_a['accuracy'], results_b['accuracy'])}\n\ndef main():\n    # These run in parallel in Argo!\n    parallel_processing = Parallel(\n        name=\"process_datasets\",\n        branches={\n            \"dataset_a\": PythonTask(function=process_data_a, name=\"process_a\"),\n            \"dataset_b\": PythonTask(function=process_data_b, name=\"process_b\")\n        }\n    )\n\n    combine_task = PythonTask(\n        function=combine_results,\n        name=\"combine\"\n    )\n\n    pipeline = Pipeline(steps=[parallel_processing, combine_task])\n    pipeline.execute()  # Generates workflow.yaml\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <pre><code>pipeline-executor:\n  type: argo\n  config:\n    image: \"my-pipeline:latest\"\n    output_file: \"workflow.yaml\"\n</code></pre> <p>Generate and deploy the workflow: <pre><code># Generate Argo workflow\nRUNNABLE_CONFIGURATION_FILE=config.yaml uv run pipeline.py\n\n# Deploy to your Kubernetes cluster\nkubectl apply -f workflow.yaml\n</code></pre></p> <p>Parallel Execution</p> <p>In this example, <code>process_data_a</code> and <code>process_data_b</code> run simultaneously on different Kubernetes pods, then <code>combine_results</code> runs after both complete.</p>"},{"location":"production/pipeline-execution/argo/#why_use_argo_workflows","title":"Why Use Argo Workflows?","text":"<p>Cloud-Scale Benefits</p> <p>Distributed parallelization: <code>parallel</code> and <code>map</code> nodes run across multiple pods</p> <ul> <li>\u26a1 Faster pipelines: Utilize multiple machines and CPU cores across your cluster</li> <li>\ud83d\udd04 Elastic scaling: Kubernetes automatically manages resources and scaling</li> <li>\ud83c\udfd7\ufe0f Production ready: Battle-tested in enterprise environments</li> <li>\ud83d\udcca Rich monitoring: Native Kubernetes and Argo UI integration</li> </ul> <p>Trade-offs</p> <ul> <li>\u2699\ufe0f Infrastructure requirement: Needs Kubernetes cluster with Argo installed</li> <li>\ud83d\udc33 Container overhead: Each task runs in separate pods</li> <li>\ud83d\udd27 Setup complexity: More moving parts than local executors</li> </ul>"},{"location":"production/pipeline-execution/argo/#advanced_features","title":"Advanced Features","text":""},{"location":"production/pipeline-execution/argo/#dynamic_parameters","title":"Dynamic Parameters","text":"<p>Runtime Parameter Control</p> <p>Make your workflows configurable by exposing parameters to the Argo UI:</p> <pre><code>pipeline-executor:\n  type: argo\n  config:\n    image: \"my-pipeline:latest\"\n    output_file: \"workflow.yaml\"\n    expose_parameters_as_inputs: true  # Enable parameter inputs\n</code></pre> <p>Now parameters become configurable in the Argo UI at runtime!</p>"},{"location":"production/pipeline-execution/argo/#storage_and_persistence","title":"Storage and Persistence","text":"<p>Shared Storage Between Tasks</p> <p>Use persistent volumes to share data between tasks:</p> <pre><code>pipeline-executor:\n  type: argo\n  config:\n    image: \"my-pipeline:latest\"\n    persistent_volumes:\n      - name: shared-data\n        mount_path: /shared\n</code></pre>"},{"location":"production/pipeline-execution/argo/#kubernetes_secrets","title":"Kubernetes Secrets","text":"<p>Secure Credential Management</p> <p>Access cluster secrets in your pipeline tasks:</p> <pre><code>pipeline-executor:\n  type: argo\n  config:\n    image: \"my-pipeline:latest\"\n    secrets_from_k8s:\n      - environment_variable: DB_CONNECTION\n        secret_name: database-credentials\n        secret_key: connection_string\n</code></pre>"},{"location":"production/pipeline-execution/argo/#resource_management","title":"Resource Management","text":"<p>Custom Resource Requirements</p> <p>Different tasks can have different compute requirements:</p> <pre><code>pipeline-executor:\n  type: argo\n  config:\n    image: \"my-pipeline:latest\"\n    # Default resources\n    resources:\n      requests:\n        memory: \"1Gi\"\n        cpu: \"250m\"\n      limits:\n        memory: \"2Gi\"\n        cpu: \"500m\"\n    # Step-specific overrides\n    overrides:\n      gpu_training:\n        resources:\n          requests:\n            memory: \"8Gi\"\n            cpu: \"2\"\n            nvidia.com/gpu: \"1\"\n          limits:\n            memory: \"16Gi\"\n            cpu: \"4\"\n            nvidia.com/gpu: \"1\"\n</code></pre> <p>Then use the override in your pipeline:</p> <pre><code>gpu_task = PythonTask(\n    function=train_model,\n    name=\"train_with_gpu\",\n    overrides={\"argo\": \"gpu_training\"}\n)\n</code></pre>"},{"location":"production/pipeline-execution/argo/#parallelism_control","title":"Parallelism Control","text":"<p>Manage Resource Usage</p> <p>Control how many tasks run simultaneously to avoid overwhelming your cluster:</p> <pre><code>pipeline-executor:\n  type: argo\n  config:\n    image: \"my-pipeline:latest\"\n    parallelism: 5  # Max 5 tasks running at once\n    overrides:\n      sequential_processing:\n        parallelism: 1  # Force sequential execution\n</code></pre>"},{"location":"production/pipeline-execution/argo/#node_selection","title":"Node Selection","text":"<p>Target Specific Nodes</p> <p>Run tasks on specific node types (e.g., GPU nodes):</p> <pre><code>pipeline-executor:\n  type: argo\n  config:\n    image: \"my-pipeline:latest\"\n    overrides:\n      gpu_nodes:\n        node_selector:\n          accelerator: \"nvidia-tesla-k80\"\n        tolerations:\n          - key: \"gpu\"\n            operator: \"Equal\"\n            value: \"true\"\n            effect: \"NoSchedule\"\n</code></pre>"},{"location":"production/pipeline-execution/argo/#scheduled_workflows_cronworkflow","title":"Scheduled Workflows (CronWorkflow)","text":"<p>Run Pipelines on a Schedule</p> <p>Use <code>cron_schedule</code> to generate an Argo CronWorkflow instead of a regular Workflow. This enables recurring pipeline execution similar to Kubernetes CronJobs:</p> <pre><code>pipeline-executor:\n  type: argo\n  config:\n    image: \"my-pipeline:latest\"\n    output_file: \"cron-workflow.yaml\"\n    cron_schedule:\n      schedules:\n        - \"0 0 * * *\"      # Daily at midnight\n      timezone: \"UTC\"      # Optional timezone\n</code></pre> <p>Multiple schedules are supported:</p> <pre><code>cron_schedule:\n  schedules:\n    - \"0 6 * * *\"   # Daily at 6 AM\n    - \"0 18 * * *\"  # Daily at 6 PM\n  timezone: \"America/New_York\"\n</code></pre> <p>Cron Expression Format</p> <p>Standard cron format: <code>minute hour day-of-month month day-of-week</code></p> <ul> <li><code>0 0 * * *</code> - Daily at midnight</li> <li><code>0 */6 * * *</code> - Every 6 hours</li> <li><code>0 0 * * 0</code> - Weekly on Sunday at midnight</li> <li><code>0 0 1 * *</code> - Monthly on the 1st at midnight</li> </ul> <p>Workflow vs CronWorkflow</p> <p>When <code>cron_schedule</code> is configured:</p> <ul> <li>Output changes from <code>kind: Workflow</code> to <code>kind: CronWorkflow</code></li> <li><code>metadata.generateName</code> becomes <code>metadata.name</code> (CronWorkflows require a fixed name)</li> <li>The workflow spec is wrapped inside <code>workflowSpec</code></li> </ul> <p>Deploy with: <code>kubectl apply -f cron-workflow.yaml</code></p>"},{"location":"production/pipeline-execution/argo/#configuration_reference","title":"Configuration Reference","text":"<p>Executes the pipeline using Argo Workflows.</p> <p>The defaults configuration is kept similar to the Argo Workflow spec.</p> <p>Configuration:</p> <pre><code>pipeline-executor:\n  type: argo\n  config:\n    pvc_for_runnable: \"my-pvc\"\n    custom_volumes:\n      - mount_path: \"/tmp\"\n        persistent_volume_claim:\n          claim_name: \"my-pvc\"\n          read_only: false/true\n    expose_parameters_as_inputs: true/false\n    configmap_cache_name: \"my-cache-name\"  # Optional: defaults to runnable-xxxxxx\n    secrets_from_k8s:\n      - key1\n      - key2\n      - ...\n    output_file: \"argo-pipeline.yaml\"\n    log_level: \"DEBUG\"/\"INFO\"/\"WARNING\"/\"ERROR\"/\"CRITICAL\"\n    cron_schedule:  # Optional: generates CronWorkflow instead of Workflow\n      schedules:\n        - \"0 0 * * *\"  # Cron expressions\n      timezone: \"UTC\"  # Optional timezone\n    defaults:\n      image: \"my-image\"\n      activeDeadlineSeconds: 86400\n      failFast: true\n      nodeSelector:\n        label: value\n      parallelism: 1\n      retryStrategy:\n        backoff:\n        duration: \"2m\"\n        factor: 2\n        maxDuration: \"1h\"\n        limit: 0\n        retryPolicy: \"Always\"\n      timeout: \"1h\"\n      tolerations:\n      imagePullPolicy: \"Always\"/\"IfNotPresent\"/\"Never\"\n      resources:\n        limits:\n          memory: \"1Gi\"\n          cpu: \"250m\"\n          gpu: 0\n        requests:\n          memory: \"1Gi\"\n          cpu: \"250m\"\n      env:\n        - name: \"MY_ENV\"\n        value: \"my-value\"\n        - name: secret_env\n        secretName: \"my-secret\"\n        secretKey: \"my-key\"\n    overrides:\n      key1:\n        ... similar structure to defaults\n\n    argoWorkflow:\n      metadata:\n        annotations:\n          key1: value1\n          key2: value2\n        generateName: \"my-workflow\"\n        labels:\n          key1: value1\n</code></pre> <p>As of now, <code>runnable</code> needs a pvc to store the logs and the catalog; provided by <code>pvc_for_runnable</code>. - <code>custom_volumes</code> can be used to mount additional volumes to the container.</p> <ul> <li><code>expose_parameters_as_inputs</code> can be used to expose the initial parameters as inputs to the workflow.</li> <li><code>secrets_from_k8s</code> can be used to expose the secrets from the k8s secret store.</li> <li><code>output_file</code> is the file where the argo pipeline will be dumped.</li> <li><code>log_level</code> is the log level for the containers.</li> <li><code>cron_schedule</code> generates an Argo CronWorkflow instead of a regular Workflow for scheduled execution.</li> <li><code>defaults</code> is the default configuration for all the containers.</li> </ul>"},{"location":"production/pipeline-execution/argo/#production_considerations","title":"Production Considerations","text":"<p>Infrastructure Requirements</p> <p>Before using Argo: Ensure your cluster has Argo Workflows installed and configured (Argo can run on Kubernetes or standalone)</p> <p>Service Compatibility</p> <p>Storage: Use shared storage (persistent volumes) for <code>catalog</code> and <code>run_log_store</code> - the <code>buffered</code> run log store won't work across pods</p> <p>Secrets: Use Kubernetes secrets via <code>secrets_from_k8s</code> rather than <code>.env</code> files</p>"},{"location":"production/pipeline-execution/argo/#complete_production_example","title":"Complete Production Example","text":"<p>Full Configuration with Best Practices</p> <pre><code># production-argo-config.yaml\npipeline-executor:\n  type: argo\n  config:\n    # Core settings\n    image: \"my-pipeline:v1.2.3\"\n    output_file: \"workflow.yaml\"\n\n    # Runtime parameters\n    expose_parameters_as_inputs: true\n\n    # Storage\n    persistent_volumes:\n      - name: shared-storage\n        mount_path: /shared\n\n    # Security\n    secrets_from_k8s:\n      - environment_variable: DB_CONNECTION\n        secret_name: database-credentials\n        secret_key: connection_string\n\n    # Resource management\n    parallelism: 10\n    resources:\n      requests:\n        memory: \"2Gi\"\n        cpu: \"500m\"\n      limits:\n        memory: \"4Gi\"\n        cpu: \"1\"\n\n    # Task-specific overrides\n    overrides:\n      gpu_training:\n        resources:\n          requests:\n            nvidia.com/gpu: \"1\"\n            memory: \"8Gi\"\n            cpu: \"2\"\n          limits:\n            nvidia.com/gpu: \"1\"\n            memory: \"16Gi\"\n            cpu: \"4\"\n        node_selector:\n          accelerator: \"nvidia-tesla-v100\"\n\n      lightweight_tasks:\n        resources:\n          requests:\n            memory: \"512Mi\"\n            cpu: \"100m\"\n          limits:\n            memory: \"1Gi\"\n            cpu: \"200m\"\n\n# Supporting services\nrun_log_store:\n  type: file-system\n  config:\n    log_folder: /shared/logs\n\ncatalog_handler:\n  type: file-system\n  config:\n    catalog_location: /shared/data\n</code></pre>"},{"location":"production/pipeline-execution/argo/#deployment_workflow","title":"Deployment Workflow","text":"<ol> <li>Generate workflow: <code>RUNNABLE_CONFIGURATION_FILE=production-argo-config.yaml uv run my_pipeline.py</code></li> <li>Review generated YAML: Check <code>workflow.yaml</code> for correctness</li> <li>Deploy to cluster: <code>kubectl apply -f workflow.yaml</code></li> <li>Monitor execution: Use Argo UI or <code>kubectl</code> to track progress</li> </ol> <p>CI/CD Integration</p> <p>In production, integrate this into your CI/CD pipeline to automatically generate and deploy workflows when your pipeline code changes.</p>"},{"location":"production/pipeline-execution/argo/#when_to_use_argo_workflows","title":"When to Use Argo Workflows","text":"<p>Choose Argo When</p> <ul> <li>Need distributed parallel execution across multiple machines</li> <li>Running production ML workloads at scale</li> <li>Want elastic resource management (auto-scaling)</li> <li>Have Kubernetes infrastructure available</li> <li>Need specialized compute for different tasks (CPUs, GPUs, memory)</li> <li>Require more parallelism than a single machine can provide</li> </ul> <p>Use Local Container When</p> <ul> <li>Testing container-based pipelines before cloud deployment</li> <li>Single-machine parallel execution is sufficient</li> <li>Want simpler setup and debugging</li> <li>Don't need distributed computing resources</li> </ul> <p>Use Local Executor When</p> <ul> <li>Development and experimentation</li> <li>All tasks use the same environment</li> <li>Want fastest possible development iteration</li> </ul>"},{"location":"production/pipeline-execution/argo/#advanced_complex_nested_workflows","title":"Advanced: Complex Nested Workflows","text":"<p>Nested Pipeline Support</p> <p>Runnable supports deeply nested workflows with <code>Map</code> inside <code>Parallel</code> inside <code>Map</code> structures. Argo handles the complexity automatically - you just write simple Python pipeline code and Runnable generates the appropriate workflow DAGs.</p>"},{"location":"production/pipeline-execution/custom-pipeline-executors/","title":"Building Custom Pipeline Executors","text":"<p>Execute pipelines on any orchestration platform by understanding the two fundamental orchestration patterns.</p>"},{"location":"production/pipeline-execution/custom-pipeline-executors/#the_two_orchestration_patterns","title":"The Two Orchestration Patterns","text":"<p>Pipeline executors handle DAG execution in two fundamentally different ways:</p>"},{"location":"production/pipeline-execution/custom-pipeline-executors/#pattern_1_dag_traversal_local_local-container","title":"Pattern 1: DAG Traversal (Local, Local-Container)","text":"<p>How it works:</p> <ul> <li>Runnable traverses the DAG in Python</li> <li>For each node: Calls <code>trigger_node_execution()</code></li> <li>Node execution happens locally or via individual job submission</li> </ul> <p>Use for: Simple platforms that submit individual jobs</p>"},{"location":"production/pipeline-execution/custom-pipeline-executors/#pattern_2_dag_transpilation_argo_workflows","title":"Pattern 2: DAG Transpilation (Argo Workflows)","text":"<p>How it works:</p> <ul> <li>Runnable converts the entire DAG into the platform's native workflow format</li> <li>The platform handles DAG traversal (Argo's workflow engine, Airflow scheduler, etc.)</li> <li>Individual nodes use CLI pattern: <code>runnable execute-single-node ...</code></li> </ul> <p>Use for: Platforms with native workflow capabilities (Argo, Airflow, Prefect)</p>"},{"location":"production/pipeline-execution/custom-pipeline-executors/#implementation_examples","title":"Implementation Examples","text":""},{"location":"production/pipeline-execution/custom-pipeline-executors/#pattern_1_dag_traversal_local_executor","title":"Pattern 1: DAG Traversal (Local Executor)","text":"<pre><code>class LocalExecutor(GenericPipelineExecutor):\n    service_name: str = \"local\"\n\n    def trigger_node_execution(self, node, map_variable=None):\n        \"\"\"Called for each node - execute directly\"\"\"\n        # Direct execution using base class\n        self.execute_node(node=node, map_variable=map_variable)\n\n    def execute_node(self, node, map_variable=None):\n        \"\"\"Execute the node directly in current process\"\"\"\n        self._execute_node(node=node, map_variable=map_variable)\n</code></pre> <p>Key insight: Runnable traverses the DAG, calls <code>trigger_node_execution()</code> for each node, which directly executes the task.</p>"},{"location":"production/pipeline-execution/custom-pipeline-executors/#pattern_2_dag_transpilation_argo_executor","title":"Pattern 2: DAG Transpilation (Argo Executor)","text":"<pre><code>class ArgoExecutor(GenericPipelineExecutor):\n    service_name: str = \"argo\"\n\n    def execute_from_graph(self, dag, map_variable=None):\n        \"\"\"Convert entire DAG to Argo WorkflowTemplate\"\"\"\n        # Transpile runnable DAG to Argo workflow YAML\n        argo_workflow = self._transpile_dag_to_argo_workflow(dag)\n        # Submit to Kubernetes - Argo handles the DAG traversal\n        self._submit_argo_workflow(argo_workflow)\n\n    def trigger_node_execution(self, node, map_variable=None):\n        \"\"\"This runs INSIDE Argo pods via CLI commands\"\"\"\n        # Called by: runnable execute-single-node &lt;node-name&gt;\n        # Set up storage access for the pod environment\n        self._setup_pod_storage_access()\n        # Execute the node\n        self._execute_node(node=node, map_variable=map_variable)\n</code></pre> <p>Key insight: Argo's workflow engine traverses the DAG, calls CLI commands that invoke <code>trigger_node_execution()</code> in each pod.</p>"},{"location":"production/pipeline-execution/custom-pipeline-executors/#the_critical_issue_storage_access","title":"The Critical Issue: Storage Access","text":"<p>Same issue as job executors: Run logs and catalog must be accessible in the execution environment.</p>"},{"location":"production/pipeline-execution/custom-pipeline-executors/#local_execution_direct_access","title":"Local Execution: Direct Access","text":"<pre><code>class LocalExecutor(GenericPipelineExecutor):\n    def trigger_node_execution(self, node, map_variable=None):\n        # Storage accessible locally - proceed directly\n        self._execute_node(node=node, map_variable=map_variable)\n</code></pre>"},{"location":"production/pipeline-execution/custom-pipeline-executors/#remote_execution_volume_mounting","title":"Remote Execution: Volume Mounting","text":"<p>Local-Container Pattern - Mount host directories into containers: <pre><code>class LocalContainerExecutor(GenericPipelineExecutor):\n    def trigger_node_execution(self, node, map_variable=None):\n        # Mount host storage into container\n        self._mount_volumes()\n        # Run CLI command in container\n        command = f\"runnable execute-single-node {node.name}\"\n        self._run_in_container(command)\n\n    def _mount_volumes(self):\n        # Map host paths to container paths\n        if self._context.run_log_store.service_name == \"file-system\":\n            host_logs = self._context.run_log_store.log_folder\n            self._volumes[host_logs] = {\"bind\": \"/tmp/run_logs/\", \"mode\": \"rw\"}\n\n        if self._context.catalog.service_name == \"file-system\":\n            host_catalog = self._context.catalog.catalog_location\n            self._volumes[host_catalog] = {\"bind\": \"/tmp/catalog/\", \"mode\": \"rw\"}\n</code></pre></p> <p>Argo/K8s Pattern - Use PersistentVolumeClaims: <pre><code>class ArgoExecutor(GenericPipelineExecutor):\n    def _transpile_dag_to_argo_workflow(self, dag):\n        # Add PVC mounts to every pod in the workflow\n        workflow_spec = {\n            \"spec\": {\n                \"volumes\": [\n                    {\"name\": \"run-logs\", \"persistentVolumeClaim\": {\"claimName\": \"runnable-logs-pvc\"}},\n                    {\"name\": \"catalog\", \"persistentVolumeClaim\": {\"claimName\": \"runnable-catalog-pvc\"}}\n                ],\n                \"templates\": [\n                    {\n                        \"container\": {\n                            \"volumeMounts\": [\n                                {\"name\": \"run-logs\", \"mountPath\": \"/tmp/run_logs/\"},\n                                {\"name\": \"catalog\", \"mountPath\": \"/tmp/catalog/\"}\n                            ]\n                        }\n                    }\n                ]\n            }\n        }\n\n    def trigger_node_execution(self, node, map_variable=None):\n        # This runs in Argo pod - update context to use mounted paths\n        self._use_mounted_storage()\n        self._execute_node(node=node, map_variable=map_variable)\n\n    def _use_mounted_storage(self):\n        # Point to PVC mount paths\n        if self._context.run_log_store.service_name == \"file-system\":\n            self._context.run_log_store.log_folder = \"/tmp/run_logs/\"\n        if self._context.catalog.service_name == \"file-system\":\n            self._context.catalog.catalog_location = \"/tmp/catalog/\"\n</code></pre></p> <p>The pattern: Make sure run logs and catalog are accessible in every execution environment (container, pod, remote job).</p>"},{"location":"production/pipeline-execution/custom-pipeline-executors/#plugin_registration","title":"Plugin Registration","text":"<p>Create your executor and register it:</p> <pre><code>from extensions.pipeline_executor import GenericPipelineExecutor\nfrom pydantic import Field\n\nclass MyPlatformExecutor(GenericPipelineExecutor):\n    service_name: str = \"my-platform\"\n\n    # Your platform config fields\n    api_endpoint: str = Field(...)\n    project_id: str = Field(...)\n\n    def trigger_node_execution(self, node, map_variable=None):\n        # Your platform node execution logic\n        pass\n\n    def execute_from_graph(self, dag, map_variable=None):\n        # Optional: For DAG transpilation platforms only\n        pass\n</code></pre> <p>Register in <code>pyproject.toml</code>: <pre><code>[project.entry-points.'pipeline_executor']\n\"my-platform\" = \"my_package.executors:MyPlatformExecutor\"\n</code></pre></p>"},{"location":"production/pipeline-execution/custom-pipeline-executors/#which_pattern_to_choose","title":"Which Pattern to Choose?","text":"<p>DAG Traversal (<code>trigger_node_execution</code> only):</p> <ul> <li>For: Simple batch platforms (AWS Batch, SLURM, etc.)</li> <li>How: Runnable calls your method for each node</li> <li>Storage: Handle volumes/mounts in <code>trigger_node_execution()</code></li> </ul> <p>DAG Transpilation (both methods):</p> <ul> <li>For: Workflow platforms (Argo, Airflow, Prefect, etc.)</li> <li>How: Convert entire DAG to platform's native workflow format</li> <li>Storage: Handle volumes/mounts in the transpiled workflow spec</li> </ul> <p>The complexity is in translating DAG semantics (parallel branches, conditionals) to your platform's workflow language.</p>"},{"location":"production/pipeline-execution/custom-pipeline-executors/#integration_advantage","title":"Integration Advantage","text":"<p>\ud83d\udd11 Key Benefit: Custom executors live entirely in your codebase, not in public repositories or external dependencies.</p>"},{"location":"production/pipeline-execution/custom-pipeline-executors/#complete_control_privacy","title":"Complete Control &amp; Privacy","text":"<pre><code># In your private repository\n# my-company/internal-ml-platform/executors/company_executor.py\n\nclass CompanyBatchExecutor(GenericPipelineExecutor):\n    service_name: str = \"company-batch\"\n\n    # Your internal configuration\n    internal_api_endpoint: str = Field(...)\n    security_group: str = Field(...)\n    compliance_tags: dict = Field(default_factory=dict)\n\n    def trigger_node_execution(self, node, map_variable=None):\n        # Your proprietary integration logic\n        # Company-specific security, monitoring, cost tracking\n        pass\n</code></pre> <p>Integration benefits:</p> <ul> <li>\ud83d\udd12 Security: No external dependencies or public code exposure</li> <li>\ud83c\udfe2 Compliance: Implement organization-specific governance and audit requirements</li> <li>\ud83d\udcb0 Cost Control: Integrate with internal cost tracking and resource management</li> <li>\ud83d\udd27 Customization: Build reusable templates for your exact infrastructure</li> <li>\ud83d\udcca Monitoring: Integrate with dashboards and alerting systems</li> </ul>"},{"location":"production/pipeline-execution/custom-pipeline-executors/#reusable_templates","title":"Reusable Templates","text":"<p>Teams can create internal libraries of executors:</p> <pre><code># Internal package: company-runnable-executors\nfrom company_runnable_executors import (\n    ProductionK8sExecutor,      # Your Kubernetes setup\n    StagingBatchExecutor,       # Your staging environment\n    ComplianceExecutor,         # SOC2/HIPAA requirements\n    CostOptimizedExecutor,      # Spot instances + cost tracking\n)\n\n# Teams use your standardized executors\nclass MLTrainingPipeline(Pipeline):\n    def production_config(self):\n        return ProductionK8sExecutor(\n            namespace=\"ml-prod\",\n            resource_limits=self.get_approved_limits(),\n            compliance_mode=True\n        )\n</code></pre>"},{"location":"production/pipeline-execution/custom-pipeline-executors/#ecosystem_integration","title":"Ecosystem Integration","text":"<pre><code># Your company's standard configuration templates\npipeline-executor:\n  type: company-batch\n  config:\n    internal_api_endpoint: \"https://internal-batch.company.com\"\n    security_group: \"ml-workloads-sg\"\n    compliance_tags:\n      project: \"{{PROJECT_ID}}\"\n      cost_center: \"{{COST_CENTER}}\"\n      data_classification: \"confidential\"\n    monitoring:\n      dashboard_url: \"https://company-monitoring.com/runnable\"\n      alert_channels: [\"#ml-alerts\", \"#devops-alerts\"]\n</code></pre> <p>This makes runnable a platform for building your internal ML infrastructure, not just using external services.</p>"},{"location":"production/pipeline-execution/custom-pipeline-executors/#need_help","title":"Need Help?","text":"<p>Custom pipeline executors are complex integrations that require deep understanding of both runnable's architecture and your target platform's orchestration model.</p> <p>Get Support</p> <p>We're here to help you succeed! Building custom executors involves intricate details about:</p> <ul> <li>Graph traversal and dependency management</li> <li>Step log coordination and error handling</li> <li>Parameter passing and context management</li> <li>Platform-specific workflow translation patterns</li> </ul> <p>Don't hesitate to reach out:</p> <ul> <li>\ud83d\udce7 Contact the team for architecture guidance and implementation support</li> <li>\ud83e\udd1d Collaboration opportunities - we're interested in supporting enterprise integrations</li> <li>\ud83d\udcd6 Documentation feedback - help us improve these guides based on your experience</li> </ul> <p>Better together: Complex orchestration integrations benefit from collaboration between platform experts (you) and runnable architecture experts (us).</p> <p>Highly Complex Integration</p> <p>These are among the most sophisticated integrations in runnable that involve:</p> <ul> <li>Deep understanding of runnable's graph execution engine and step lifecycle</li> <li>Complex orchestration platform APIs and workflow specification formats</li> <li>Distributed execution coordination, failure handling, and state management</li> <li>Advanced container orchestration, networking, and resource management patterns</li> </ul> <p>Success requires significant expertise in both domains. The existing orchestration integrations (especially Argo) took substantial development effort to get right - collaboration dramatically increases your chances of success.</p> <p>Your success with custom pipeline executors helps the entire runnable community!</p>"},{"location":"production/pipeline-execution/local-container/","title":"Local Container Pipeline Execution","text":"<p>Execute pipelines using Docker containers with optional parallel processing - perfect for testing container-based deployments locally with environment isolation.</p> <p>Installation Required</p> <p>Container execution requires the optional Docker dependency: <pre><code>pip install runnable[docker]\n</code></pre></p> <p>Container Setup Made Simple</p> <p>Just build a Docker image from your project root - it automatically includes your code, dependencies, and environment!</p> <pre><code>docker build -t my-project:latest .\n</code></pre>"},{"location":"production/pipeline-execution/local-container/#getting_started","title":"Getting Started","text":""},{"location":"production/pipeline-execution/local-container/#basic_configuration","title":"Basic Configuration","text":"<pre><code>pipeline-executor:\n  type: local-container\n  config:\n    docker_image: \"my-project:latest\"\n</code></pre>"},{"location":"production/pipeline-execution/local-container/#simple_example","title":"Simple Example","text":"pipeline.pyconfig.yaml <pre><code>from runnable import Pipeline, PythonTask\n\ndef hello_from_container():\n    import platform\n    print(f\"Hello from container running: {platform.platform()}\")\n    return \"success\"\n\ndef main():\n    task = PythonTask(\n        function=hello_from_container,\n        name=\"hello\"\n    )\n\n    pipeline = Pipeline(steps=[task])\n    pipeline.execute()\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <pre><code>pipeline-executor:\n  type: local-container\n  config:\n    docker_image: \"my-project:latest\"\n</code></pre> <p>Run the pipeline: <pre><code>RUNNABLE_CONFIGURATION_FILE=config.yaml uv run pipeline.py\n</code></pre></p> <p>Container Isolation</p> <p>Each task runs in a fresh container, giving you clean isolation between steps.</p>"},{"location":"production/pipeline-execution/local-container/#why_use_containers_locally","title":"Why Use Containers Locally?","text":"<p>Perfect for Production Testing</p> <p>Environment reproduction: Test exactly what runs in production</p> <ul> <li>\u2705 Dependency isolation: Each step gets a clean container environment</li> <li>\u2705 Local validation: Catch container issues before cloud deployment</li> <li>\u2705 Multiple environments: Different containers for different pipeline steps</li> </ul> <p>Execution Models</p> <p>Sequential (Default):</p> <ul> <li>\ud83d\udd04 One step at a time: Tasks run sequentially for simplicity</li> <li>\ud83d\udc33 Container per step: Each task gets a fresh, isolated container</li> <li>\ud83d\udcbb Local resources: Uses your machine's CPU/memory limits</li> </ul> <p>Parallel (Optional):</p> <ul> <li>\u26a1 Parallel branches: <code>parallel</code> and <code>map</code> nodes can run simultaneously</li> <li>\ud83d\udc33 Multiple containers: Each branch gets its own container</li> <li>\ud83d\udccb Requires compatible run log store: Use <code>chunked-fs</code> for parallel writes</li> </ul>"},{"location":"production/pipeline-execution/local-container/#parallel_execution","title":"Parallel Execution","text":"<p>Enable parallel processing for container-based workflows:</p> pipeline.pyparallel_container.yaml <pre><code>from runnable import Pipeline, PythonTask, Parallel\n\ndef process_in_container(data_chunk):\n    import platform\n    print(f\"Processing chunk {data_chunk} on {platform.platform()}\")\n    return f\"processed_{data_chunk}\"\n\ndef main():\n    # Parallel branches that run in separate containers\n    parallel_node = Parallel(\n        name=\"container_parallel\",\n        branches={\n            \"process_a\": [PythonTask(function=process_in_container, name=\"task_a\")],\n            \"process_b\": [PythonTask(function=process_in_container, name=\"task_b\")],\n            \"process_c\": [PythonTask(function=process_in_container, name=\"task_c\")]\n        }\n    )\n\n    pipeline = Pipeline(steps=[parallel_node])\n\n    # Execute with parallel container support\n    pipeline.execute(configuration_file=\"parallel_container.yaml\")\n\n    return pipeline\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <pre><code>pipeline-executor:\n  type: local-container\n  config:\n    docker_image: \"my-project:latest\"\n    enable_parallel: true\n\n# Required for parallel execution\nrun-log-store:\n  type: chunked-fs\n\ncatalog:\n  type: file-system\n</code></pre> <p>Run with parallel containers: <pre><code># Build your image first\ndocker build -t my-project:latest .\n\n# Execute the pipeline\nuv run pipeline.py\n</code></pre></p> <p>Parallel Container Benefits</p> <ul> <li>True isolation: Each parallel branch runs in its own container</li> <li>Resource utilization: Uses multiple CPU cores simultaneously</li> <li>Production testing: Test parallel behavior before deploying to Kubernetes</li> </ul>"},{"location":"production/pipeline-execution/local-container/#advanced_usage","title":"Advanced Usage","text":""},{"location":"production/pipeline-execution/local-container/#dynamic_container_images","title":"Dynamic Container Images","text":"<p>Runtime Image Selection</p> <p>Use different images at runtime with environment variables:</p> <pre><code>pipeline-executor:\n  type: local-container\n  config:\n    docker_image: $my_docker_image\n</code></pre> <pre><code># Set the image dynamically\nexport RUNNABLE_VAR_my_docker_image=\"my-project:v2.0\"\nRUNNABLE_CONFIGURATION_FILE=config.yaml uv run pipeline.py\n</code></pre>"},{"location":"production/pipeline-execution/local-container/#step-specific_containers","title":"Step-Specific Containers","text":"<p>Different steps can use different container images - useful when you need specialized environments for different parts of your pipeline.</p> <p>How it works:</p> <ol> <li>Define multiple configurations in your config file using <code>overrides</code></li> <li>Reference the override in your task using the <code>overrides</code> parameter</li> <li>Each task runs in its specified container environment</li> </ol> pipeline.pyconfig.yaml <pre><code>from runnable import Pipeline, ShellTask\n\ndef main():\n    # Uses default Python container (from main config)\n    step1 = ShellTask(\n        name=\"python_analysis\",\n        command=\"python --version &amp;&amp; python analyze.py\"\n    )\n\n    # Uses specialized R container (from \"r_override\" configuration)\n    step2 = ShellTask(\n        name=\"r_modeling\",\n        command=\"Rscript model.R\",\n        overrides={\"local-container\": \"r_override\"}  # References config below\n    )\n\n    pipeline = Pipeline(steps=[step1, step2])\n    pipeline.execute()\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Understanding the Override</p> <p><code>overrides={\"local-container\": \"r_override\"}</code> means:</p> <ul> <li>\"local-container\": The executor type we're overriding</li> <li>\"r_override\": The name of the override configuration (defined in config.yaml)</li> <li>Result: This task will use the R container instead of the default Python container</li> </ul> <pre><code>pipeline-executor:\n  type: local-container\n  config:\n    docker_image: \"my-python:latest\"  # Default for most steps\n  overrides:\n    r_override:\n      docker_image: \"my-r-env:latest\"  # Specialized R environment\n</code></pre>"},{"location":"production/pipeline-execution/local-container/#debugging_failed_containers","title":"Debugging Failed Containers","text":"<p>Debug Failed Containers</p> <p>Keep containers around for debugging:</p> <pre><code>pipeline-executor:\n  type: local-container\n  config:\n    docker_image: \"my-project:latest\"\n    auto_remove_container: false  # Keep failed containers\n</code></pre> <p>Then inspect the failed container:</p> <pre><code># List containers to find the failed one\ndocker ps -a\n\n# Get into the failed container\ndocker exec -it &lt;container-id&gt; /bin/bash\n\n# Or check its logs\ndocker logs &lt;container-id&gt;\n</code></pre>"},{"location":"production/pipeline-execution/local-container/#configuration_reference","title":"Configuration Reference","text":"<pre><code>pipeline-executor:\n  type: local-container\n  config:\n    docker_image: \"my-project:latest\"  # Required: Docker image to use\n    enable_parallel: false             # Enable parallel execution\n    auto_remove_container: true        # Remove containers after execution\n    environment:                       # Environment variables for containers\n      VAR_NAME: \"value\"\n    overrides:                        # Step-specific configurations\n      alt_config:\n        docker_image: \"alternative:latest\"\n        auto_remove_container: false\n        environment:\n          SPECIAL_VAR: \"special_value\"\n</code></pre>"},{"location":"production/pipeline-execution/local-container/#when_to_use_local_container","title":"When to Use Local Container","text":"<p>Choose Local Container When</p> <ul> <li>Testing container-based deployments before going to cloud</li> <li>Need environment isolation between pipeline steps</li> <li>Want to replicate production container behavior locally</li> <li>Different steps require different software environments</li> </ul> <p>Use Regular Local Executor When</p> <ul> <li>Simple development and experimentation</li> <li>All steps use the same environment</li> <li>Want fastest possible execution (no container overhead)</li> </ul> <p>Upgrade to Cloud Executors When</p> <ul> <li>Need true parallel execution (Argo)</li> <li>Want distributed compute resources</li> <li>Running production workloads</li> </ul>"},{"location":"production/pipeline-execution/local/","title":"Local Pipeline Execution","text":"<p>Execute pipelines in your local environment with optional parallel processing - perfect for development and testing.</p> <p>All pipeline steps execute in the same local environment where they were triggered.</p>"},{"location":"production/pipeline-execution/local/#features","title":"Features","text":"<ul> <li>\u2705 Comfortable development: Direct access to your local environment</li> <li>\u2705 Optional parallelization: Enable parallel execution for <code>parallel</code> and <code>map</code> nodes</li> <li>\u2705 Automatic fallback: Gracefully falls back to sequential if run log store doesn't support parallel writes</li> <li>\u26a0\ufe0f Local resource constraints: Scalability limited by your machine's resources</li> <li>\u274c Single environment: All steps share the same compute environment</li> </ul>"},{"location":"production/pipeline-execution/local/#quick_start","title":"Quick Start","text":""},{"location":"production/pipeline-execution/local/#sequential_execution_default","title":"Sequential Execution (Default)","text":"<pre><code>from runnable import Pipeline, PythonTask\n\ndef step_one():\n    print(\"Step 1 running...\")\n    return \"data_from_step_1\"\n\ndef step_two():\n    print(\"Step 2 running...\")\n    return \"final_result\"\n\ndef main():\n    pipeline = Pipeline(steps=[\n        PythonTask(function=step_one, name=\"step1\"),\n        PythonTask(function=step_two, name=\"step2\")\n    ])\n\n    # Sequential execution (default)\n    pipeline.execute()\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"production/pipeline-execution/local/#parallel_execution","title":"Parallel Execution","text":"<p>Enable parallel processing for <code>parallel</code> and <code>map</code> nodes:</p> pipeline.pyparallel_config.yaml <pre><code>from runnable import Pipeline, PythonTask, Parallel\n\ndef process_data(item):\n    print(f\"Processing {item}\")\n    return f\"processed_{item}\"\n\ndef main():\n    # Create parallel branches\n    parallel_node = Parallel(\n        name=\"process_parallel\",\n        branches={\n            \"branch_1\": [PythonTask(function=process_data, name=\"task1\")],\n            \"branch_2\": [PythonTask(function=process_data, name=\"task2\")],\n            \"branch_3\": [PythonTask(function=process_data, name=\"task3\")]\n        }\n    )\n\n    pipeline = Pipeline(steps=[parallel_node])\n\n    # Enable parallel execution\n    pipeline.execute(configuration_file=\"parallel_config.yaml\")\n\n    return pipeline\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <pre><code>pipeline-executor:\n  type: local\n  config:\n    enable_parallel: true\n\n# Parallel-compatible run log store\nrun-log-store:\n  type: chunked-fs\n\ncatalog:\n  type: file-system\n</code></pre> <p>Run the pipeline: <pre><code>uv run pipeline.py\n</code></pre></p> <p>Automatic Fallback</p> <p>If you use a run log store that doesn't support parallel writes (like <code>file-system</code>), the executor automatically falls back to sequential execution with a helpful warning.</p>"},{"location":"production/pipeline-execution/local/#parallel_execution_requirements","title":"Parallel Execution Requirements","text":"<p>For parallel execution to work, you need:</p> <ol> <li>Enable parallel: Set <code>enable_parallel: true</code> in pipeline executor config</li> <li>Compatible run log store: Use <code>chunked-fs</code> (file-system doesn't support parallel writes)</li> </ol> Run Log Store Parallel Support Use Case <code>file-system</code> \u274c Sequential only Simple development <code>chunked-fs</code> \u2705 Parallel ready Parallel local execution"},{"location":"production/pipeline-execution/local/#configuration_reference","title":"Configuration Reference","text":"<pre><code>pipeline-executor:\n  type: local\n  config:\n    enable_parallel: true\n</code></pre>"},{"location":"production/pipeline-execution/local/#when_to_use_local_execution","title":"When to Use Local Execution","text":"<p>Perfect for Development</p> <ul> <li>Quick experimentation with immediate feedback</li> <li>Debugging with direct access to your environment</li> <li>Small to medium datasets that fit in local memory</li> <li>Iterative development with fast execution cycles</li> </ul>"},{"location":"production/pipeline-execution/local/#sequential_vs_parallel","title":"Sequential vs Parallel","text":"<p>Use sequential (default) when:</p> <ul> <li>Simple linear pipelines</li> <li>Steps depend heavily on each other</li> <li>You want minimal overhead and complexity</li> </ul> <p>Use parallel when:</p> <ul> <li>You have independent <code>parallel</code> or <code>map</code> branches</li> <li>Your machine has multiple cores to utilize</li> <li>You're testing parallel patterns before deploying to production</li> </ul> <p>Development Foundation</p> <p>All the conceptual examples use <code>local</code> executors for simplicity. Start here, then move to production executors when ready.</p>"},{"location":"production/pipeline-execution/mocked/","title":"Mocked Pipeline Execution","text":"<p>Test and validate your pipeline structure without running the actual tasks - perfect for development, testing, and debugging workflows.</p>"},{"location":"production/pipeline-execution/mocked/#what_is_mocked_execution","title":"What is Mocked Execution?","text":"<p>Mocked execution allows you to:</p> <ul> <li>Validate pipeline structure - Check that your workflow logic is correct</li> <li>Test failure scenarios - Simulate different outcomes without side effects</li> <li>Debug pipeline issues - Isolate problems by mocking successful steps</li> <li>Speed up development - Skip time-consuming tasks during pipeline development</li> </ul> <p>Pipeline Validation Made Simple</p> <p>Mocked execution runs your entire pipeline workflow but skips the actual task execution, letting you verify the logic and flow instantly.</p>"},{"location":"production/pipeline-execution/mocked/#getting_started","title":"Getting Started","text":""},{"location":"production/pipeline-execution/mocked/#basic_configuration","title":"Basic Configuration","text":"<pre><code>pipeline-executor:\n  type: mocked\n</code></pre>"},{"location":"production/pipeline-execution/mocked/#simple_example","title":"Simple Example","text":"pipeline.pyconfig.yaml <pre><code>from runnable import Pipeline, PythonTask\n\ndef slow_data_processing():\n    # This would normally take hours\n    print(\"Processing massive dataset...\")\n    return {\"processed_records\": 1000000}\n\ndef train_model(data):\n    # This would normally take even longer\n    print(f\"Training on {data['processed_records']} records...\")\n    return {\"accuracy\": 0.95}\n\ndef main():\n    pipeline = Pipeline(steps=[\n        PythonTask(function=slow_data_processing, name=\"process_data\"),\n        PythonTask(function=train_model, name=\"train_model\")\n    ])\n\n    # Same pipeline code - just different execution\n    pipeline.execute()\n    return pipeline\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <pre><code>pipeline-executor:\n  type: mocked  # Skip actual execution, just validate structure\n</code></pre> <p>Run with mocking: <pre><code>RUNNABLE_CONFIGURATION_FILE=config.yaml uv run pipeline.py\n</code></pre></p> <p>Result: Pipeline completes in seconds, validating the workflow without running the time-consuming tasks.</p>"},{"location":"production/pipeline-execution/mocked/#why_use_mocked_execution","title":"Why Use Mocked Execution?","text":"<p>Development Benefits</p> <p>Fast iteration: Test pipeline changes without waiting for long-running tasks</p> <p>Safe testing: No side effects, no resource usage, no accidental data changes</p> <p>Testing Benefits</p> <p>Structure validation: Verify your pipeline logic and flow</p> <p>Failure scenario testing: Test different outcomes using patches</p> <p>Debugging Benefits</p> <p>Issue isolation: Mock successful steps to focus on problematic ones</p> <p>State recreation: Debug failed executions by replaying with mocked setup</p>"},{"location":"production/pipeline-execution/mocked/#advanced_usage_patching_tasks","title":"Advanced Usage: Patching Tasks","text":"<p>Override specific tasks to test different scenarios:</p> pipeline.pytest_success.yamltest_failure.yaml <pre><code>from runnable import Pipeline, ShellTask\n\ndef main():\n    pipeline = Pipeline(steps=[\n        ShellTask(command=\"process_data.py\", name=\"data_processing\"),\n        ShellTask(command=\"train_model.py\", name=\"model_training\"),\n        ShellTask(command=\"deploy_model.py\", name=\"deployment\")\n    ])\n\n    pipeline.execute()\n    return pipeline\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Test the happy path by mocking a potentially failing step:</p> <pre><code>pipeline-executor:\n  type: mocked\n  config:\n    patches:\n      data_processing:\n        command: \"echo 'Data processing successful'\"\n      # model_training and deployment will be mocked (pass-through)\n</code></pre> <p>Test failure handling by simulating a failing step:</p> <pre><code>pipeline-executor:\n  type: mocked\n  config:\n    patches:\n      model_training:\n        command: \"exit 1\"  # Simulate training failure\n      # Other steps will be mocked (pass-through)\n</code></pre> <p>Test different scenarios: <pre><code># Test success scenario\nRUNNABLE_CONFIGURATION_FILE=test_success.yaml uv run pipeline.py\n\n# Test failure scenario\nRUNNABLE_CONFIGURATION_FILE=test_failure.yaml uv run pipeline.py\n</code></pre></p>"},{"location":"production/pipeline-execution/mocked/#configuration_reference","title":"Configuration Reference","text":"<pre><code>pipeline-executor:\n  type: mocked\n  config:\n    patches:                    # Optional: Override specific tasks\n      task_name:\n        command: \"new_command\"  # Replace task command\n        # Other task configuration options available\n</code></pre>"},{"location":"production/pipeline-execution/mocked/#patch_options","title":"Patch Options","text":"<p>For Python tasks: - <code>command</code>: Override the function being called</p> <p>For Shell tasks: - <code>command</code>: Replace the shell command</p> <p>For Notebook tasks: - <code>command</code>: Override notebook path - Additional notebook-specific options available</p>"},{"location":"production/pipeline-execution/mocked/#common_use_cases","title":"Common Use Cases","text":""},{"location":"production/pipeline-execution/mocked/#1_development_workflow","title":"1. Development Workflow","text":"<pre><code># Skip expensive operations during development\npipeline-executor:\n  type: mocked\n  config:\n    patches:\n      expensive_computation:\n        command: \"echo 'Skipped expensive step'\"\n</code></pre>"},{"location":"production/pipeline-execution/mocked/#2_cicd_testing","title":"2. CI/CD Testing","text":"<pre><code># Validate pipeline structure in CI without running actual workloads\npipeline-executor:\n  type: mocked\n  # No patches needed - just validate structure\n</code></pre>"},{"location":"production/pipeline-execution/mocked/#3_debugging_failed_pipelines","title":"3. Debugging Failed Pipelines","text":"<p>Debug production failures locally by recreating the exact state from the failed run:</p> <pre><code># Mock successful steps to isolate the failing one\npipeline-executor:\n  type: mocked\n  config:\n    patches:\n      failing_step:\n        command: \"echo 'This step now succeeds'\"\n      # Don't patch the step you want to debug\n</code></pre>"},{"location":"production/pipeline-execution/mocked/#4_training_documentation","title":"4. Training &amp; Documentation","text":"<pre><code># Demonstrate pipeline behavior without side effects\npipeline-executor:\n  type: mocked\n</code></pre>"},{"location":"production/pipeline-execution/mocked/#debugging_failed_production_runs","title":"Debugging Failed Production Runs","text":"<p>When a pipeline fails in production, you can debug it locally using mocked execution to recreate the exact conditions.</p> <p>Prerequisites for Debugging</p> <p>Local machine access: Mocked executor only runs locally</p> <p>Failed run data: Access to catalog data and run logs from the failed execution</p> <p>Same codebase: The pipeline code that failed in production</p>"},{"location":"production/pipeline-execution/mocked/#step-by-step_debugging_workflow","title":"Step-by-Step Debugging Workflow","text":"<p>1. Identify the Failed Step</p> <p>First, examine the run log to find which step failed:</p> <pre><code># Find the failed run log (usually in .run_log_store/)\nls .run_log_store/\n\n# Or if using remote run log store, download it locally\n# Example: aws s3 cp s3://my-logs/failed-run-id.json ./\n</code></pre> <p>Look for the failed step in the run log JSON: <pre><code>{\n  \"run_id\": \"failed-production-run\",\n  \"status\": \"FAIL\",\n  \"steps\": {\n    \"data_processing\": {\"status\": \"SUCCESS\"},\n    \"model_training\": {\"status\": \"FAIL\"},  # &lt;- This step failed\n    \"deployment\": {\"status\": \"NOT_EXECUTED\"}\n  }\n}\n</code></pre></p> <p>2. Copy Catalog Data Locally</p> <p>The failing step needs access to the exact data state from when it failed:</p> <pre><code># Copy catalog data from failed run to local debugging location\n# If using file-system catalog:\ncp -r .catalog/failed-production-run ./debug-catalog/\n\n# If using S3 catalog, download the data:\n# aws s3 sync s3://my-catalog/failed-production-run/ ./debug-catalog/\n\n# Make it available for debugging with a new run-id\ncp -r ./debug-catalog/ .catalog/debug-session/\n</code></pre> <p>3. Create Debug Configuration</p> <p>Mock all successful steps, let the failed step run with the real data:</p> <pre><code># debug-config.yaml\npipeline-executor:\n  type: mocked\n  config:\n    patches:\n      # Mock the successful steps (they already ran in production)\n      data_processing:\n        command: \"echo 'Data processing - already completed in production'\"\n\n      # Don't patch model_training - let it run with real data\n      # deployment step will be mocked by default since model_training might still fail\n\n# Use local storage to access the copied catalog data\ncatalog:\n  type: file-system\n  config:\n    catalog_location: \".catalog\"\n\nrun-log-store:\n  type: file-system\n  config:\n    log_folder: \".run_log_store\"\n</code></pre> <p>4. Debug the Failed Step</p> <p>Run the pipeline with the debug configuration:</p> <pre><code># Run with specific run-id to access the copied catalog data\nRUNNABLE_CONFIGURATION_FILE=debug-config.yaml uv run pipeline.py --run-id debug-session\n</code></pre> <p>What happens: - <code>data_processing</code> step is mocked (skipped) - <code>model_training</code> step runs with the actual data from production - You can now debug the failing step locally with production data</p> <p>5. Debug with IDE/Debugger</p> <p>Since mocked executor runs locally, you can use your favorite debugging tools:</p> <pre><code>def train_model(data):\n    import pdb; pdb.set_trace()  # Set breakpoint\n\n    # Debug the actual failing logic with production data\n    model = train_complex_model(data)\n    return model\n\ndef main():\n    pipeline = Pipeline(steps=[\n        PythonTask(function=load_data, name=\"data_processing\"),\n        PythonTask(function=train_model, name=\"model_training\"),  # Will hit breakpoint\n        PythonTask(function=deploy_model, name=\"deployment\")\n    ])\n\n    pipeline.execute()\n    return pipeline\n</code></pre>"},{"location":"production/pipeline-execution/mocked/#example_complete_debug_session","title":"Example: Complete Debug Session","text":"<p>Original failed pipeline: <pre><code>def main():\n    pipeline = Pipeline(steps=[\n        PythonTask(function=load_data, name=\"load_data\"),\n        PythonTask(function=process_data, name=\"process_data\"),\n        PythonTask(function=train_model, name=\"train_model\"),  # Failed here\n        PythonTask(function=deploy_model, name=\"deploy\")\n    ])\n    return pipeline\n</code></pre></p> <p>Debug configuration: <pre><code># debug-failed-training.yaml\npipeline-executor:\n  type: mocked\n  config:\n    patches:\n      load_data:\n        command: \"echo 'Mocked: Data already loaded in production'\"\n      process_data:\n        command: \"echo 'Mocked: Data already processed in production'\"\n      # train_model - no patch, let it run with real data\n      # deploy - will be mocked by default\n</code></pre></p> <p>Debug session: <pre><code># Copy production catalog data\ncp -r .catalog/prod-failure-run-id .catalog/debug-training-issue\n\n# Run debug session\nRUNNABLE_CONFIGURATION_FILE=debug-failed-training.yaml uv run pipeline.py --run-id debug-training-issue\n\n# Now you can debug train_model with the exact production data state\n</code></pre></p>"},{"location":"production/pipeline-execution/mocked/#debug_tips","title":"Debug Tips","text":"<p>Debugging Best Practices</p> <p>Preserve original data: Always copy catalog data, never modify the original</p> <p>Use meaningful run-ids: Name debug sessions clearly (e.g., <code>debug-training-failure-2024</code>)</p> <p>Mock successful steps: Only run the failing step and its dependencies</p> <p>Check parameters: Verify the failing step receives the same parameters as in production</p> <p>Important Notes</p> <p>Local environment: Ensure your local environment matches production (dependencies, versions)</p> <p>Data access: Make sure you have permission to access production catalog data</p> <p>Secrets: Production secrets may not be available locally - use debug values if needed</p>"},{"location":"production/pipeline-execution/mocked/#best_practices","title":"Best Practices","text":"<p>When to Use Mocked Execution</p> <p>During development: Validate logic without running expensive tasks</p> <p>In testing: Verify pipeline structure and failure handling</p> <p>For debugging: Isolate issues by mocking successful steps</p> <p>In demos: Show pipeline behavior without side effects</p> <p>Limitations</p> <p>No actual output: Mocked tasks don't produce real results</p> <p>Limited validation: Can't test actual task logic, only pipeline structure</p> <p>Local only: Mocked executor runs only on local machine</p>"},{"location":"production/pipeline-execution/mocked/#when_to_use_mocked_execution","title":"When to Use Mocked Execution","text":"<p>Choose Mocked When</p> <ul> <li>Developing and iterating on pipeline structure</li> <li>Testing pipeline logic and failure scenarios</li> <li>Debugging complex workflow issues</li> <li>Demonstrating pipelines without side effects</li> <li>Validating pipeline changes in CI/CD</li> </ul> <p>Use Other Executors When</p> <ul> <li>Need actual task execution and results</li> <li>Testing production performance characteristics</li> <li>Running final validation before deployment</li> </ul> <p>Related: Local Executor | Pipeline Testing Patterns</p>"},{"location":"production/pipeline-execution/overview/","title":"Pipeline Execution Overview","text":"<p>Configure how multi-step workflows are orchestrated and executed across different environments.</p>"},{"location":"production/pipeline-execution/overview/#available_pipeline_executors","title":"Available Pipeline Executors","text":"Executor Use Case Environment Execution Model Local Development Local machine Sequential + Conditional Parallel* Local Container Isolated development Docker containers** Sequential + Conditional Parallel* Argo Workflows Production Argo cluster Parallel + Sequential (full orchestration) Mocked Testing &amp; validation Local machine Simulation (no actual execution) <p>*Parallel execution requires <code>enable_parallel: true</code> and compatible run log store</p> <p>**Container environments easily match local setup (just build from project root!)</p>"},{"location":"production/pipeline-execution/overview/#execution_models","title":"Execution Models","text":""},{"location":"production/pipeline-execution/overview/#local_execution_models","title":"Local Execution Models","text":"<p>Local and Local Container executors support both sequential and parallel execution:</p>"},{"location":"production/pipeline-execution/overview/#sequential_execution_default","title":"Sequential Execution (Default)","text":"<ul> <li>\u2705 Fast startup: No orchestration overhead</li> <li>\u2705 Simple debugging: Linear execution, easy to trace</li> <li>\u2705 Resource efficient: Single process, minimal memory usage</li> <li>\u2705 Universal compatibility: Works with all run log stores</li> </ul>"},{"location":"production/pipeline-execution/overview/#conditional_parallel_execution","title":"Conditional Parallel Execution","text":"<ul> <li>\u2705 Optional parallelization: Enable with <code>enable_parallel: true</code></li> <li>\u2705 Automatic fallback: Falls back to sequential if run log store doesn't support parallel writes</li> <li>\u2705 Local multiprocessing: Uses your machine's multiple cores</li> <li>\u26a0\ufe0f Run log store dependency: Requires <code>chunked-fs</code> or compatible run log store</li> </ul> <p>Best for: Development, debugging, small-to-medium workflows, single-machine execution</p>"},{"location":"production/pipeline-execution/overview/#production_execution_orchestrated","title":"Production Execution (Orchestrated)","text":"<p>Argo Workflows supports both sequential and parallel execution:</p> <ul> <li>\u2705 True parallelization: Independent tasks run simultaneously</li> <li>\u2705 Complex workflows: DAG-based execution with dependencies</li> <li>\u2705 Scalability: Distributed across multiple nodes/pods</li> <li>\u2705 Production features: Retry logic, monitoring, resource management</li> <li>\u26a0\ufe0f Higher overhead: Kubernetes orchestration complexity</li> </ul> <p>Best for: Production workflows, parallel processing, complex dependencies</p>"},{"location":"production/pipeline-execution/overview/#testing_and_validation_local","title":"Testing and Validation (Local)","text":"<p>Mocked Executor provides pipeline validation without execution:</p> <ul> <li>\u2705 Fast validation: Check pipeline structure without running tasks</li> <li>\u2705 Configuration testing: Validate configs before deployment</li> <li>\u2705 Development workflow: Test pipeline logic without side effects</li> <li>\u2705 Local only: Runs on your machine for testing purposes</li> </ul> <p>Best for: Pipeline validation, testing configurations, development workflows</p>"},{"location":"production/pipeline-execution/overview/#key_concepts","title":"Key Concepts","text":"<p>Pipeline executors handle:</p> <ul> <li>Workflow orchestration: Managing step dependencies and execution order</li> <li>Cross-step data flow: Passing parameters and artifacts between tasks</li> <li>Environment management: Ensuring each step runs in the correct context</li> <li>Failure handling: Managing retries, error propagation, and cleanup</li> </ul>"},{"location":"production/pipeline-execution/overview/#quick_start","title":"Quick Start","text":"<pre><code>from runnable import Pipeline, PythonTask\n\ndef step1():\n    # Your processing logic\n    return \"processed_data\"\n\ndef step2(processed_data):\n    # Your analysis logic\n    return \"analysis_result\"\n\ndef main():\n    pipeline = Pipeline(steps=[\n        PythonTask(function=step1, name=\"process\"),\n        PythonTask(function=step2, name=\"analyze\")\n    ])\n\n    # Environment determines executor via RUNNABLE_CONFIGURATION_FILE\n    pipeline.execute()\n    return pipeline\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Run with different executors:</p> <pre><code># Local execution (default)\nuv run my_pipeline.py\n\n# Container execution\nexport RUNNABLE_CONFIGURATION_FILE=local-container.yaml\nuv run my_pipeline.py\n\n# Production orchestration\nexport RUNNABLE_CONFIGURATION_FILE=argo.yaml\nuv run my_pipeline.py\n</code></pre>"},{"location":"production/pipeline-execution/overview/#custom_pipeline_executors","title":"Custom Pipeline Executors","text":"<p>Need to deploy to your unique infrastructure? Runnable's plugin architecture makes it simple to build custom pipeline executors for any orchestration platform.</p> <p>No Vendor Lock-in</p> <p>Your infrastructure, your way: Deploy pipelines on Argo Workflows, Apache Airflow, Azure Data Factory, or any custom orchestration platform.</p> <ul> <li>\ud83d\udd0c Cloud orchestrators: Argo Workflows, Apache Airflow, Prefect, Azure Data Factory</li> <li>\ud83c\udfe2 HPC systems: SLURM, PBS, custom job schedulers</li> <li>\ud83c\udfaf Container platforms: Kubernetes, Docker Swarm, Nomad</li> <li>\ud83d\udd10 Enterprise platforms: Custom workflow engines, proprietary orchestrators</li> </ul>"},{"location":"production/pipeline-execution/overview/#building_custom_pipeline_executors","title":"Building Custom Pipeline Executors","text":"<p>Learn how to create production-ready custom pipeline executors:</p> <p>\ud83d\udcd6 Custom Pipeline Executors Development Guide</p> <p>The guide provides:</p> <ul> <li>Complete stubbed implementation showing integration patterns</li> <li>Node-by-node vs full DAG transpilation execution models</li> <li>YAML to Pydantic configuration mapping with validation</li> <li>Testing workflow with mock modes for safe development</li> </ul> <p>Quick Example</p> <p>Create a custom pipeline executor in just 3 steps:</p> <ol> <li>Implement key methods by extending <code>GenericPipelineExecutor</code></li> <li>Register via entry point in your <code>pyproject.toml</code></li> <li>Configure via YAML for seamless integration</li> </ol> <pre><code>from extensions.pipeline_executor import GenericPipelineExecutor\n\nclass MyPlatformExecutor(GenericPipelineExecutor):\n    service_name: str = \"my-platform\"\n\n    def trigger_node_execution(self, node, map_variable=None):\n        # Your orchestration platform integration here\n        pass\n</code></pre> <p>Ready to build? See the development guide for complete patterns and examples.</p>"},{"location":"production/pipeline-execution/overview/#choosing_the_right_executor","title":"Choosing the Right Executor","text":""},{"location":"production/pipeline-execution/overview/#development_testing","title":"Development &amp; Testing","text":"<ul> <li>Local: Quick development, debugging, small workflows</li> <li>Local Container: Isolated development, dependency consistency</li> <li>Mocked: Pipeline validation, configuration testing</li> </ul>"},{"location":"production/pipeline-execution/overview/#production_deployment","title":"Production Deployment","text":"<ul> <li>Argo Workflows: Production orchestration, parallel processing, complex workflows</li> </ul>"},{"location":"production/pipeline-execution/overview/#when_to_use_pipeline_execution","title":"When to Use Pipeline Execution","text":"<p>Choose pipeline execution when you need:</p> <ul> <li>Multi-step workflows with dependencies between tasks</li> <li>Cross-step data passing via parameters or catalog</li> <li>Complex orchestration with parallel branches or conditional logic</li> </ul>"},{"location":"production/pipeline-execution/overview/#when_to_use_job_execution_instead","title":"When to Use Job Execution Instead","text":"<p>For single task execution, consider Job Execution:</p> <ul> <li>Single functions without workflow dependencies</li> <li>Independent tasks that don't share data</li> <li>Simple execution without orchestration complexity</li> </ul> <p>Related: Job Execution Overview | Configuration Overview</p>"},{"location":"tutorial/","title":"Getting Started Tutorial","text":"<p>Transform a simple machine learning function into a production-ready pipeline, solving real challenges along the way.</p>"},{"location":"tutorial/#what_youll_build","title":"What You'll Build","text":"<p>By the end of this tutorial, you'll have:</p> <ul> <li>\u2705 Reproducible ML pipeline: Automatic tracking of all runs and results</li> <li>\u2705 Configurable experiments: Change parameters without touching code</li> <li>\u2705 Multi-step workflow: Data loading \u2192 preprocessing \u2192 training \u2192 evaluation</li> <li>\u2705 Large dataset handling: Efficient storage and retrieval of data artifacts</li> <li>\u2705 Shareable results: Model artifacts and metrics that persist between runs</li> <li>\u2705 Deployment ready: Same pipeline runs on laptop, containers, or Kubernetes</li> </ul>"},{"location":"tutorial/#the_journey","title":"The Journey","text":"<p>Each chapter tackles a real problem you'll face moving from \"works on my laptop\" to production:</p> <ol> <li>The Starting Point - A typical ML function with common problems</li> <li>Making It Reproducible - Track everything automatically</li> <li>Adding Flexibility - Configure without code changes</li> <li>Connecting the Workflow - Multi-step ML pipeline</li> <li>Handling Large Datasets - Efficient data management</li> <li>Sharing Results - Persistent model artifacts and metrics</li> <li>Running Anywhere - Same code, different environments</li> </ol>"},{"location":"tutorial/#prerequisites","title":"Prerequisites","text":"<ul> <li>Basic Python knowledge</li> <li>Familiarity with scikit-learn (we'll use simple examples)</li> <li>Python environment with runnable installed: <code>pip install runnable[examples]</code></li> </ul> <p>Time Investment: ~30-45 minutes total, designed for step-by-step learning</p> <p>Ready to start? \u2192 The Starting Point</p>"},{"location":"tutorial/01-starting-point/","title":"The Starting Point","text":"<p>Let's start with a realistic scenario: you have a machine learning function that works great on your laptop, but suffers from common problems that prevent it from being production-ready.</p>"},{"location":"tutorial/01-starting-point/#the_problem","title":"The Problem","text":"<p>Here's a typical ML training function that many data scientists write:</p> examples/tutorials/getting-started/functions.py (excerpt)<pre><code>def train_ml_model_basic():\n    \"\"\"\n    Basic ML training function - works locally but has typical problems:\n    - Hardcoded parameters\n    - No tracking of runs\n    - Results get overwritten\n    - No reproducibility guarantees\n    \"\"\"\n    print(\"Loading data...\")\n    df = load_data(\"data.csv\")\n\n    print(\"Preprocessing...\")\n    preprocessed = preprocess_data(df, test_size=0.2, random_state=42)\n\n    print(\"Training model...\")\n    model_data = train_model(preprocessed, n_estimators=100, random_state=42)\n\n    print(\"Evaluating...\")\n    results = evaluate_model(model_data, preprocessed)\n\n    print(f\"Accuracy: {results['accuracy']:.4f}\")\n\n    # Save everything (gets overwritten each run!)\n    save_model(model_data, \"model.pkl\")\n    save_results(results, \"results.json\")\n\n    return results\n</code></pre> <p>Try it yourself:</p> <pre><code>uv run examples/tutorials/getting-started/01_starting_point.py\n</code></pre>"},{"location":"tutorial/01-starting-point/#whats_wrong_here","title":"What's Wrong Here?","text":"<p>This function works, but it has several problems that will bite you in production:</p>"},{"location":"tutorial/01-starting-point/#no_execution_tracking","title":"\ud83d\udeab No Execution Tracking","text":"<ul> <li>When did you run this?</li> <li>What were the exact parameters?</li> <li>Which version of the code produced these results?</li> </ul>"},{"location":"tutorial/01-starting-point/#results_get_overwritten","title":"\ud83d\udeab Results Get Overwritten","text":"<ul> <li>Run it twice \u2192 lose the first results</li> <li>No way to compare different experiments</li> <li>Can't track model performance over time</li> </ul>"},{"location":"tutorial/01-starting-point/#hardcoded_parameters","title":"\ud83d\udeab Hardcoded Parameters","text":"<ul> <li>Want to try different <code>n_estimators</code>? Edit the code</li> <li>Want different train/test split? Edit the code</li> <li>Testing becomes cumbersome and error-prone</li> </ul>"},{"location":"tutorial/01-starting-point/#no_reproducibility","title":"\ud83d\udeab No Reproducibility","text":"<ul> <li>Even with <code>random_state</code>, environment differences can cause variations</li> <li>No record of what Python packages were used</li> <li>Impossible to recreate exact results months later</li> </ul>"},{"location":"tutorial/01-starting-point/#hard_to_share_and_deploy","title":"\ud83d\udeab Hard to Share and Deploy","text":"<ul> <li>How do you run this in a container?</li> <li>What about on Kubernetes?</li> <li>Sharing with colleagues means sharing your entire environment</li> </ul>"},{"location":"tutorial/01-starting-point/#the_real_impact","title":"The Real Impact","text":"<p>These aren't just theoretical problems. In real projects, this leads to:</p> <ul> <li>\"Which model was that?\" - Lost track of good results</li> <li>\"I can't reproduce the paper results\" - Different environments, different outcomes</li> <li>\"It worked yesterday\" - No history of what changed</li> <li>\"How do I run this in production?\" - Deployment becomes a separate project</li> </ul>"},{"location":"tutorial/01-starting-point/#what_well_build","title":"What We'll Build","text":"<p>Throughout this tutorial, we'll transform this exact function into a production-ready ML pipeline that solves all these problems:</p> <p>\u2705 Automatic execution tracking - Every run logged with timestamps and parameters</p> <p>\u2705 Result preservation - All experiments saved and easily comparable</p> <p>\u2705 Flexible configuration - Change parameters without touching code</p> <p>\u2705 Full reproducibility - Recreate exact results anytime, anywhere</p> <p>\u2705 Deploy anywhere - Same code runs on laptop, containers, Kubernetes</p> <p>Your functions won't change - we'll just wrap them with Runnable patterns.</p> <p>Next: Making It Reproducible - Add automatic tracking without changing your ML logic</p>"},{"location":"tutorial/02-making-it-reproducible/","title":"Making It Reproducible","text":"<p>Now let's solve the first major problem: lack of execution tracking. We'll transform our basic function into a reproducible, tracked job without changing the ML logic at all.</p>"},{"location":"tutorial/02-making-it-reproducible/#the_solution_pythonjob","title":"The Solution: PythonJob","text":"<p>Instead of calling our function directly, we'll wrap it with Runnable's <code>PythonJob</code>:</p> examples/tutorials/getting-started/02_making_it_reproducible.py<pre><code>from runnable import PythonJob\nfrom functions import train_ml_model_basic\n\ndef main():\n    # Same function, now wrapped as a Job\n    job = PythonJob(function=train_ml_model_basic)\n    job.execute()\n    return job\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Try it:</p> <pre><code>uv run examples/tutorials/getting-started/02_making_it_reproducible.py\n</code></pre>"},{"location":"tutorial/02-making-it-reproducible/#what_just_happened","title":"What Just Happened?","text":"<p>Your ML function ran exactly the same way, but Runnable automatically added powerful tracking capabilities:</p>"},{"location":"tutorial/02-making-it-reproducible/#execution_logging","title":"\ud83d\udcdd Execution Logging","text":"<p>Every run gets logged with complete details:</p> <pre><code>ls .run_log_store/\n# Shows JSON files with run IDs: curious-jang-0442.json\n</code></pre> <p>Each run directory contains:</p> <ul> <li>Execution metadata: when it ran, how long it took</li> <li>Environment info: Python version, package versions</li> <li>Results: function return values</li> <li>Status: success/failure with any error details</li> </ul>"},{"location":"tutorial/02-making-it-reproducible/#result_preservation","title":"\u267b\ufe0f Result Preservation","text":"<p>Unlike the basic version that overwrote <code>model.pkl</code> and <code>results.json</code>, each Runnable execution gets its own directory. Your results are never lost.</p>"},{"location":"tutorial/02-making-it-reproducible/#full_reproducibility","title":"\ud83d\udd0d Full Reproducibility","text":"<p>Each run captures everything needed to reproduce it:</p> <ul> <li>Exact timestamp</li> <li>Code version (if using git)</li> <li>Environment details</li> <li>Input parameters (we'll add those next!)</li> </ul>"},{"location":"tutorial/02-making-it-reproducible/#zero_code_changes","title":"\ud83c\udfaf Zero Code Changes","text":"<p>Notice that <code>train_ml_model_basic()</code> didn't change at all. Runnable works with your existing functions - no decorators, no API changes, no refactoring required.</p>"},{"location":"tutorial/02-making-it-reproducible/#run_it_multiple_times","title":"Run It Multiple Times","text":"<p>Try running the script several times:</p> <pre><code>uv run examples/tutorials/getting-started/02_making_it_reproducible.py\nuv run examples/tutorials/getting-started/02_making_it_reproducible.py\nuv run examples/tutorials/getting-started/02_making_it_reproducible.py\n</code></pre> <p>Each run creates a separate log entry in <code>.run_log_store/</code>. You now have a complete history of all your experiments!</p>"},{"location":"tutorial/02-making-it-reproducible/#compare_before_vs_after","title":"Compare: Before vs After","text":"<p>Before (Chapter 1):</p> <ul> <li>\u274c Results overwritten each time</li> <li>\u274c No execution history</li> <li>\u274c No timestamps or metadata</li> <li>\u274c Hard to track what worked</li> </ul> <p>After (Chapter 2):</p> <ul> <li>\u2705 Every run preserved with timestamp</li> <li>\u2705 Complete execution history</li> <li>\u2705 Full metadata captured automatically</li> <li>\u2705 Easy to see what worked when</li> </ul>"},{"location":"tutorial/02-making-it-reproducible/#whats_still_missing","title":"What's Still Missing?","text":"<p>We solved execution tracking, but we still have:</p> <ul> <li>Parameters hardcoded in the function</li> <li>No easy way to run experiments with different settings</li> </ul> <p>Next chapter: We'll make the function configurable without changing the ML logic.</p> <p>Next: Adding Flexibility - Configure experiments without touching code</p>"},{"location":"tutorial/03-adding-flexibility/","title":"Adding Flexibility","text":"<p>Now let's solve another major problem: hardcoded parameters. We'll make our ML function configurable so you can run different experiments without touching any code.</p>"},{"location":"tutorial/03-adding-flexibility/#the_problem_with_hardcoded_parameters","title":"The Problem with Hardcoded Parameters","text":"<p>In Chapter 2, our function still had hardcoded values:</p> <pre><code># Fixed values - need code changes for experiments\npreprocessed = preprocess_data(df, test_size=0.2, random_state=42)\nmodel_data = train_model(preprocessed, n_estimators=100, random_state=42)\n</code></pre> <p>Want to try <code>n_estimators=200</code>? Edit the code. Different train/test split? Edit the code. This doesn't scale for experimentation.</p>"},{"location":"tutorial/03-adding-flexibility/#the_solution_parameterized_functions","title":"The Solution: Parameterized Functions","text":"<p>Let's create a flexible version that accepts parameters:</p> examples/tutorials/getting-started/functions_parameterized.py<pre><code>def train_ml_model_flexible(\n    data_path=\"data.csv\",\n    test_size=0.2,\n    n_estimators=100,\n    random_state=42,\n    model_path=\"model.pkl\",\n    results_path=\"results.json\"\n):\n    \"\"\"Same ML logic, now configurable!\"\"\"\n    print(\"Loading data...\")\n    df = load_data(data_path)\n\n    print(\"Preprocessing...\")\n    preprocessed = preprocess_data(df, test_size=test_size, random_state=random_state)\n\n    print(f\"Training model with {n_estimators} estimators...\")\n    model_data = train_model(preprocessed, n_estimators=n_estimators, random_state=random_state)\n\n    # ... rest unchanged but uses parameters\n</code></pre>"},{"location":"tutorial/03-adding-flexibility/#running_with_parameters","title":"Running with Parameters","text":"<p>Now you can run different experiments without changing code:</p>"},{"location":"tutorial/03-adding-flexibility/#environment_variables","title":"\ud83c\udf0d Environment Variables","text":"<pre><code># Default parameters\nuv run examples/tutorials/getting-started/03_adding_flexibility.py\n\n# Large forest experiment\nRUNNABLE_PRM_n_estimators=200 uv run examples/tutorials/getting-started/03_adding_flexibility.py\n\n# Different train/test split\nRUNNABLE_PRM_test_size=0.3 RUNNABLE_PRM_n_estimators=150 uv run examples/tutorials/getting-started/03_adding_flexibility.py\n</code></pre>"},{"location":"tutorial/03-adding-flexibility/#configuration_files","title":"\ud83d\udcc1 Configuration Files","text":"<p>Create experiment configurations:</p> examples/tutorials/getting-started/experiment_configs/basic.yaml<pre><code>test_size: 0.2\nn_estimators: 50\nrandom_state: 42\nmodel_path: \"models/basic_model.pkl\"\nresults_path: \"results/basic_results.json\"\n</code></pre> examples/tutorials/getting-started/experiment_configs/large_forest.yaml<pre><code>test_size: 0.25\nn_estimators: 200\nrandom_state: 123\nmodel_path: \"models/large_forest.pkl\"\nresults_path: \"results/large_forest_results.json\"\n</code></pre> <p>Run different experiments:</p> <pre><code># Basic experiment\nuv run examples/tutorials/getting-started/03_adding_flexibility.py --parameters-file experiment_configs/basic.yaml\n\n# Large forest experiment\nuv run examples/tutorials/getting-started/03_adding_flexibility.py --parameters-file experiment_configs/large_forest.yaml\n</code></pre>"},{"location":"tutorial/03-adding-flexibility/#parameter_precedence","title":"Parameter Precedence","text":"<p>Runnable handles parameter conflicts intelligently:</p> <ol> <li>Environment variables (highest priority): <code>RUNNABLE_PRM_n_estimators=300</code></li> <li>Command line config: <code>--parameters-file config.yaml</code></li> <li>Function defaults (lowest priority): What you defined in the function signature</li> </ol> <p>This means you can have a base configuration file but override specific values with environment variables.</p>"},{"location":"tutorial/03-adding-flexibility/#what_you_get_now","title":"What You Get Now","text":""},{"location":"tutorial/03-adding-flexibility/#easy_experimentation","title":"\ud83e\uddea Easy Experimentation","text":"<ul> <li>Test different hyperparameters instantly</li> <li>Compare multiple approaches without code changes</li> <li>Save each experiment configuration for reproducibility</li> </ul>"},{"location":"tutorial/03-adding-flexibility/#automatic_experiment_tracking","title":"\ud83d\udcca Automatic Experiment Tracking","text":"<p>Every run gets logged with the exact parameters used:</p> <pre><code>ls .runnable/run-log-store/\n# Each timestamped directory contains the parameters for that run\n</code></pre>"},{"location":"tutorial/03-adding-flexibility/#reproducible_experiments","title":"\ud83d\udd04 Reproducible Experiments","text":"<p>Want to recreate that great result from last week? Just rerun with the same config file.</p>"},{"location":"tutorial/03-adding-flexibility/#clean_separation","title":"\ud83c\udfaf Clean Separation","text":"<ul> <li>Your ML logic: Stays in the function, unchanged</li> <li>Experiment configuration: Lives in config files or environment variables</li> <li>Execution tracking: Handled automatically by Runnable</li> </ul>"},{"location":"tutorial/03-adding-flexibility/#try_it_yourself","title":"Try It Yourself","text":"<p>Run these experiments and watch how each gets tracked separately:</p> <pre><code>cd examples/tutorials/getting-started\n\n# Experiment 1: Default\nuv run 03_adding_flexibility.py\n\n# Experiment 2: Large forest\nRUNNABLE_PRM_n_estimators=200 uv run 03_adding_flexibility.py\n\n# Experiment 3: From config file\nuv run 03_adding_flexibility.py --parameters-file experiment_configs/large_forest.yaml\n\n# Check the logs - each run preserved with its parameters\nls .run_log_store/\n</code></pre>"},{"location":"tutorial/03-adding-flexibility/#compare_before_vs_after","title":"Compare: Before vs After","text":"<p>Before:</p> <ul> <li>\u274c Parameters hardcoded in functions</li> <li>\u274c Code changes needed for experiments</li> <li>\u274c Hard to track which parameters produced which results</li> </ul> <p>After:</p> <ul> <li>\u2705 Functions accept parameters with sensible defaults</li> <li>\u2705 Experiments configurable via environment or config files</li> <li>\u2705 Every run logged with exact parameters used</li> <li>\u2705 Easy to reproduce any experiment</li> </ul> <p>Next: We'll break our monolithic function into a proper multi-step ML pipeline.</p> <p>Next: Connecting the Workflow - Multi-step ML pipeline with automatic data flow</p>"},{"location":"tutorial/04-connecting-workflow/","title":"Connecting the Workflow","text":"<p>So far we've been treating ML training as one big function. In reality, ML workflows have distinct steps: data loading, preprocessing, training, and evaluation. Let's break our monolithic function into a proper pipeline.</p>"},{"location":"tutorial/04-connecting-workflow/#why_break_it_up","title":"Why Break It Up?","text":"<p>Our current approach has limitations:</p> <pre><code>def train_ml_model_flexible():\n    # All steps in one function\n    df = load_data()           # Step 1\n    preprocessed = preprocess_data()  # Step 2\n    model = train_model()      # Step 3\n    results = evaluate_model() # Step 4\n    return results\n</code></pre> <p>Problems:</p> <ul> <li>If training fails, you lose preprocessing work</li> <li>Hard to debug specific steps</li> <li>Can't reuse preprocessing for different models</li> <li>No visibility into step-by-step progress</li> </ul>"},{"location":"tutorial/04-connecting-workflow/#the_solution_pipeline_with_tasks","title":"The Solution: Pipeline with Tasks","text":"<p>Let's use the individual functions we already have and connect them as a pipeline:</p> examples/tutorials/getting-started/04_connecting_workflow.py<pre><code>from runnable import Pipeline, PythonTask, pickled\nfrom functions import load_data, preprocess_data, train_model, evaluate_model\n\ndef main():\n    pipeline = Pipeline(steps=[\n        PythonTask(\n            function=load_data,\n            name=\"load_data\",\n            returns=[pickled(\"df\")]\n        ),\n        PythonTask(\n            function=preprocess_data,\n            name=\"preprocess\",\n            returns=[pickled(\"preprocessed_data\")]\n        ),\n        PythonTask(\n            function=train_model,\n            name=\"train\",\n            returns=[pickled(\"model_data\")]\n        ),\n        PythonTask(\n            function=evaluate_model,\n            name=\"evaluate\",\n            returns=[pickled(\"evaluation_results\")]\n        )\n    ])\n\n    pipeline.execute()\n    return pipeline\n</code></pre> <p>Try it:</p> <pre><code>uv run examples/tutorials/getting-started/04_connecting_workflow.py\n</code></pre>"},{"location":"tutorial/04-connecting-workflow/#how_data_flows_automatically","title":"How Data Flows Automatically","text":"<p>Notice something magical: we didn't write any glue code! Runnable automatically connects the steps:</p> <ol> <li><code>load_data()</code> returns a DataFrame</li> <li><code>preprocess_data(df)</code> - gets the DataFrame automatically (parameter name matches!)</li> <li><code>train_model(preprocessed_data)</code> - gets preprocessing results automatically</li> <li><code>evaluate_model(model_data, preprocessed_data)</code> - gets both model and data automatically</li> </ol> <p>The secret: Parameter names in your functions determine data flow. If <code>train_model()</code> expects a parameter called <code>preprocessed_data</code>, and a previous step returns something called <code>preprocessed_data</code>, they get connected automatically.</p>"},{"location":"tutorial/04-connecting-workflow/#what_you_get_with_pipelines","title":"What You Get with Pipelines","text":""},{"location":"tutorial/04-connecting-workflow/#step-by-step_execution","title":"\u26a1 Step-by-Step Execution","text":"<p>Each step runs individually and you can see progress:</p> <pre><code>load_data: \u2705 Completed in 0.1s\npreprocess: \u2705 Completed in 0.3s\ntrain: \u2705 Completed in 2.4s\nevaluate: \u2705 Completed in 0.2s\n</code></pre>"},{"location":"tutorial/04-connecting-workflow/#intermediate_results_preserved","title":"\ud83d\udd0d Intermediate Results Preserved","text":"<p>Each step's output is saved. You can inspect intermediate results without rerunning expensive steps:</p> <pre><code># Check what the preprocessing step produced\nls .runnable/\n</code></pre>"},{"location":"tutorial/04-connecting-workflow/#better_debugging","title":"\ud83d\udee0\ufe0f Better Debugging","text":"<p>If training fails, you don't lose your preprocessing work. You can debug just the training step.</p>"},{"location":"tutorial/04-connecting-workflow/#individual_step_tracking","title":"\ud83d\udcca Individual Step Tracking","text":"<p>See timing and resource usage for each step, helping identify bottlenecks.</p>"},{"location":"tutorial/04-connecting-workflow/#advanced_parameters_in_pipelines","title":"Advanced: Parameters in Pipelines","text":"<p>You can still use parameters, but now at the step level:</p> <pre><code># Add parameters to specific steps\npipeline = Pipeline(steps=[\n    PythonTask(function=load_data, name=\"load_data\", returns=[pickled(\"df\")]),\n    PythonTask(function=preprocess_data, name=\"preprocess\", returns=[pickled(\"preprocessed_data\")]),\n    PythonTask(function=train_model, name=\"train\", returns=[pickled(\"model_data\")]),\n    PythonTask(function=evaluate_model, name=\"evaluate\", returns=[pickled(\"evaluation_results\")])\n])\n\n# Parameters still work the same way\n# RUNNABLE_PRM_test_size=0.3 uv run 04_connecting_workflow.py\n</code></pre> <p>Parameters get passed to the appropriate functions based on their parameter names.</p>"},{"location":"tutorial/04-connecting-workflow/#compare_monolithic_vs_pipeline","title":"Compare: Monolithic vs Pipeline","text":"<p>Monolithic Function (Chapters 1-3):</p> <ul> <li>\u274c All-or-nothing execution</li> <li>\u274c Hard to debug failed steps</li> <li>\u274c Expensive to rerun everything</li> <li>\u274c No intermediate result visibility</li> </ul> <p>Pipeline (Chapter 4):</p> <ul> <li>\u2705 Step-by-step execution with progress</li> <li>\u2705 Intermediate results preserved</li> <li>\u2705 Resume from failed steps</li> <li>\u2705 Better debugging and development</li> <li>\u2705 Automatic data flow between steps</li> </ul>"},{"location":"tutorial/04-connecting-workflow/#your_functions_didnt_change","title":"Your Functions Didn't Change","text":"<p>Notice that we're using the exact same functions from earlier:</p> <ul> <li><code>load_data()</code></li> <li><code>preprocess_data()</code></li> <li><code>train_model()</code></li> <li><code>evaluate_model()</code></li> </ul> <p>No refactoring required. Runnable works with your existing functions - you just organize them into steps.</p>"},{"location":"tutorial/04-connecting-workflow/#whats_next","title":"What's Next?","text":"<p>We have a great pipeline, but we're still dealing with everything in memory. What about large datasets that don't fit in RAM? Or sharing intermediate results with teammates?</p> <p>Next chapter: We'll add efficient data management for large-scale ML workflows.</p> <p>Next: Handling Large Datasets - Efficient storage and retrieval of data artifacts</p>"},{"location":"tutorial/05-handling-datasets/","title":"Handling Large Datasets","text":"<p>Our pipeline is working great with small datasets that fit in memory. But what happens when your dataset is 100GB? Or when preprocessing generates gigabytes of intermediate results? Let's solve this with efficient file-based storage.</p>"},{"location":"tutorial/05-handling-datasets/#the_memory_problem","title":"The Memory Problem","text":"<p>In Chapter 4, we passed data between steps using <code>pickled()</code>:</p> <pre><code>PythonTask(\n    function=preprocess_data,\n    returns=[pickled(\"preprocessed_data\")]  # All data kept in memory!\n)\n</code></pre> <p>Problems with this approach:</p> <ul> <li>Large datasets won't fit in memory</li> <li>Pickling/unpickling is slow for big objects</li> <li>Can't easily inspect intermediate results</li> <li>Memory pressure on your system</li> </ul>"},{"location":"tutorial/05-handling-datasets/#the_solution_catalog_for_file_storage","title":"The Solution: Catalog for File Storage","text":"<p>Instead of passing data through memory, save it to files and let Runnable manage them:</p> examples/tutorials/getting-started/05_handling_datasets.py<pre><code>from runnable import Pipeline, PythonTask, Catalog, pickled\n\ndef load_data_to_file(data_path=\"data.csv\"):\n    \"\"\"Load data and save to file.\"\"\"\n    df = load_data(data_path)\n    df.to_csv(\"dataset.csv\", index=False)\n    return {\"rows\": len(df), \"columns\": len(df.columns)}\n\n# Store the dataset file automatically\nPythonTask(\n    function=load_data_to_file,\n    name=\"load_data\",\n    catalog=Catalog(put=[\"dataset.csv\"]),  # Store this file\n    returns=[pickled(\"dataset_info\")]  # Only metadata in memory\n)\n</code></pre> <p>Try it:</p> <pre><code>uv run examples/tutorials/getting-started/05_handling_datasets.py\n</code></pre>"},{"location":"tutorial/05-handling-datasets/#how_catalog_works","title":"How Catalog Works","text":""},{"location":"tutorial/05-handling-datasets/#step_1_create_and_store_files","title":"Step 1: Create and Store Files","text":"<pre><code>def preprocess_from_file(test_size=0.2, random_state=42):\n    # Load from file\n    df = pd.read_csv(\"dataset.csv\")\n\n    # Do your preprocessing\n    preprocessed = preprocess_data(df, test_size, random_state)\n\n    # Save results to files\n    preprocessed['X_train'].to_csv(\"X_train.csv\", index=False)\n    preprocessed['X_test'].to_csv(\"X_test.csv\", index=False)\n    preprocessed['y_train'].to_csv(\"y_train.csv\", index=False)\n    preprocessed['y_test'].to_csv(\"y_test.csv\", index=False)\n\n    return {\"train_samples\": len(preprocessed['X_train'])}\n\nPythonTask(\n    function=preprocess_from_file,\n    name=\"preprocess\",\n    catalog=Catalog(\n        get=[\"dataset.csv\"],  # Get input file\n        put=[\"X_train.csv\", \"X_test.csv\", \"y_train.csv\", \"y_test.csv\"]  # Store outputs\n    )\n)\n</code></pre>"},{"location":"tutorial/05-handling-datasets/#step_2_retrieve_and_use_files","title":"Step 2: Retrieve and Use Files","text":"<pre><code>def train_from_files(n_estimators=100, random_state=42):\n    # Files are automatically available!\n    X_train = pd.read_csv(\"X_train.csv\")\n    y_train = pd.read_csv(\"y_train.csv\")['target']\n\n    # Train your model\n    model = train_model(...)\n    return model\n\nPythonTask(\n    function=train_from_files,\n    name=\"train\",\n    catalog=Catalog(get=[\"X_train.csv\", \"y_train.csv\"])  # Get only what you need\n)\n</code></pre>"},{"location":"tutorial/05-handling-datasets/#complete_file-based_pipeline","title":"Complete File-Based Pipeline","text":"<p>Here's the full pipeline using file storage for large data:</p> examples/tutorials/getting-started/05_handling_datasets.py<pre><code>def evaluate_from_files(model_data):\n    \"\"\"Load test data from files and evaluate model.\"\"\"\n    # Load test data from files\n    X_test = pd.read_csv(\"X_test.csv\")\n    y_test = pd.read_csv(\"y_test.csv\")['target']\n\n    print(f\"Evaluating on {len(X_test)} test samples\")\n\n    preprocessed_data = {\n        'X_train': None,  # Not needed for evaluation\n        'y_train': None,\n        'X_test': X_test,\n        'y_test': y_test\n    }\n\n    results = evaluate_model(model_data, preprocessed_data)\n    return results\n\n\ndef main():\n    \"\"\"Demonstrate file-based data management with Catalog.\"\"\"\n    print(\"=\" * 50)\n    print(\"Chapter 5: Handling Large Datasets\")\n    print(\"=\" * 50)\n\n    pipeline = Pipeline(steps=[\n        # Load data and store the dataset file\n        PythonTask(\n            function=load_data_to_file,\n            name=\"load_data\",\n            catalog=Catalog(put=[\"dataset.csv\"]),\n            returns=[pickled(\"dataset_info\")]\n        ),\n        # Preprocess and store all intermediate files\n        PythonTask(\n            function=preprocess_from_file,\n            name=\"preprocess\",\n            catalog=Catalog(\n                get=[\"dataset.csv\"],  # Get the dataset\n                put=[\"X_train.csv\", \"X_test.csv\", \"y_train.csv\", \"y_test.csv\"]  # Store results\n            ),\n            returns=[pickled(\"preprocess_info\")]\n        ),\n        # Train model using files\n        PythonTask(\n            function=train_from_files,\n            name=\"train\",\n            catalog=Catalog(get=[\"X_train.csv\", \"y_train.csv\"]),\n            returns=[pickled(\"model_data\")]\n        ),\n        # Evaluate using files\n        PythonTask(\n            function=evaluate_from_files,\n            name=\"evaluate\",\n            catalog=Catalog(get=[\"X_test.csv\", \"y_test.csv\"]),\n            returns=[pickled(\"evaluation_results\")]\n</code></pre>"},{"location":"tutorial/05-handling-datasets/#what_you_get_with_file-based_storage","title":"What You Get with File-Based Storage","text":""},{"location":"tutorial/05-handling-datasets/#handle_large_datasets","title":"\ud83d\udcbe Handle Large Datasets","text":"<p>Your dataset can be bigger than available RAM - only load what you need when you need it:</p> <pre><code># Only load training data for training step\nX_train = pd.read_csv(\"X_train.csv\")  # Maybe 50GB\n# X_test isn't loaded - saves memory!\n</code></pre>"},{"location":"tutorial/05-handling-datasets/#automatic_file_management","title":"\ud83d\udd04 Automatic File Management","text":"<p>Runnable handles file locations transparently:</p> <ul> <li><code>put=[\"file.parquet\"]</code> - Stores file safely in <code>.runnable/</code> catalog</li> <li><code>get=[\"file.parquet\"]</code> - Makes file available in your working directory</li> <li>Files appear exactly where your code expects them</li> </ul>"},{"location":"tutorial/05-handling-datasets/#inspect_intermediate_results","title":"\ud83d\udce6 Inspect Intermediate Results","text":"<p>All intermediate files are preserved:</p> <pre><code># Check what preprocessing produced\nls .runnable/catalog/\n# X_train.csv  X_test.csv  y_train.csv  y_test.csv\n</code></pre>"},{"location":"tutorial/05-handling-datasets/#resume_without_reloading","title":"\ud83d\ude80 Resume Without Reloading","text":"<p>If training fails, you don't need to reload and preprocess your 100GB dataset - it's already there!</p>"},{"location":"tutorial/05-handling-datasets/#share_results","title":"\ud83e\udd1d Share Results","text":"<p>Team members can reuse your preprocessed data without running expensive preprocessing steps.</p>"},{"location":"tutorial/05-handling-datasets/#when_to_use_files_vs_memory","title":"When to Use Files vs Memory","text":"<p>Use <code>Catalog(put=[...])</code> for files when:</p> <ul> <li>Dataset is large (&gt;1GB)</li> <li>Preprocessing is expensive</li> <li>You want to inspect intermediate results</li> <li>Team members need to share data</li> </ul> <p>Use <code>pickled()</code> for memory when:</p> <ul> <li>Data is small (&lt;100MB)</li> <li>Objects are complex (models, configs)</li> <li>You need fast passing between steps</li> </ul>"},{"location":"tutorial/05-handling-datasets/#mixing_files_and_memory","title":"Mixing Files and Memory","text":"<p>You can use both approaches in the same pipeline:</p> <pre><code>pipeline = Pipeline(steps=[\n    PythonTask(\n        function=load_data_to_file,\n        catalog=Catalog(put=[\"dataset.csv\"]),  # Large data \u2192 file\n        returns=[pickled(\"metadata\")]  # Small metadata \u2192 memory\n    ),\n    PythonTask(\n        function=train_from_files,\n        catalog=Catalog(get=[\"dataset.csv\"]),  # Get large data from file\n        returns=[pickled(\"model\")]  # Model usually fits in memory\n    )\n])\n</code></pre>"},{"location":"tutorial/05-handling-datasets/#compare_memory_vs_file_storage","title":"Compare: Memory vs File Storage","text":"<p>Memory Passing (Chapters 1-4):</p> <ul> <li>\u274c Limited by available RAM</li> <li>\u274c Slow for large objects</li> <li>\u274c Hard to inspect intermediate data</li> <li>\u2705 Simple for small objects</li> <li>\u2705 Fast for small data</li> </ul> <p>File Storage (Chapter 5):</p> <ul> <li>\u2705 Handle datasets larger than RAM</li> <li>\u2705 Efficient for large files</li> <li>\u2705 Easy to inspect intermediate results</li> <li>\u2705 Shareable across runs and team members</li> <li>\u2705 Automatic file management</li> </ul>"},{"location":"tutorial/05-handling-datasets/#whats_next","title":"What's Next?","text":"<p>We can now handle large datasets efficiently. But what about saving your trained models and results permanently? What if your teammate wants to use your model without rerunning everything?</p> <p>Next chapter: We'll add persistent storage for models and results that can be shared across runs and team members.</p> <p>Next: Sharing Results - Persistent model artifacts and metrics</p>"},{"location":"tutorial/06-sharing-results/","title":"Sharing Results","text":"<p>You've trained a great model! But what happens next? Your teammate needs to use it, or you want to compare today's model with yesterday's. Let's make your results persistent and shareable.</p>"},{"location":"tutorial/06-sharing-results/#the_disappearing_results_problem","title":"The Disappearing Results Problem","text":"<p>After running your pipeline, where are your results?</p> <pre><code># Train a model...\npipeline.execute()\n# Great! But where is the model now?\n# Can your teammate use it?\n# Can you compare with yesterday's run?\n</code></pre> <p>Problems:</p> <ul> <li>Models only exist during execution</li> <li>No way to track metrics over time</li> <li>Can't share trained models with teammates</li> <li>Hard to compare different runs</li> </ul>"},{"location":"tutorial/06-sharing-results/#the_solution_persistent_storage","title":"The Solution: Persistent Storage","text":"<p>Let's save models and metrics that persist beyond execution:</p> examples/tutorials/getting-started/06_sharing_results.py<pre><code>from runnable import Pipeline, PythonTask, Catalog, metric\nimport pickle\n\ndef save_model_artifact(model_data):\n    \"\"\"Save trained model to a file.\"\"\"\n    with open(\"trained_model.pkl\", \"wb\") as f:\n        pickle.dump(model_data, f)\n    return model_data\n\nPythonTask(\n    function=save_model_artifact,\n    name=\"save_model\",\n    catalog=Catalog(put=[\"trained_model.pkl\"]),  # Store in catalog\n    returns=[pickled(\"model_data\")]\n)\n</code></pre> <p>Try it:</p> <pre><code>uv run examples/tutorials/getting-started/06_sharing_results.py\n</code></pre>"},{"location":"tutorial/06-sharing-results/#storing_model_artifacts","title":"Storing Model Artifacts","text":"<p>Save your trained models so they can be reused:</p> <pre><code>def save_model_artifact(model_data):\n    \"\"\"Save model to file for sharing.\"\"\"\n    with open(\"trained_model.pkl\", \"wb\") as f:\n        pickle.dump(model_data, f)\n\n    print(f\"Model saved: trained_model.pkl\")\n    return model_data\n\n# Store the model file in catalog\nPythonTask(\n    function=save_model_artifact,\n    name=\"save_model\",\n    catalog=Catalog(put=[\"trained_model.pkl\"])\n)\n</code></pre> <p>What happens:</p> <ol> <li>Model is saved to <code>trained_model.pkl</code> in your working directory</li> <li>Runnable copies it to <code>.catalog/</code> for permanent storage</li> <li>File is now available for future runs or teammates</li> </ol>"},{"location":"tutorial/06-sharing-results/#tracking_metrics_over_time","title":"Tracking Metrics Over Time","text":"<p>Use <code>metric()</code> to track performance metrics:</p> <pre><code>def save_evaluation_metrics(evaluation_results):\n    \"\"\"Save metrics for tracking.\"\"\"\n    accuracy = evaluation_results['accuracy']\n    report = evaluation_results['classification_report']\n\n    # Save detailed report\n    with open(\"evaluation_report.json\", \"w\") as f:\n        json.dump(report, f, indent=2)\n\n    # Return metrics for tracking\n    return {\n        \"accuracy\": accuracy,\n        \"precision\": report['weighted avg']['precision'],\n        \"recall\": report['weighted avg']['recall'],\n        \"f1_score\": report['weighted avg']['f1-score']\n    }\n\nPythonTask(\n    function=save_evaluation_metrics,\n    name=\"save_metrics\",\n    catalog=Catalog(put=[\"evaluation_report.json\"]),\n    returns=[\n        metric(\"accuracy\"),      # Tracked as metrics\n        metric(\"precision\"),\n        metric(\"recall\"),\n        metric(\"f1_score\")\n    ]\n)\n</code></pre> <p>Metrics are special:</p> <ul> <li>Automatically tracked in run logs</li> <li>Easy to compare across runs</li> <li>Can be visualized over time</li> <li>Help identify model improvements</li> </ul>"},{"location":"tutorial/06-sharing-results/#loading_saved_models","title":"Loading Saved Models","text":"<p>Your teammate (or you in a future run) can load the saved model:</p> <pre><code>def load_and_verify_model():\n    \"\"\"Load a previously saved model.\"\"\"\n    with open(\"trained_model.pkl\", \"rb\") as f:\n        model_data = pickle.load(f)\n\n    print(f\"Model loaded successfully!\")\n    return {\"model_verified\": True}\n\nPythonTask(\n    function=load_and_verify_model,\n    name=\"verify_model\",\n    catalog=Catalog(get=[\"trained_model.pkl\"])  # Get the saved model\n)\n</code></pre>"},{"location":"tutorial/06-sharing-results/#complete_pipeline_with_persistent_storage","title":"Complete Pipeline with Persistent Storage","text":"<p>Here's the full pipeline that saves and shares results:</p> examples/tutorials/getting-started/06_sharing_results.py<pre><code>def main():\n    \"\"\"Demonstrate persistent storage of models and metrics.\"\"\"\n    print(\"=\" * 50)\n    print(\"Chapter 6: Sharing Results\")\n    print(\"=\" * 50)\n\n    pipeline = Pipeline(steps=[\n        # Load and preprocess data\n        PythonTask(\n            function=load_data,\n            name=\"load_data\",\n            returns=[pickled(\"df\")]\n        ),\n        PythonTask(\n            function=preprocess_data,\n            name=\"preprocess\",\n            returns=[pickled(\"preprocessed_data\")]\n        ),\n        # Train model\n        PythonTask(\n            function=train_model,\n            name=\"train\",\n            returns=[pickled(\"model_data\")]\n        ),\n        # Save model artifact to catalog\n        PythonTask(\n            function=save_model_artifact,\n            name=\"save_model\",\n            catalog=Catalog(put=[\"trained_model.pkl\"]),  # Store model file\n            returns=[pickled(\"model_data\")]\n        ),\n        # Evaluate and save metrics\n        PythonTask(\n            function=evaluate_model,\n            name=\"evaluate\",\n            returns=[pickled(\"evaluation_results\")]\n        ),\n        PythonTask(\n            function=save_evaluation_metrics,\n            name=\"save_metrics\",\n            catalog=Catalog(put=[\"evaluation_report.json\", \"metrics_summary.json\"]),\n            returns=[\n                metric(\"accuracy\"),\n                metric(\"precision\"),\n                metric(\"recall\"),\n                metric(\"f1_score\")\n            ]\n        ),\n        # Verify model can be loaded (simulating another run or teammate)\n        PythonTask(\n            function=load_and_verify_model,\n            name=\"verify_model\",\n            catalog=Catalog(get=[\"trained_model.pkl\"]),  # Get the saved model\n            returns=[pickled(\"verification\")]\n        )\n    ])\n\n    pipeline.execute()\n\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Persistent storage benefits:\")\n</code></pre>"},{"location":"tutorial/06-sharing-results/#what_you_get_with_persistent_storage","title":"What You Get with Persistent Storage","text":""},{"location":"tutorial/06-sharing-results/#shareable_model_artifacts","title":"\ud83d\udce6 Shareable Model Artifacts","text":"<p>Models are stored in the catalog and can be shared:</p> <pre><code># Your trained model is here\nls .catalog/*/trained_model.pkl\n\n# Teammate can copy and use it\ncp .catalog/run-id-123/trained_model.pkl shared-models/\n</code></pre>"},{"location":"tutorial/06-sharing-results/#metrics_tracking","title":"\ud83d\udcca Metrics Tracking","text":"<p>All metrics are tracked in the run log:</p> <pre><code>{\n  \"run_id\": \"happy-euler-0123\",\n  \"metrics\": {\n    \"accuracy\": 0.9234,\n    \"precision\": 0.9156,\n    \"recall\": 0.9234,\n    \"f1_score\": 0.9189\n  }\n}\n</code></pre>"},{"location":"tutorial/06-sharing-results/#performance_history","title":"\ud83d\udcc8 Performance History","text":"<p>Compare different runs:</p> <pre><code># See all your runs\nls .run_log_store/\n\n# Compare metrics from different runs\ncat .run_log_store/run-1/run_log.json | grep accuracy\ncat .run_log_store/run-2/run_log.json | grep accuracy\n</code></pre>"},{"location":"tutorial/06-sharing-results/#team_collaboration","title":"\ud83e\udd1d Team Collaboration","text":"<ul> <li>Share trained models without retraining</li> <li>Compare your model with teammate's models</li> <li>Reuse preprocessing results</li> <li>Track team's overall progress</li> </ul>"},{"location":"tutorial/06-sharing-results/#storage_locations","title":"Storage Locations","text":"<p>Runnable keeps everything organized:</p> <pre><code>.catalog/                    # Persistent file storage\n  \u251c\u2500\u2500 run-id-123/\n  \u2502   \u251c\u2500\u2500 trained_model.pkl       # Your model\n  \u2502   \u251c\u2500\u2500 evaluation_report.json  # Detailed metrics\n  \u2502   \u2514\u2500\u2500 metrics_summary.json    # Quick summary\n  \u2514\u2500\u2500 run-id-124/\n      \u2514\u2500\u2500 trained_model.pkl       # Next run's model\n\n.run_log_store/             # Execution metadata\n  \u251c\u2500\u2500 run-id-123/\n  \u2502   \u2514\u2500\u2500 run_log.json           # Includes metrics\n  \u2514\u2500\u2500 run-id-124/\n      \u2514\u2500\u2500 run_log.json\n</code></pre>"},{"location":"tutorial/06-sharing-results/#compare_transient_vs_persistent","title":"Compare: Transient vs Persistent","text":"<p>Transient Results (Chapters 1-5):</p> <ul> <li>\u274c Results lost after execution</li> <li>\u274c Can't compare different runs</li> <li>\u274c Can't share with teammates</li> <li>\u274c Must retrain to reuse model</li> </ul> <p>Persistent Results (Chapter 6):</p> <ul> <li>\u2705 Models saved permanently</li> <li>\u2705 Metrics tracked over time</li> <li>\u2705 Easy to share with team</li> <li>\u2705 Reuse without retraining</li> <li>\u2705 Performance history available</li> </ul>"},{"location":"tutorial/06-sharing-results/#real-world_use_cases","title":"Real-World Use Cases","text":""},{"location":"tutorial/06-sharing-results/#model_versioning","title":"Model Versioning","text":"<pre><code># Save each model with version info\ndef save_model_v2(model_data):\n    with open(f\"model_v2.pkl\", \"wb\") as f:\n        pickle.dump(model_data, f)\n\ncatalog=Catalog(put=[\"model_v2.pkl\"])\n</code></pre>"},{"location":"tutorial/06-sharing-results/#ab_testing","title":"A/B Testing","text":"<pre><code># Compare two models\nreturns=[\n    metric(\"model_a_accuracy\"),\n    metric(\"model_b_accuracy\")\n]\n</code></pre>"},{"location":"tutorial/06-sharing-results/#experiment_tracking","title":"Experiment Tracking","text":"<pre><code># Track different hyperparameters\nreturns=[\n    metric(\"accuracy\"),\n    metric(\"n_estimators\"),  # Track hyperparameter used\n    metric(\"max_depth\")\n]\n</code></pre>"},{"location":"tutorial/06-sharing-results/#whats_next","title":"What's Next?","text":"<p>We have reproducible pipelines, flexible configuration, efficient data handling, and persistent results. But everything is running on your laptop. What about production?</p> <p>Next chapter: We'll show how the same pipeline runs anywhere - your laptop, containers, or Kubernetes - without code changes.</p> <p>Next: Running Anywhere - Same code, different environments</p>"},{"location":"tutorial/07-running-anywhere/","title":"Running Anywhere","text":"<p>You've built a great ML pipeline that works on your laptop. But what about production? Containers? Kubernetes? The good news: your code doesn't need to change.</p>"},{"location":"tutorial/07-running-anywhere/#the_deployment_challenge","title":"The Deployment Challenge","text":"<p>Traditional ML pipelines require code changes for different environments:</p> <pre><code># Development version\nresults = train_model(data, local=True)\n\n# Production version\nresults = train_model(data, use_kubernetes=True, replicas=5)\n\n# Container version\nresults = train_model(data, docker=True, image=\"my-model:latest\")\n</code></pre> <p>Problems:</p> <ul> <li>Different code for different environments</li> <li>Hard to test production code locally</li> <li>Risk of bugs when deploying</li> <li>Code becomes cluttered with infrastructure logic</li> </ul>"},{"location":"tutorial/07-running-anywhere/#the_runnable_way_configuration_over_code","title":"The Runnable Way: Configuration Over Code","text":"<p>With Runnable, your code stays the same. Only the configuration changes:</p> examples/tutorials/getting-started/07_running_anywhere.py<pre><code># This exact same code runs everywhere!\npipeline = Pipeline(steps=[\n    PythonTask(function=load_data, name=\"load_data\", returns=[pickled(\"df\")]),\n    PythonTask(function=preprocess_data, name=\"preprocess\", returns=[pickled(\"preprocessed_data\")]),\n    PythonTask(function=train_model, name=\"train\", returns=[pickled(\"model_data\")]),\n    PythonTask(function=evaluate_model, name=\"evaluate\", returns=[pickled(\"evaluation_results\")])\n])\n\npipeline.execute()  # Environment determined by config, not code\n</code></pre> <p>Try it:</p> <pre><code>uv run examples/tutorials/getting-started/07_running_anywhere.py\n</code></pre>"},{"location":"tutorial/07-running-anywhere/#same_code_different_environments","title":"Same Code, Different Environments","text":""},{"location":"tutorial/07-running-anywhere/#1_local_execution_development","title":"1. Local Execution (Development)","text":"<p>Run on your laptop with default settings:</p> <pre><code>uv run examples/tutorials/getting-started/07_running_anywhere.py\n</code></pre> <p>What happens:</p> <ul> <li>Runs directly on your machine</li> <li>Uses local file system for storage</li> <li>Fast iteration during development</li> <li>No infrastructure required</li> </ul>"},{"location":"tutorial/07-running-anywhere/#2_container_execution_testing","title":"2. Container Execution (Testing)","text":"<p>Run in containers for isolated testing:</p> <pre><code>uv run examples/tutorials/getting-started/07_running_anywhere.py \\\n  --config examples/configs/local-container.yaml\n</code></pre> examples/configs/local-container.yaml<pre><code>pipeline-executor:\n  type: \"local-container\"\n  config:\n    docker_image: runnable-m1:latest\n    enable_parallel: true\n</code></pre> <p>What changes:</p> <ul> <li>Each step runs in a Docker container</li> <li>Same local file system access</li> <li>Tests containerized behavior locally</li> <li>Your code: unchanged</li> </ul>"},{"location":"tutorial/07-running-anywhere/#3_cloud_storage_production-like","title":"3. Cloud Storage (Production-like)","text":"<p>Use cloud storage for data:</p> <pre><code>uv run examples/tutorials/getting-started/07_running_anywhere.py \\\n  --config examples/configs/s3-storage.yaml\n</code></pre> examples/configs/s3-storage.yaml<pre><code>catalog:\n  type: \"s3\"\n  config:\n    bucket: \"my-ml-artifacts\"\n    region: \"us-west-2\"\n</code></pre> <p>What changes:</p> <ul> <li>Artifacts stored in S3</li> <li>Team can access results</li> <li>Production-ready storage</li> <li>Your code: unchanged</li> </ul>"},{"location":"tutorial/07-running-anywhere/#4_kubernetes_execution_production","title":"4. Kubernetes Execution (Production)","text":"<p>Run on Kubernetes cluster:</p> <pre><code>uv run examples/tutorials/getting-started/07_running_anywhere.py \\\n  --config examples/configs/kubernetes.yaml\n</code></pre> examples/configs/kubernetes.yaml<pre><code>pipeline-executor:\n  type: \"kubernetes\"\n  config:\n    namespace: \"ml-pipelines\"\n    image: \"my-registry/ml-pipeline:v1\"\n\ncatalog:\n  type: \"s3\"\n  config:\n    bucket: \"production-ml-artifacts\"\n</code></pre> <p>What changes:</p> <ul> <li>Runs on Kubernetes pods</li> <li>Scales automatically</li> <li>Production-grade execution</li> <li>Your code: unchanged</li> </ul>"},{"location":"tutorial/07-running-anywhere/#the_power_of_configuration","title":"The Power of Configuration","text":"<p>All these configurations are external to your code:</p> <pre><code># Your pipeline code (never changes)\nfrom functions import load_data, train_model\nfrom runnable import Pipeline, PythonTask, pickled\n\npipeline = Pipeline(steps=[\n    PythonTask(function=load_data, returns=[pickled(\"df\")]),\n    PythonTask(function=train_model, returns=[pickled(\"model\")])\n])\n\n# Environment determined at runtime by config\npipeline.execute()\n</code></pre>"},{"location":"tutorial/07-running-anywhere/#development_to_production_workflow","title":"Development to Production Workflow","text":""},{"location":"tutorial/07-running-anywhere/#step_1_develop_locally","title":"Step 1: Develop Locally","text":"<pre><code># Fast iteration on your laptop\nuv run 07_running_anywhere.py\n</code></pre>"},{"location":"tutorial/07-running-anywhere/#step_2_test_in_containers","title":"Step 2: Test in Containers","text":"<pre><code># Verify containerized behavior\nuv run 07_running_anywhere.py --config local-container.yaml\n</code></pre>"},{"location":"tutorial/07-running-anywhere/#step_3_deploy_to_staging","title":"Step 3: Deploy to Staging","text":"<pre><code># Run on staging cluster with cloud storage\nuv run 07_running_anywhere.py --config staging.yaml\n</code></pre>"},{"location":"tutorial/07-running-anywhere/#step_4_deploy_to_production","title":"Step 4: Deploy to Production","text":"<pre><code># Same code, production configuration\nuv run 07_running_anywhere.py --config production.yaml\n</code></pre> <p>At no point did you change your pipeline code!</p>"},{"location":"tutorial/07-running-anywhere/#configuration_options","title":"Configuration Options","text":"<p>Runnable supports many deployment scenarios through configuration:</p>"},{"location":"tutorial/07-running-anywhere/#execution_environments","title":"Execution Environments","text":"<ul> <li>local: Run directly on your machine</li> <li>local-container: Run in Docker containers locally</li> <li>kubernetes: Run on Kubernetes cluster</li> <li>argo: Use Argo Workflows for complex DAGs</li> </ul>"},{"location":"tutorial/07-running-anywhere/#storage_options","title":"Storage Options","text":"<ul> <li>file-system: Local file storage</li> <li>s3: AWS S3 buckets</li> <li>minio: Self-hosted S3-compatible storage</li> <li>azure-blob: Azure Blob Storage</li> </ul>"},{"location":"tutorial/07-running-anywhere/#run_log_storage","title":"Run Log Storage","text":"<ul> <li>file-system: Local JSON files</li> <li>chunked-fs: Optimized local storage</li> <li>database: PostgreSQL, MySQL</li> <li>cloud: S3, Azure, GCS</li> </ul>"},{"location":"tutorial/07-running-anywhere/#secret_management","title":"Secret Management","text":"<ul> <li>env-secrets: Environment variables</li> <li>dotenv: .env files</li> <li>aws-secrets: AWS Secrets Manager</li> <li>azure-secrets: Azure Key Vault</li> </ul>"},{"location":"tutorial/07-running-anywhere/#complete_example_pipeline","title":"Complete Example Pipeline","text":"<p>Here's the complete pipeline that runs anywhere:</p> examples/tutorials/getting-started/07_running_anywhere.py<pre><code>def main():\n    \"\"\"The exact same pipeline from Chapter 4 - no code changes!\"\"\"\n    print(\"=\" * 50)\n    print(\"Chapter 7: Running Anywhere\")\n    print(\"=\" * 50)\n\n    # This is the EXACT same pipeline from Chapter 4\n    # No modifications needed to run in different environments\n    pipeline = Pipeline(steps=[\n        PythonTask(\n            function=load_data,\n            name=\"load_data\",\n            returns=[pickled(\"df\")]\n        ),\n        PythonTask(\n            function=preprocess_data,\n            name=\"preprocess\",\n            returns=[pickled(\"preprocessed_data\")]\n        ),\n        PythonTask(\n            function=train_model,\n            name=\"train\",\n            returns=[pickled(\"model_data\")]\n        ),\n        PythonTask(\n            function=evaluate_model,\n            name=\"evaluate\",\n            returns=[pickled(\"evaluation_results\")]\n        )\n    ])\n\n    # Execute the pipeline\n    # The environment is determined by configuration, not code\n    pipeline.execute()\n\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Running anywhere benefits:\")\n    print(\"- \ud83d\udcbb Same code runs on laptop, containers, or cloud\")\n    print(\"- \ud83d\udd27 Environment controlled by configuration files\")\n    print(\"- \ud83d\ude80 No code changes for different deployments\")\n    print(\"- \ud83c\udfaf Develop locally, deploy anywhere\")\n    print(\"- \ud83d\udd04 Easy migration between platforms\")\n    print(\"=\" * 50)\n\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Example: Run this pipeline in different ways:\")\n    print()\n    print(\"1. Local execution (default):\")\n    print(\"   uv run examples/tutorials/getting-started/07_running_anywhere.py\")\n    print()\n    print(\"2. With containers (if Docker available):\")\n    print(\"   uv run examples/tutorials/getting-started/07_running_anywhere.py \\\\\")\n    print(\"     --config examples/configs/local-container.yaml\")\n    print()\n    print(\"3. With custom catalog location:\")\n    print(\"   uv run examples/tutorials/getting-started/07_running_anywhere.py \\\\\")\n    print(\"     --config examples/configs/custom-storage.yaml\")\n    print()\n    print(\"Same code. Different environments. Zero changes.\")\n    print(\"=\" * 50)\n\n    return pipeline\n\n\nif __name__ == \"__main__\":\n</code></pre>"},{"location":"tutorial/07-running-anywhere/#what_youve_achieved","title":"What You've Achieved","text":""},{"location":"tutorial/07-running-anywhere/#code_portability","title":"\ud83d\udcbb Code Portability","text":"<p>Your pipeline code works everywhere:</p> <ul> <li>Local laptop for development</li> <li>Docker containers for testing</li> <li>Kubernetes for production</li> <li>Cloud platforms without changes</li> </ul>"},{"location":"tutorial/07-running-anywhere/#configuration-driven","title":"\ud83d\udd27 Configuration-Driven","text":"<p>Change behavior without code changes:</p> <ul> <li>Switch storage backends</li> <li>Change execution environments</li> <li>Scale up or down</li> <li>All through configuration</li> </ul>"},{"location":"tutorial/07-running-anywhere/#develop_locally_deploy_anywhere","title":"\ud83c\udfaf Develop Locally, Deploy Anywhere","text":"<p>The best development experience:</p> <ol> <li>Write code locally with fast feedback</li> <li>Test in containers for isolation</li> <li>Deploy to production with confidence</li> <li>No code changes between environments</li> </ol>"},{"location":"tutorial/07-running-anywhere/#production_ready","title":"\ud83d\ude80 Production Ready","text":"<p>Built-in support for:</p> <ul> <li>Distributed execution</li> <li>Cloud storage</li> <li>Secret management</li> <li>Monitoring and logging</li> </ul>"},{"location":"tutorial/07-running-anywhere/#real-world_example","title":"Real-World Example","text":"<p>A typical ML team workflow:</p> <pre><code># Data scientist develops locally\npython train.py\n\n# CI/CD tests in containers\npython train.py --config ci-container.yaml\n\n# Model engineer validates on staging\npython train.py --config staging-k8s.yaml\n\n# Production deployment\npython train.py --config production-k8s.yaml\n</code></pre> <p>Same Python file. Four different environments. Zero code changes.</p>"},{"location":"tutorial/07-running-anywhere/#tutorial_complete","title":"Tutorial Complete!","text":"<p>Congratulations! You've learned how to:</p> <ol> <li>\u2705 Start simple - Transform a basic ML function into a pipeline</li> <li>\u2705 Make it reproducible - Automatic tracking of all runs and results</li> <li>\u2705 Add flexibility - Configure experiments without code changes</li> <li>\u2705 Connect workflow - Multi-step pipelines with automatic data flow</li> <li>\u2705 Handle large datasets - Efficient file-based storage</li> <li>\u2705 Share results - Persistent models and metrics</li> <li>\u2705 Run anywhere - Same code, different environments</li> </ol>"},{"location":"tutorial/07-running-anywhere/#whats_next","title":"What's Next?","text":"<p>Explore more advanced features:</p> <ul> <li>Parallel Execution - Run independent steps concurrently</li> <li>Conditional Workflows - Dynamic workflow decisions</li> <li>Map Patterns - Process items in parallel</li> <li>Deploy Anywhere - Production deployment strategies</li> </ul>"},{"location":"tutorial/07-running-anywhere/#summary","title":"Summary","text":"<p>The key insight of this tutorial:</p> <p>Separate your ML logic from infrastructure concerns. Your functions stay pure and simple. Runnable handles the orchestration, storage, tracking, and deployment.</p> <p>This separation enables:</p> <ul> <li>Faster development (test locally)</li> <li>Easier testing (same code everywhere)</li> <li>Confident deployment (proven code)</li> <li>Better collaboration (shared understanding)</li> </ul> <p>Ready to build production ML pipelines? You now have all the foundations!</p>"}]}